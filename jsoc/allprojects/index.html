<!doctype html> <html lang=en > <meta charset=utf-8 > <meta name=viewport  content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv=x-ua-compatible  content="ie=edge"> <meta name=author  content="Jeff Bezanson, Stefan Karpinski, Viral Shah, Alan Edelman, et al."> <meta name=description  content="The official website for the Julia Language. Julia is a language that is fast, dynamic, easy to use, and open source. Click here to learn more."> <meta name=robots  content="max-image-preview:large"> <meta name="twitter:site:id" content=1237720952 > <meta name=google-site-verification  content=9VDSjBtchQj6PQYIVwugTPY7pVCfLYgvkXiRHjc_Bzw  /> <link rel=icon  href="/assets/infra/julia.ico"> <link rel=stylesheet  href="/libs/bootstrap/bootstrap.min.css"> <link rel=stylesheet  href="/css/app.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/fonts.css"> <link href="https://fonts.googleapis.com/css?family=Roboto:400,400i,500,500i,700,700i" rel=stylesheet > <link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel=stylesheet > <script async defer src="/libs/buttons.js"></script> <script type="application/javascript"> var doNotTrack = false; if (!doNotTrack) { window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date; ga('create', 'UA-28835595-1', 'auto', { 'anonymize_ip': true }); ga('send', 'pageview', { 'anonymize_ip': true }); } </script> <script async src='https://www.google-analytics.com/analytics.js'></script> <title>View all GSoC/JSoC Projects</title> <meta property="og:title" content="View all GSoC/JSoC Projects"> <meta property="og:description" content=""> <meta property="og:image" content="/assets/images/julia-open-graph.png"> <div class="container py-3 py-lg-0"> <nav class="navbar navbar-expand-lg navbar-light bg-light" id=main-menu > <a class=navbar-brand  href="/"> <img src="/assets/infra/logo.svg" alt="JuliaLang Logo"> </a> <button class="navbar-toggler ml-auto hidden-sm-up float-xs-left" type=button  data-toggle=collapse  data-target="#navbarSupportedContent" aria-controls=navbarSupportedContent  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarSupportedContent > <ul class="navbar-nav mx-auto"> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="/downloads/">Download</a> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="https://docs.julialang.org">Documentation</a> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="/blog/">Blog</a> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="/community/">Community</a> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="/learning/">Learn</a> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="/research/">Research</a> <li class="nav-item active flex-md-fill text-md-center"> <a class=nav-link  href="/jsoc/">JSoC</a> </ul> <span class=navbar-right > <a class=github-button  href="https://github.com/sponsors/julialang" data-icon=octicon-heart  data-size=large  aria-label="Sponsor @julialang on GitHub">Sponsor</a> </span> </div> </nav> </div> <br><br> <a href="https://github.com/JuliaLang/www.julialang.org/blob/master/jsoc/allprojects.md" title="Edit this page on GitHub" class=edit-float > </a> <div class="container main"><h2 id=view_all_gsocjsoc_projects ><a href="#view_all_gsocjsoc_projects">View all GSoC/JSoC Projects</a></h2> <p>This page is designed to improve discoverability of projects. You can, for example, search this page for specific keywords and find all of the relevant projects.</p> <h2 id=projects ><a href="#projects">Projects</a></h2> <h1 id=geostatsjl_-_summer_of_code ><a href="#geostatsjl_-_summer_of_code">GeoStats.jl - Summer of Code</a></h1> <p><a href="https://github.com/JuliaEarth/GeoStats.jl">GeoStats.jl</a> is an extensible framework for high-performance geostatistics in Julia. It is a project that aims to redefine the way statistics is done with geospatial data &#40;e.g. data on geographics maps, 3D meshes&#41;.</p> <p>Project mentors: <a href="https://github.com/juliohm">Júlio Hoffimann</a>, <a href="https://github.com/rmcaixeta">Rafael Caixeta</a></p> <h2 id=new_geostatistical_clustering_methods ><a href="#new_geostatistical_clustering_methods">New geostatistical clustering methods</a></h2> <p>Statistical clustering cannot be applied straightforwardly to geospatial data. Geospatial constraints require clusters to be contiguous volumes in the map, something that is not taken into account by traditional methods &#40;e.g. K-Means, Spectral Clustering&#41;.</p> <p>The goal of this project is to implement a geospatial clustering method from the geostatistics literature using the GeoStats.jl API.</p> <p><strong>Desired skills:</strong> Statistics, Clustering, Graph Theory</p> <p><strong>Difficulty level:</strong> Medium</p> <p><strong>References:</strong></p> <div class=tight-list ><ul> <li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S2211675316300367">A hierarchical clustering method for multivariate geostatistical data</a></p> <li><p><a href="https://www.sciencedirect.com/science/article/pii/S0098300415001314">Unsupervised classification of multivariate geostatistical data: Two algorithms</a></p> <li><p><a href="https://www.sciencedirect.com/science/article/pii/S0098300411004419">A density-based spatial clustering algorithm considering both spatial proximity and attribute similarity</a></p> </ul></div> <h2 id=new_geostatistical_simulation_methods ><a href="#new_geostatistical_simulation_methods">New geostatistical simulation methods</a></h2> <p>Geostatistical simulation consists of generating multiple alternative realizations of geospatial data according to a given geospatial distribution. The litetaure on simulation methods is vast, but a few of them are particularly useful.</p> <p>The goal of this project is to implement a geostatistical simulation method from the geostatistics literature using the GeoStats.jl API.</p> <p><strong>Desired skills:</strong> Geostatistics, Stochastics, HPC</p> <p><strong>Difficulty level:</strong> Hard</p> <p><strong>References:</strong></p> <div class=tight-list ><ul> <li><p><a href="https://link.springer.com/article/10.1023/A:1014009426274">Conditional Simulation of Complex Geological Structures Using Multiple-Point Statistics</a></p> <li><p><a href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2008WR007621">The Direct Sampling method to perform multiple‐point geostatistical simulations</a></p> </ul></div> <h2 id=migrate_from_plotsjl_to_makiejl_recipes ><a href="#migrate_from_plotsjl_to_makiejl_recipes">Migrate from Plots.jl to Makie.jl recipes</a></h2> <p>The project currently relies on Plots.jl recipes to visualize geospatial data sets as well as many other objects defined in the framework. However, very large data sets &#40;e.g. 3D volumes&#41; cannot be visualized easily. The Makie.jl project is a promising alternative.</p> <p>The goal of this project is to migrate all plot recipes from Plots.jl to Makie.jl.</p> <p><strong>Desired skills:</strong> Visualization, Plotting, Geometry, HPC, GPU</p> <p><strong>Difficulty level:</strong> Medium</p> <h2 id=how_to_get_started ><a href="#how_to_get_started">How to get started?</a></h2> <p>Get familiar with the framework by reading the <a href="https://juliaearth.github.io/GeoStats.jl/stable">documentation</a> and <a href="https://github.com/JuliaEarth/GeoStatsTutorials">tutorials</a>.</p> <p>Please contact the project maintainers in <a href="https://gitter.im/JuliaEarth/GeoStats.jl">Gitter</a> or <a href="https://julialang.zulipchat.com/#narrow/stream/276201-geostats.2Ejl">Zulip</a>.</p> <h1 id=gsoc_projects ><a href="#gsoc_projects">GSOC projects</a></h1> <h1 id=2021_ideas ><a href="#2021_ideas">2021 Ideas</a></h1> <h1 id=titles_possible_mentors ><a href="#titles_possible_mentors">Titles &amp; possible mentors</a></h1> <ul> <li><p><a href="#Time-series-forecasting-at-scale---speed-up-via-Julia">Time series forecasting at scale - speed up via Julia</a></p> <li><p><a href="#Interpretable-Machine-Learning-in-Julia">Interpretable Machine Learning in Julia</a></p> <li><p><a href="#Model-visualization-in-MLJ">Model visualization in MLJ</a></p> <li><p><a href="#Deeper-Bayes">Deeper integration with Bayesian methods and Bayesian Stacking</a></p> <li><p><a href="#MLJ-and-MLFlow-integration">MLJ and MLFlow integration</a></p> <li><p><a href="#Speed-demons-only-need-apply">Speed demons only need apply</a></p> </ul> <h1 id=mlj_projects_summer_of_code ><a href="#mlj_projects_summer_of_code">MLJ Projects – Summer of Code</a></h1> <p><a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a> is a machine learning framework for Julia aiming to provide a convenient way to use and combine a multitude of tools and models available in the Julia ML/Stats ecosystem.</p> <p>MLJ is released under the MIT license and sponsored by the Alan Turing Institute.</p> <h2 id=time_series_forecasting_at_scale_-_speed_up_via_julia ><a href="#time_series_forecasting_at_scale_-_speed_up_via_julia">Time series forecasting at scale - speed up via Julia</a></h2> <p>Time series are ubiquitous - stocks, sensor reading, vital signs. This projects aims at adding time series forecasting to MLJ and perform benchmark comparisons to <a href="https://github.com/alan-turing-institute/sktime">sktime</a>, <a href="https://github.com/rtavenar/tslearn">tslearn</a>, <a href="https://github.com/uea-machine-learning/tsml/">tsml</a>&#41;.</p> <p><strong>Difficulty.</strong> Easy - moderate. <strong>Duration.</strong> 350 hours.</p> <h3 id=prerequisites ><a href="#prerequisites">Prerequisites</a></h3> <ul> <li><p>Julia language fluency essential.</p> <li><p>Git-workflow essential</p> <li><p>Some prior contact with time series forecasting</p> <li><p>HPC in julia is a desirable</p> </ul> <h3 id=your_contribution ><a href="#your_contribution">Your contribution</a></h3> <p>MLJ is so far focused on tabular data and time series classification. This project is to add support for time series data in a modular, composable way.</p> <p>Time series are everywhere in real-world applications and there has been an increase in interest in time series frameworks recently &#40;see e.g. <a href="https://github.com/alan-turing-institute/sktime">sktime</a>, <a href="https://github.com/rtavenar/tslearn">tslearn</a>, <a href="https://github.com/uea-machine-learning/tsml/">tsml</a>&#41;.</p> <p>But there are still very few principled time-series libraries out there, so you would be working on something that could be very useful for a large number of people. To find out more, check out this <a href="http://learningsys.org/neurips19/assets/papers/sktime_ml_systems_neurips2019.pdf">paper</a> on sktime.</p> <p><strong>Mentors</strong>: <a href="https://www.turing.ac.uk/people/programme-directors/sebastian-vollmer">Sebastian Vollmer</a>, <a href="https://github.com/mloning">Markus Löning</a> &#40;sktime developer&#41;.</p> <h3 id=references ><a href="#references">References</a></h3> <ul> <li><p><a href="https://github.com/alan-turing-institute/sktime">sktime</a></p> <li><p><a href="https://github.com/rtavenar/tslearn">tslearn</a></p> <li><p><a href="https://github.com/uea-machine-learning/tsml/">tsml</a></p> <li><p><a href="http://learningsys.org/neurips19/assets/papers/sktime_ml_systems_neurips2019.pdf">sktime paper</a></p> </ul> <h2 id=interpretable_machine_learning_in_julia ><a href="#interpretable_machine_learning_in_julia">Interpretable Machine Learning in Julia</a></h2> <p>Interpreting and explaining black box interpretation crucial to establish trust and improve performance</p> <p><strong>Difficulty.</strong> Easy - moderate. <strong>Duration.</strong> 350 hours.</p> <h3 id=description ><a href="#description">Description</a></h3> <p>It is important to have mechanisms in place to interpret the results of machine learning models. Identify the relevant factors of a decision or scoring of a model.</p> <p>This project will implement methods for model and feature interpretability.</p> <p><strong>Mentors.</strong> <a href="https://github.com/darenasc">Diego Arenas</a>, <a href="https://www.turing.ac.uk/people/programme-directors/sebastian-vollmer">Sebastian Vollmer</a>.</p> <h3 id=prerequisites__2 ><a href="#prerequisites__2">Prerequisites</a></h3> <ul> <li><p>Julia language fluency essential.</p> <li><p>Git-workflow familiarity strongly preferred.</p> <li><p>Some prior contact with explainable AI/ML methods is desirable.</p> <li><p>A passing familiarity with machine learning goals and workflow preferred</p> </ul> <h3 id=your_contribution__2 ><a href="#your_contribution__2">Your contribution</a></h3> <p>The aim of this project is to implement multiple variants implementation algorithms such as:</p> <ul> <li><p>Implement methods to show feature importance</p> <li><p>Partial dependence plots</p> <li><p>Tree surrogate</p> <li><p>LocalModel: Local Interpretable Model-agnostic Explanations</p> <li><p>Add Dataset loaders for standard interpretability datasets.</p> <li><p>Add performance metrics for interpretability</p> <li><p>Add interpretability algorithms</p> <li><p>Glue code to SHAP package</p> </ul> <p>Specifically you will</p> <ul> <li><p>Familiarize yourself with MLJ</p> <li><p>Survey of some of the literature and existing implementations in Julia and other languages, and preparing a short summary</p> <li><p>Implement visualisations of explanations</p> <li><p>Implement use cases</p> <li><p>You will learn about the benefits and short comings of model interpretation and how to use them.</p> </ul> <h3 id=references__2 ><a href="#references__2">References</a></h3> <ul> <li><p><a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning - A Guide for Making Black Box Models Explainable by Christoph Molnalr</a></p> <li><p><a href="https://github.com/christophM/iml/">iml R package</a></p> <li><p></p> </ul> <p>Tutorials</p> <ul> <li><p><a href="https://dl.acm.org/doi/abs/10.1145/3351095.3375667">AI explainability 360: hands-on tutorial</a></p> <li><p><a href="https://mlr3book.mlr-org.com/iml.html">IML tutorial</a></p> </ul> <h2 id=model_visualization_in_mlj ><a href="#model_visualization_in_mlj">Model visualization in MLJ</a></h2> <p>Design and implement a data visualization module for MLJ.</p> <p><strong>Difficulty</strong>. Easy. <strong>Duration.</strong> 350 hours.</p> <h3 id=description__2 ><a href="#description__2">Description</a></h3> <p>Design and implement a data visualization module for MLJ to visualize numeric and categorical features &#40;histograms, boxplots, correlations, frequencies&#41;, intermediate results, and metrics generated by MLJ machines.</p> <p>Using a suitable Julia package for data visualization.</p> <p>The idea is to implement a similar resource to what <a href="https://github.com/mlr-org/mlr3viz">mlr3viz</a> does for <a href="https://mlr3.mlr-org.com">mlr3</a>.</p> <h3 id=prerequisites__3 ><a href="#prerequisites__3">Prerequisites</a></h3> <ul> <li><p>Julia language fluency essential.</p> <li><p>Git-workflow essential.</p> <li><p>Some prior work on data visualization is desirable</p> </ul> <h3 id=your_contribution__3 ><a href="#your_contribution__3">Your contribution</a></h3> <p>So far visualizing data or features in MLJ is an ad-hoc task. Defined by the user case by case. You will be implementing a standard way to visualize model performance, residuals, benchmarks and predictions for MLJ users.</p> <p>The structures and metrics will be given from the results of models or data sets used; your task will be to implement the right visualizations depending on the data type of the features.</p> <p>A relevant part of this project is to visualize the target variable against the rest of the features.</p> <p>You will enhance your visualisation skills as well as your ability to &quot;debug&quot; and understand models and their prediction visually.</p> <h3 id=references__3 ><a href="#references__3">References</a></h3> <ul> <li><p><a href="https://github.com/mlr-org/mlr3viz">mlr3viz</a></p> <li><p><a href="https://github.com/JuliaPlots/StatsPlots.jl">StatsPlots</a></p> </ul> <p><strong>Mentors</strong>: <a href="https://www.turing.ac.uk/people/programme-directors/sebastian-vollmer">Sebastian Vollmer</a>, <a href="https://github.com/darenasc">Diego Arenas</a>.</p> <h2 id=deeper_bayesian_integration ><a href="#deeper_bayesian_integration">Deeper Bayesian Integration</a></h2> <p>Bayesian methods and probabilistic supervised learning provide uncertainty quantification. This project aims increasing integration to combine Bayesian and non-Bayesian methods using Turing.</p> <p><strong>Difficulty.</strong> Difficult. <strong>Duration.</strong> 350 hours.</p> <h3 id=description__3 ><a href="#description__3">Description</a></h3> <p>As an initial step reproduce <a href="https://github.com/cscherrer/SossMLJ.jl">SOSSMLJ</a> in Turing. The bulk of the project is to implement methods that combine multiple predictive distributions.</p> <h3 id=your_contributions ><a href="#your_contributions">Your contributions</a></h3> <ul> <li><p>Interface between Turing and MLJ</p> <li><p>Comparisons of ensambling, stacking of predictive distribution</p> <li><p>reproducible benchmarks across various settings.</p> </ul> <h3 id=references__4 ><a href="#references__4">References</a></h3> <p><a href="http://www.stat.columbia.edu/~gelman/research/published/stacking_paper_discussion_rejoinder.pdf">Bayesian Stacking</a> <a href="https://github.com/alan-turing-institute/skpro/blob/master/README.md">SKpro</a></p> <h3 id=difficulty_medium_to_hard ><a href="#difficulty_medium_to_hard">Difficulty: Medium to Hard</a></h3> <p><strong>Mentors</strong>: <a href="https://github.com/yebai">Hong Ge</a> <a href="https://www.turing.ac.uk/people/programme-directors/sebastian-vollmer">Sebastian Vollmer</a></p> <h2 id=mlj_and_mlflow_integration ><a href="#mlj_and_mlflow_integration">MLJ and MLFlow integration</a></h2> <p>Integrate MLJ with <a href="https://mlflow.org">MLFlow</a>.</p> <p><strong>Difficulty.</strong> Easy. <strong>Duration.</strong> 350 hours.</p> <h3 id=description__4 ><a href="#description__4">Description</a></h3> <p>MLFlow is a flexible model management tool. The project consists of writing the necessary functions to integrate MLJ with <a href="https://mlflow.org/docs/latest/rest-api.html">MLFlow REST API</a> so models built using MLJ can keep track of its runs, evaluation metrics, parameters, and can be registered and monitored using MLFlow.</p> <h3 id=prerequisites__4 ><a href="#prerequisites__4">Prerequisites</a></h3> <ul> <li><p>Julia language fluency essential.</p> <li><p>Git-workflow familiarity strongly preferred.</p> </ul> <h3 id=your_contribution__4 ><a href="#your_contribution__4">Your contribution</a></h3> <ul> <li><p>Provide to MLJ users a way to keep track of their machine learning models using MLflow, as a local or remote server.</p> <li><p>Implement a reproducible way to store and load machine learning models.</p> <li><p>Implement functions wrapping the REST API calls that makes possible the use of MLflow.</p> </ul> <h3 id=references__5 ><a href="#references__5">References</a></h3> <ul> <li><p><a href="https://mlflow.org">MLFlow</a> website.</p> <li><p><a href="https://mlflow.org/docs/latest/rest-api.html">MLFlow REST API</a>.</p> </ul> <h2 id=speed_demons_only_need_apply ><a href="#speed_demons_only_need_apply">Speed demons only need apply</a></h2> <p>Diagnose and exploit opportunities for speeding up common MLJ workflows.</p> <p><strong>Difficulty.</strong> Moderate. <strong>Duration.</strong> 350 hours.</p> <h3 id=description__5 ><a href="#description__5">Description</a></h3> <p>In addition to investigating a number of known performance bottlenecks, you will have some free reign in this to identify opportunities to speed up common MLJ workflows, as well as making better use of memory resources.</p> <h3 id=prerequisites__5 ><a href="#prerequisites__5">Prerequisites</a></h3> <ul> <li><p>Julia language fluency essential.</p> <li><p>Experience with multi-threading and multi-processor computing essential, preferably in Julia.</p> <li><p>Git-workflow familiarity strongly preferred.</p> <li><p>Familiarity with machine learning goals and workflow preferred</p> </ul> <h3 id=your_contribution__5 ><a href="#your_contribution__5">Your contribution</a></h3> <p>In this project you will:</p> <ul> <li><p>familiarize yourself with the training, evaluation and tuning of machine learning models in MLJ</p> <li><p>work towards addressing a number of known performance issues, including:</p> <li><p>limitations of the generic Tables.jl interface for interacting with tabular data which, in common cases &#40;DataFrames&#41;, has extra functionality that can be exploited</p> <li><p>rolling out new data front-end for models to avoid unnecessary copying of data</p> <li><p>in conjunction with your mentor, identify best design for introducing better sparse data support to MLJ models &#40;e.g., naive Bayes&#41;</p> <li><p>implement a multi-threading and/or multi-processor parallelism to the current learning networks scheduler</p> <li><p>benchmark and profile common workflows to identify opportunities for further code optimizations</p> <li><p>implement some of these optimizations</p> </ul> <h3 id=references__6 ><a href="#references__6">References</a></h3> <ul> <li><p><a href="https://github.com/alan-turing-institute/MLJ.jl/blob/dev/ROADMAP.md#scalability">MLJ Roadmap</a>. See, in particular &quot;Scalability&quot; section.</p> <li><p><a href="https://github.com/alan-turing-institute/MLJBase.jl/issues/309">Taking performance more seriously GitHub issue</a></p> <li><p><a href="https://alan-turing-institute.github.io/MLJ.jl/dev/adding_models_for_general_use/#Implementing-a-data-front-end-1">Data front end</a> for MLJ models.</p> </ul> <p><strong>Mentors.</strong> <a href="https://ablaom.github.io">Anthony Blaom</a></p> <h1 id=bayesianoptimization ><a href="#bayesianoptimization">BayesianOptimization</a></h1> <p>Bayesian optimization is a global optimization strategy for &#40;potentially noisy&#41; functions with unknown derivatives. With well-chosen priors, it can find optima with fewer function evaluations than alternatives, making it well suited for the optimization of costly objective functions. Well known examples include hyper-parameter tuning of machine learning models &#40;see e.g. <a href="https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf">Taking the Human Out of the Loop: A Review of Bayesian Optimization</a>&#41;. The Julia package <a href="https://github.com/jbrea/BayesianOptimization.jl">BayesianOptimization.jl</a> currently supports only basic Bayesian optimization methods. There are multiple directions to improve the package, including &#40;but not limited to&#41;</p> <ul> <li><p><strong>Hybrid Bayesian Optimization &#40;duration: 175h, expected difficulty: medium&#41;</strong> with discrete and continuous variables. Implement e.g. <a href="https://arxiv.org/abs/2106.04682v1">HyBO</a> see also <a href="https://github.com/jbrea/BayesianOptimization.jl/issues/26">here</a>.</p> <li><p><strong>Scalable Bayesian Optimization &#40;duration: 175h, expected difficulty: medium&#41;</strong>: implement e.g. <a href="https://proceedings.neurips.cc/paper/2019/hash/6c990b7aca7bc7058f5e98ea909e924b-Abstract.html">TuRBO</a> or <a href="http://proceedings.mlr.press/v130/eriksson21a.html">SCBO</a>.</p> <li><p><strong>Better Defaults &#40;duration: 175h, expected difficulty: easy&#41;</strong>: write an extensive test suite and implement better defaults; draw inspiration from e.g. <a href="https://github.com/dragonfly/dragonfly">dragonfly</a>.</p> </ul> <p><strong>Recommended Skills:</strong> Familiarity with Bayesian inference, non-linear optimization, writing Julia code and reading Python code. <strong>Expected Outcome:</strong> Well-tested and well-documented new features. <strong>Mentor:</strong> <a href="https://github.com/jbrea">Johanni Brea</a></p> <h1 id=compiler_projects_summer_of_code ><a href="#compiler_projects_summer_of_code">Compiler Projects – Summer of Code</a></h1> <p>There are a number of compiler projects that are currently being worked on. Please contact Ian Atol or Jameson Nash for additional details and let us know what specifically interests you about this area of contribution. That way, we can tailor your project to better suit your interests and skillset.</p> <ul> <li><p><strong>Julia Optimization Passes:</strong></p> <p>The Julia compiler performs optimizations at two distinct times during native code generation: first at the &quot;Julia level&quot;, and then at the &quot;LLVM level&quot;. At the Julia level, we have some basic optimization passes &#40;inlining, basic DCE, SROA&#41;, but currently many other interesting passes simply don&#39;t yet exist, or have a partial PR but need significant effort to finish. We see potential for many future optimizations at this phase of compilation, especially with some new analyses that have been recently added. For this proposal, we can work together to define which optimizations we could tackle next.</p> </ul> <ul> <li><p><strong>LLVM:</strong> As previously mentioned, the Julia language utilizes LLVM as a backend for code generation. This means that there are plenty of opportunities for those with knowledge of or interest in LLVM to contribute via working on Julia&#39;s code generation process. Together, we can figure out an appropriate task if you would like to work in this area. Below are some LLVM-related projects that may be of interest.</p> <ul> <li><p>Investigating OrcJIT v2 improvements:</p> <p>The LLVM JIT has gained many new features. This project would involve finding out what they are and making use of them. Some examples include better resource tracking, parallel compilation, a new linker &#40;which may need upstream work too&#41;, and fine-grained tracking of relocations.</p> </ul> </ul> <ul> <li><p><strong>Parser error messages &#40;and other parts&#41;:</strong></p> <p>Error messages and infrastructure could use some work to track source locations more precisely. This may be a large project. Contact me and @c42f for more details if this interests you.</p> <li><p><strong>Macro hygiene re-implementation, to eliminate incorrect predictions inherent in current approach:</strong></p> <p>This may be a good project for someone that wants to learn lisp/scheme&#33; Our current algorithm runs in multiple passes, which means sometimes we compute the wrong scope for a variable in the earlier pass than when we assign the actual scope to each value. See <a href="https://github.com/JuliaLang/julia/labels/macros">https://github.com/JuliaLang/julia/labels/macros</a>, and particularly issues such as <a href="https://github.com/JuliaLang/julia/issues/20241">https://github.com/JuliaLang/julia/issues/20241</a> and <a href="https://github.com/JuliaLang/julia/issues/34164">https://github.com/JuliaLang/julia/issues/34164</a>.</p> <li><p><strong>Better debug information output for variables:</strong></p> <p>We have part of the infrastructure in place for representing DWARF information for our variables, but only from limited places. We could do much better since there are numerous opportunities for improvement&#33;</p> </ul> <p><strong>Recommended Skills</strong>: Most of these projects involve algorithms work, requiring a willingness and interest in seeing how to integrate with a large system.</p> <p><strong>Mentors</strong>: <a href="https://github.com/vtjnash">Jameson Nash</a>, <a href="https://github.com/ianatol">Ian Atol</a></p> <h2 id=improving_test_coverage ><a href="#improving_test_coverage">Improving test coverage</a></h2> <p>Code coverage reports very good coverage of all of the Julia Stdlib packages, but it&#39;s not complete. Additionally, the coverage tools themselves &#40;–track-coverage and <a href="https://github.com/JuliaCI/Coverage.jl">https://github.com/JuliaCI/Coverage.jl</a>&#41; could be further enhanced, such as to give better accuracy of statement coverage, or more precision. A successful project may combine a bit of both building code and finding faults in others&#39; code.</p> <p>Another related side-project might be to explore adding Type information to the coverage reports?</p> <p><strong>Recommended Skills</strong>: An eye for detail, a thrill for filing code issues, and the skill of breaking things.</p> <p><strong>Contact:</strong> <a href="https://github.com/vtjnash">Jameson Nash</a></p> <h2 id=multi-threading_improvement_projects ><a href="#multi-threading_improvement_projects">Multi-threading Improvement Projects</a></h2> <p>A few ideas to get you started, in brief:</p> <ul> <li><p>Make better use of threads for GC &#40;and particularly, make the page-allocator multi-threaded&#41;</p> <li><p>Improve granularity of codegen JIT for multi-threading</p> <li><p>Improve granularity of IO operations for multi-threading &#40;and set up a worker thread for running the main libuv event loop&#41;</p> <li><p>Measure and optimize the performance of the <code>partr</code> algorithm, and add the ability to dynamically scale it by workload size</p> <li><p>Automatic insertion of GC safe-points/regions, particularly around loops</p> <li><p>Work towards supporting a dynamic number of threads</p> </ul> <p>Join the regularly scheduled multithreading call for discussion of any of these at <a href="https://calendar.google.com/event?action&#61;TEMPLATE&amp;tmeid&#61;MzQ1MnZxMGNucGt2NGQwYW1zZjA4MzM5dGtfMjAyMTAyMTdUMTYzMDAwWiBqdWxpYWxhbmcub3JnX2tvbWF1YXFldDE0ZW9nOW9pdjNwNm83cG1nQGc&amp;tmsrc&#61;julialang.org_komauaqet14eog9oiv3p6o7pmg&#37;40group.calendar.google.com&amp;scp&#61;ALL">#multithreading BoF calendar invite</a> on the Julia Language Public Events calendar.</p> <p> <strong>Recommended Skills</strong>: Varies by project</p> <p><strong>Contact:</strong> <a href="https://github.com/vtjnash">Jameson Nash</a></p> <h2 id=automation_of_testing_performance_benchmarking ><a href="#automation_of_testing_performance_benchmarking">Automation of testing / performance benchmarking</a></h2> <p>The Nanosoldier.jl project &#40;and related <a href="https://github.com/JuliaCI/BaseBenchmarks.jl">https://github.com/JuliaCI/BaseBenchmarks.jl</a>&#41; tests for performance impacts of some changes. However, there remains many areas that are not covered &#40;such as compile time&#41; while other areas are over-covered &#40;greatly increasing the duration of the test for no benefit&#41; and some tests may not be configured appropriately for statistical power. Furthermore, the current reports are very primitive and can only do a basic pair-wise comparison, while graphs and other interactive tooling would be more valuable. Thus, there would be many great projects for a summer student to tackle here&#33;</p> <p><strong>Contact:</strong> <a href="https://github.com/vtjnash">Jameson Nash</a>, <a href="https://github.com/maleadt">Tim Besard</a></p> <h1 id=deepchemjl_development_projects_summer_of_code ><a href="#deepchemjl_development_projects_summer_of_code">DeepChem.jl development projects – Summer of Code</a></h1> <h2 id=towards_deepchemjl_combining_machine_learning_with_chemical_knowledge ><a href="#towards_deepchemjl_combining_machine_learning_with_chemical_knowledge">Towards DeepChem.jl: Combining Machine Learning with Chemical Knowledge</a></h2> <p>We have been developing the AtomicGraphNets.jl package, which began modestly as a Julia port of <a href="https://github.com/txie-93/cgcnn">CGCNN</a>, but now has plans to expand to a variety of more advanced graph-based methods for state-of-the-art ML performance making predictions on atomic systems. In support of this package, we are also developing ChemistryFeaturization.jl, which contains functions for building and featurizing atomic graphs from a variety of standard input files. ChemistryFeaturization will eventually form the bedrock of a DeepChem.jl umbrella organization to host a Julia-based port of the popular <a href="http://deepchem.io">Deepchem</a> Python package.</p> <p>Some of the features we&#39;re excited about working on include:</p> <div class=tight-list ><ul> <li><p>smarter hyperparameter optimization for built-in model types, potentially making use of Hyperopt.jl or other existing optimization packages</p> <li><p>building tools to enable sensitivity analysis along values of various input features as well as testing the importance of including those features at all</p> <li><p>implementing <a href="https://arxiv.org/abs/1905.12712">Path-Augmented Graph Transformer</a> layers</p> <li><p>allowing new types of graph features &#40;e.g. edge features, user-defined features rather than only pulling from databases, etc.&#41; and building network layers that can make use of these features</p> <li><p>building more physically-informed pooling operations</p> <li><p>Improving documentation, example sets, and building tutorials for both of these packages &#40;<a href="/jsoc/gsod/projects">see cross-posting at Julia GSoD site</a>&#41;</p> </ul></div> <p><strong>Recommended Skills</strong>: Basic graph theory and linear algebra, some knowledge of chemistry</p> <p><strong>Expected Results</strong>: Contributions of new features in the eventual DeepChem.jl ecosystem</p> <p><strong>Mentors</strong>: <a href="https://github.com/rkurchin">Rachel Kurchin</a></p> <h1 id=dftkjl_development_projects_summer_of_code ><a href="#dftkjl_development_projects_summer_of_code">DFTK.jl development projects – Summer of Code</a></h1> <h2 id=bringing_dftk_to_graphics-processing_units_gpus ><a href="#bringing_dftk_to_graphics-processing_units_gpus">Bringing DFTK to graphics-processing units &#40;GPUs&#41;</a></h2> <p>Density-functional theory &#40;DFT&#41; is probably the most widespread method for simulating the quantum-chemical behaviour of electrons in matter and applications cover a wide range of fields such as materials research, chemistry or pharmacy. For aspects like designing the batteries, catalysts or drugs of tomorrow DFT is nowadays a key building block of the ongoing research. The aim to tackle even larger and more involved systems with DFT, however, keeps posing novel challenges with respect to physical models, reliability and performance. For tackling these aspects in the multidisciplinary context of DFT we recently started the <a href="https://dftk.org">density functional toolkit &#40;DFTK&#41;</a>, a DFT package written in pure Julia.</p> <p>Employing GPUs to bring speed improvements to DFT simulations is an established idea. However, in state-of-the-art DFT simulation packages the GPU version of the solution algorithm is usually implemented in a separate code base. In other words the CPU and the GPU version co-exist, which has the drawback of the duplicated effort to fix bugs or for keeping both code bases in sync whenever a novel method or algorithm becomes available. Since conventional GPU programming frameworks feature a steep learning curve for newcomers, oftentimes the GPU version is lagging behind and features an increased code complexity making the investigation of novel GPU algorithms challenging.</p> <p>In this project we want to build on the extensive GPU programming capabilities of the Julia ecosystem to enable DFTK to offload computations to a local GPU. Key aim will be to minimise the code which needs to be adapted from the present CPU code base in DFTK to achieve this. Since GPU counterparts already exist for most computational bottlenecks of a DFT computation, the key challenge of this project will be to handle the overall orchestration of the computational workflow as well as the data transfer between the CPU and the GPU. To keep the task manageable we will not directly tackle the full DFT problem &#40;a non-linear eigenvalue problem&#41;, but restrict ourselves to the reduced setting of linear eigenvalue problems. Expanding from there towards the full DFT is an optional stretch goal of the project.</p> <p><strong>Level of difficulty:</strong> Medium to difficult</p> <p><strong>Project size:</strong> large, i.e. 12 weeks a 30 hours</p> <p><strong>Recommended skills:</strong> Interest to work on an multidisciplinary project bordering physics, mathematics and computer science with a good working knowledge of numerical linear algebra and Julia. Detailed knowledge in the physical background &#40;electrostatics, material science&#41; or about GPU programming is not required, but be prepared to take a closer look at these domains during the project.</p> <p><strong>Expected results:</strong> Use Julias GPU programming ecosystem to implement an algorithm for solving the type of eigenvalue problems arising in density-functional theory.</p> <p><strong>Mentors:</strong> Valentin Churavy, Michael F. Herbst, Antoine Levitt</p> <p><strong>References:</strong> For a nice intro to DFT and DFTK.jl see <a href="https://www.youtube.com/watch?v&#61;-RomkxjlIcQ">Michael&#39;s talk at JuliaCon 2020</a> and the literature given in the <a href="https://docs.dftk.org/stable/guide/density_functional_theory/">DFTK documentation</a>. For an introduction to GPU computing in Julia, see <a href="https://www.youtube.com/watch?v&#61;Hz9IMJuW5hU">the GPU workshop at JuliaCon 2021</a> by Tim Besard, Julian Samaroo and Valentin.</p> <p><strong>Contact:</strong> For any questions, feel free to email <a href="https://github.com/mfherbst">@mfherbst</a>, <a href="https://github.com/antoine-levitt">@antoine-levitt</a> or write us on the <a href="https://join.slack.com/t/juliamolsim/shared_invite/zt-tc060co0-HgiKApazzsQzBHDlQ58A7g">JuliaMolSim slack</a>.</p> <h1 id=numerical_differential_equations_projects_summer_of_code ><a href="#numerical_differential_equations_projects_summer_of_code">Numerical Differential Equations Projects – Summer of Code</a></h1> <h2 id=native_julia_ode_sde_dae_dde_and_spde_solvers ><a href="#native_julia_ode_sde_dae_dde_and_spde_solvers">Native Julia ODE, SDE, DAE, DDE, and &#40;S&#41;PDE Solvers</a></h2> <p>The DifferentialEquations.jl ecosystem has an extensive set of state-of-the-art methods for solving differential equations hosted by the <a href="https://sciml.ai/">SciML Scientific Machine Learning Software Organization</a>. By mixing native methods and wrapped methods under the same dispatch system, <a href="https://arxiv.org/abs/1807.06430">DifferentialEquations.jl serves both as a system to deploy and research the most modern efficient methodologies</a>. While most of the basic methods have been developed and optimized, many newer methods need high performance implementations and real-world tests of their efficiency claims. In this project students will be paired with current researchers in the discipline to get a handle on some of the latest techniques and build efficient implementations into the &#42;DiffEq libraries &#40;OrdinaryDiffEq.jl, StochasticDiffEq.jl, DelayDiffEq.jl&#41;. Possible families of methods to implement are:</p> <div class=tight-list ><ul> <li><p>Global error estimating ODE solvers</p> <li><p>Implicit-Explicit &#40;IMEX&#41; Methods</p> <li><p>Geometric &#40;exponential&#41; integrators</p> <li><p>Low memory Runge-Kutta methods</p> <li><p>Multistep methods specialized for second order ODEs &#40;satellite simulation&#41;</p> <li><p>Parallel &#40;multithreaded&#41; extrapolation &#40;both explicit and implicit&#41;</p> <li><p>Parallel Implicit Integrating Factor Methods &#40;PDEs and SPDEs&#41;</p> <li><p>Parallel-in-time ODE Methods</p> <li><p>Rosenbrock-W methods</p> <li><p>Approximate matrix factorization</p> <li><p>Runge-Kutta-Chebyshev Methods &#40;high stability RK methods&#41;</p> <li><p>Fully Implicit Runge-Kutta &#40;FIRK&#41; methods</p> <li><p>Anderson Acceleration</p> <li><p>Boundary value problem &#40;BVP&#41; solvers like MIRK and collocation methods</p> <li><p>BDF methods for differential-algebraic equations &#40;DAEs&#41;</p> <li><p>Methods for stiff stochastic differential equations</p> </ul></div> <p>Many of these methods are the basis of high-efficiency partial differential equation &#40;PDE&#41; solvers and are thus important to many communities like computational fluid dynamics, mathematical biology, and quantum mechanics.</p> <p>This project is good for both software engineers interested in the field of numerical analysis and those students who are interested in pursuing graduate research in the field.</p> <p><strong>Recommended Skills</strong>: Background knowledge in numerical analysis, numerical linear algebra, and the ability &#40;or eagerness to learn&#41; to write fast code.</p> <p><strong>Expected Results</strong>: Contributions of production-quality solver methods.</p> <p><strong>Mentors</strong>: <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a></p> <h2 id=improvements_to_physics-informend_neural_networks_pinn_for_solving_differential_equations ><a href="#improvements_to_physics-informend_neural_networks_pinn_for_solving_differential_equations">Improvements to Physics-Informend Neural networks &#40;PINN&#41; for solving differential equations</a></h2> <p>Neural networks can be used as a method for efficiently solving difficult partial differential equations. Efficient implementations of physics-informed machine learning from recent papers are being explored as part of the <a href="https://github.com/SciML/NeuralPDE.jl">NeuralPDE.jl</a> package. The <a href="https://github.com/SciML/NeuralPDE.jl/issues">issue tracker</a> contains links to papers which would be interesting new neural network based methods to implement and benchmark against classical techniques.</p> <p><strong>Recommended Skills</strong>: Background knowledge in numerical analysis and machine learning.</p> <p><strong>Expected Results</strong>: New neural network based solver methods.</p> <p><strong>Mentors</strong>: <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a></p> <h2 id=performance_enhancements_for_differential_equation_solvers ><a href="#performance_enhancements_for_differential_equation_solvers">Performance enhancements for differential equation solvers</a></h2> <p>Wouldn&#39;t it be cool to have had a part in the development of widely used efficient differential equation solvers? DifferentialEquations.jl has a wide range of existing methods and <a href="https://github.com/SciML/DiffEqBenchmarks.jl">an extensive benchmark suite</a> which is used for tuning the methods for performance. Many of its methods are already the fastest in their class, but there is still a lot of performance enhancement work that can be done. In this project you can learn the details about a wide range of methods and dig into the optimization of the algorithm&#39;s strategy and the implementation in order to improve benchmarks. Projects that could potentially improve the performance of the full differential equations ecosystem include:</p> <div class=tight-list ><ul> <li><p>Alternative adaptive stepsize techniques and step optimization</p> <li><p>Pointer swapping tricks</p> <li><p>Quasi-Newton globalization and optimization</p> <li><p>Cache size reductions</p> <li><p>Enhanced within-method multithreading, distributed parallelism, and GPU usage</p> <li><p>Improved automated method choosing</p> <li><p>Adaptive preconditioning on large-scale &#40;PDE&#41; discretizations</p> </ul></div> <p><strong>Recommended Skills</strong>: Background knowledge in numerical analysis, numerical linear algebra, and the ability &#40;or eagerness to learn&#41; to write fast code.</p> <p><strong>Expected Results</strong>: Improved benchmarks to share with the community.</p> <p><strong>Mentors</strong>: <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a></p> <h2 id=discretizations_of_partial_differential_equations ><a href="#discretizations_of_partial_differential_equations">Discretizations of partial differential equations</a></h2> <p>There are two ways to approach libraries for partial differential equations &#40;PDEs&#41;: one can build &quot;toolkits&quot; which enable users to discretize any PDE but require knowledge of numerical PDE methods, or one can build &quot;full-stop&quot; PDE solvers for specific PDEs. There are many different ways solving PDEs could be approached, and here are some ideas for potential projects:</p> <div class=tight-list ><ol> <li><p>Automated PDE discretization tooling. We want users to describe a PDE in its mathematical form and automate the rest of the solution process. See <a href="https://github.com/SciML/DifferentialEquations.jl/issues/469">this issue for details</a>.</p> <li><p>Enhancement of existing tools for discretizing PDEs. The finite differencing &#40;FDM&#41; library <a href="https://github.com/SciML/DiffEqOperators.jl">DiffEqOperators.jl</a> could be enhanced to allow non-uniform grids or composition of operators. The finite element method &#40;FEM&#41; library <a href="https://github.com/SciML/FEniCS.jl">FEniCS.jl</a> could wrap more of the FEniCS library.</p> <li><p>Full stop solvers of common fluid dynamical equations, such as diffusion-advection-convection equations, or of hyperbolic PDEs such as the Hamilton-Jacobi-Bellman equations would be useful to many users.</p> <li><p>Using stochastic differential equation &#40;SDE&#41; solvers to efficiently &#40;and highly parallel&#41; approximate certain PDEs.</p> <li><p>Development of ODE solvers for more efficiently solving specific types of PDE discretizations. See the &quot;Native Julia solvers for ordinary differential equations&quot; project.</p> </ol></div> <p><strong>Recommended Skills</strong>: Background knowledge in numerical methods for solving differential equations. Some basic knowledge of PDEs, but mostly a willingness to learn and a strong understanding of calculus and linear algebra.</p> <p><strong>Expected Results</strong>: A production-quality PDE solver package for some common PDEs.</p> <p><strong>Mentors</strong>: <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a></p> <h2 id=tools_for_global_sensitivity_analysis ><a href="#tools_for_global_sensitivity_analysis">Tools for global sensitivity analysis</a></h2> <p>Global Sensitivity Analysis is a popular tool to assess the effect that parameters have on a differential equation model. A good introduction <a href="https://discovery.ucl.ac.uk/id/eprint/19896/">can be found in this thesis</a>. Global Sensitivity Analysis tools can be much more efficient than Local Sensitivity Analysis tools, and give a better view of how parameters affect the model in a more general sense. The goal of this project would be to implement more global sensitivity analysis methods like the eFAST method into <a href="https://github.com/SciML/DiffEqSensitivity.jl">DiffEqSensitivity.jl</a> which can be used with any differential equation solver on the common interface.</p> <p><strong>Recommended Skills</strong>: An understanding of how to use DifferentialEquations.jl to solve equations.</p> <p><strong>Expected Results</strong>: Efficient functions for performing global sensitivity analysis.</p> <p><strong>Mentors</strong>: <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a>, <a href="https://github.com/Vaibhavdixit02">Vaibhav Dixit</a></p> <h2 id=parameter_identifiability_analysis ><a href="#parameter_identifiability_analysis">Parameter identifiability analysis</a></h2> <p>Parameter identifiability analysis is an analysis that describes whether the parameters of a dynamical system can be identified from data or whether they are redundant. There are two forms of identifiability analysis: structural and practical. Structural identifiability analysis relates changes in the solution of the ODE directly to other parameters, showcasing that it is impossible to distinguish between parameter A being higher and parameter B being lower, or the vice versa situation, given only data about the solution because of how the two interact. This could be done directly on the symbolic form of the equation as part of <a href="https://github.com/SciML/ModelingToolkit.jl">ModelingToolkit.jl</a>. Meanwhile, practical identifiability analysis looks as to whether the parameters are non-identifiable in a practical sense, for example if two parameters are numerically indistinguishable &#40;given possibly noisy data&#41;. In this case, numerical techniques being built in DiffEqSensitivity.jl, such as a <a href="https://github.com/SciML/DiffEqSensitivity.jl/issues/109">nonlinear likelihood profiler</a> or an <a href="https://github.com/SciML/DiffEqSensitivity.jl/issues/108">information sensitivity measure</a> can be used to showcase whether a parameter has a unique enough effect to be determined.</p> <p><strong>Recommended Skills</strong>: A basic background in differential equations and the ability to use numerical ODE solver libraries. Background in the numerical analysis of differential equation solvers is not required.</p> <p><strong>Expected Results</strong>: Efficient and high-quality implementations of parameter identifiability methods.</p> <p><strong>Mentors</strong>: <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a></p> <h2 id=model_order_reduction ><a href="#model_order_reduction">Model Order Reduction</a></h2> <p>Model order reduction is a technique for automatically finding a small model which approximates the large model but is computationally much cheaper. We plan to use the infrastructure built by ModelingToolkit.jl to <a href="https://github.com/SciML/ModelingToolkit.jl/issues/58">implement a litany of methods</a> and find out the best way to accelerate differential equation solves.</p> <p><strong>Recommended Skills</strong>: A basic background in differential equations and the ability to use numerical ODE solver libraries. Background in the numerical analysis of differential equation solvers is not required.</p> <p><strong>Expected Results</strong>: Efficient and high-quality implementations of model order reduction methods.</p> <p><strong>Mentors</strong>: <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a></p> <h2 id=automated_symbolic_manipulations_of_differential_equation_systems ><a href="#automated_symbolic_manipulations_of_differential_equation_systems">Automated symbolic manipulations of differential equation systems</a></h2> <p>Numerically solving a differential equation can be difficult, and thus it can be helpful for users to simplify their model before handing it to the solver. Alas this takes time... so let&#39;s automate it&#33; <a href="https://github.com/SciML/ModelingToolkit.jl">ModelingToolkit.jl</a> is a project for automating the model transformation process. Various parts of the library are still open, such as:</p> <div class=tight-list ><ul> <li><p>Support for DAEs, DDEs, and SDEs</p> <li><p>Pantelides algorithm for DAE index reduction</p> <li><p>Lamperti transforms</p> <li><p>Automatic construction of adjoint solutions</p> <li><p>Tearing in nonlinear solvers</p> <li><p>Solving distributed delay equations</p> </ul></div> <p><strong>Recommended Skills</strong>: A basic background in differential equations and the ability to use numerical ODE solver libraries. Background in the numerical analysis of differential equation solvers is not required.</p> <p><strong>Expected Results</strong>: Efficient and high-quality implementations of model transformation methods.</p> <p><strong>Mentors</strong>: <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a></p> <h1 id=documentation_tooling ><a href="#documentation_tooling">Documentation tooling</a></h1> <h2 id=documenterjl ><a href="#documenterjl">Documenter.jl</a></h2> <p>The Julia manual and the documentation for a large chunk of the ecosystem is generated using <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> – essentially a static site generator that integrates with Julia and its docsystem. There are tons of opportunities for improvements for anyone interested in working on the interface of Julia, documentation and various front-end technologies &#40;web, LaTeX&#41;.</p> <ul> <li><p><strong>ElasticSearch-based search backend for Documenter.</strong> Loading the search page of Julia manual is slow because the index is huge and needs to be downloaded and constructed on the client side on every page load. Instead, we should look at hosting the search server-side. Goal is to continue the work done during a MLH fellowship for implementing an <a href="https://www.elastic.co/">ElasticSearch</a>-based search backend.</p> <li><p><strong>Improve the generated PDF in the PDF/LaTeX backend.</strong> The goals is to improve the look of the generated PDF and make sure backend works reliably &#40;improved testing&#41;. See <a href="https://github.com/JuliaDocs/Documenter.jl/issues/949">#949</a>, <a href="https://github.com/JuliaDocs/Documenter.jl/issues/1342">#1342</a> and <a href="https://github.com/JuliaDocs/Documenter.jl/labels/Format&#37;3A&#37;20LaTeX">other related issues</a>.</p> </ul> <p><strong>Recommended skills:</strong> Basic knowledge of web-development &#40;JS, CSS, HTML&#41; or LaTeX, depending on the project.</p> <p><strong>Mentors:</strong> <a href="https://github.com/mortenpi">Morten Piibeleht</a></p> <h2 id=docsystem_api ><a href="#docsystem_api">Docsystem API</a></h2> <p>Julia supports docstrings – inline documentation which gets parsed together with the code and can be accessed dynamically in a Julia session &#40;e.g. via the REPL <code>?&gt;</code> help mode; implemented mostly in the <a href="https://github.com/JuliaLang/julia/tree/master/base/docs">Docs module</a>&#41;.</p> <p>Not all docstrings are created equal however. There are bugs in Julia&#39;s docsystem code, which means that some docstrings do not get stored or are stored with the wrong key &#40;parametric methods&#41;. In addition, the API to fetch and work with docstrings programmatically is not documented, not considered public and could use some polishing.</p> <div class=tight-list ><ul> <li><p>Create a package which would provide a clean up the API for working with docstrings, and abstract away the implementation details &#40;and potential differences between Julia versions&#41; of the docsystem in Base.</p> <li><p>Fix as many docsystem-related bugs in the Julia core as possible &#91;<a href="http://mortenpi.eu/gsoc2019/latest/notes/docsystem-internals/#Docsystem-bugs-1">further reading</a>, <a href="https://github.com/JuliaLang/julia/issues/16730">#16730</a>, <a href="https://github.com/JuliaLang/julia/issues/29437">#29437</a>, <a href="https://github.com/JuliaDocs/Documenter.jl/issues/558">JuliaDocs/Documenter.jl#558</a>&#93;</p> </ul></div> <p><strong>Recommended skills:</strong> Basic familiarity with Julia is sufficient.</p> <p><strong>Mentors:</strong> <a href="https://github.com/mortenpi">Morten Piibeleht</a></p> <h1 id=fluxml_projects_-_summer_of_code ><a href="#fluxml_projects_-_summer_of_code">FluxML Projects - Summer of Code</a></h1> <p>Flux usually takes part in <a href="https://summerofcode.withgoogle.com">Google Summer of Code</a> as a NumFocus organization. We follow the same <a href="/jsoc/projects/">rules and application guidelines</a> as Julia, so please check there for more information on applying. Below are a set of ideas for potential projects &#40;though you are welcome to explore anything you are interested in&#41;.</p> <p>Flux projects are typically very competitive; we encourage you to get started early, as successful students typically have early PRs or working prototypes as part of the application. It is a good idea to simply start contributing via issue discussion and PRs and let a project grow from there; you can take a look at <a href="https://github.com/FluxML/Flux.jl/issues?q&#61;is&#37;3Aopen&#43;is&#37;3Aissue&#43;label&#37;3A&#37;22help&#43;wanted&#37;22">this list of issues</a> for some starter contributions.</p> <h3 id=metalheadjl_developement ><a href="#metalheadjl_developement">Metalhead.jl Developement</a></h3> <p><strong>Difficulty:</strong> Medium</p> <p>Help us improve <a href="https://github.com/FluxML/Metalhead.jl">Metalhead.jl</a> by adding new models, porting pre-trained weights, and extending the model interfaces to make them more customizable.</p> <p><strong>Skills:</strong> Familiarity with vision model architectures and Flux.jl</p> <p><strong>Mentors:</strong> <a href="https://github.com/darsnack">Kyle Daruwalla</a></p> <h3 id=fastaijl_time_series_development ><a href="#fastaijl_time_series_development">FastAI.jl Time Series Development</a></h3> <p><strong>Difficulty:</strong> Medium</p> <p>In this project, you will assist the <a href="https://julialang.zulipchat.com/#narrow/stream/237432-ml-ecosystem-coordination">ML community team</a> with building time series methods for FastAI.jl on top of the existing JuliaML &#43; FluxML ecosystem packages. This will require building the models/learners, documenting them, and creating the appropriate tutorials. Some familiarity with the following Julia packages is preferred, but it is not required:</p> <div class=tight-list ><ul> <li><p><a href="https://github.com/JuliaML/MLDataPattern.jl.git">MLDataPattern.jl</a></p> <li><p><a href="https://github.com/lorenzoh/FluxTraining.jl.git">FluxTraining.jl</a></p> <li><p><a href="https://github.com/lorenzoh/DataAugmentation.jl">DataAugmentation.jl</a></p> </ul></div> <p><strong>Skills:</strong> Familiarity with deep learning pipelines, common practices, Flux.jl, and recurrent neural networks</p> <p><strong>Mentors:</strong> <a href="https://github.com/lorenzoh">Lorenz Ohly</a>, <a href="https://github.com/darsnack">Kyle Daruwalla</a>, <a href="https://github.com/ToucheSir">Brian Chen</a></p> <h3 id=fastaijl_text_development ><a href="#fastaijl_text_development">FastAI.jl Text Development</a></h3> <p><strong>Difficulty:</strong> Medium</p> <p>In this project, you will assist the <a href="https://julialang.zulipchat.com/#narrow/stream/237432-ml-ecosystem-coordination">ML community team</a> with building text methods for FastAI.jl on top of the existing JuliaML &#43; FluxML ecosystem packages. This will require building the models/learners, documenting them, and creating the appropriate tutorials. Some familiarity with the following Julia packages is preferred, but it is not required:</p> <div class=tight-list ><ul> <li><p><a href="https://github.com/JuliaML/MLDataPattern.jl.git">MLDataPattern.jl</a></p> <li><p><a href="https://github.com/lorenzoh/FluxTraining.jl.git">FluxTraining.jl</a></p> <li><p><a href="https://github.com/JuliaText">JuliaText</a></p> </ul></div> <p><strong>Skills:</strong> Familiarity with deep learning pipelines, common practices, Flux.jl, and JuliaText</p> <p><strong>Mentors:</strong> <a href="https://github.com/lorenzoh">Lorenz Ohly</a>, <a href="https://github.com/darsnack">Kyle Daruwalla</a>, <a href="https://github.com/ToucheSir">Brian Chen</a></p> <h3 id=differentiable_computer_vision_hard ><a href="#differentiable_computer_vision_hard">Differentiable Computer Vision &#91;HARD&#93;</a></h3> <p>Expected Outcome:</p> <p>Create a library of utility functions that can consume Julia&#39;s Imaging libraries to make them differentiable. With Zygote.jl, we have the platform to take a general purpose package and apply automatic differentiation to it. This project is motivated to use existing libraries that offer perform computer vision tasks, and augment them with AD to perform tasks such as homography regression.</p> <p>Skills: Familiarity with automatic differentiation, deep learning, and defining &#40;a lot of&#41; Custom Adjoints</p> <p>Mentors: <a href="https://github.com/DhairyaLGandhi/">Dhairya Gandhi</a></p> <h3 id=ferminets_generative_synthesis_for_automating_the_choice_of_neural_architectures ><a href="#ferminets_generative_synthesis_for_automating_the_choice_of_neural_architectures">FermiNets: Generative Synthesis for Automating the Choice of Neural Architectures</a></h3> <p><strong>Difficulty:</strong> Hard</p> <p>The application of machine learning requires an understanding a practitioner to optimize a neural architecture for a given problem, or does it? Recently techniques in automated machine learning, also known as AutoML, have dropped this requirement by allowing for good architectures to be found automatically. One such method is the <a href="https://arxiv.org/abs/1809.05989">FermiNet</a> which employs generative synthesis to give a neural architecture which respects certain operational requirements. The goal of this project is to implement the FermiNet in Flux to allow for automated synthesis of neural networks.</p> <p><strong>Mentors:</strong> <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a> and <a href="https://github.com/DhairyaLGandhi/">Dhairya Gandhi</a></p> <h3 id=differentiable_rendering_hard ><a href="#differentiable_rendering_hard">Differentiable Rendering &#91;HARD&#93;</a></h3> <p>Expected Outcome: This is motivated to create SoftRasterizer/DiB-R based projects. We already have RayTracer.jl which is motivated by OpenDR. &#40;Of course, if someone wants to implement NERF - like models they are most welcome to submit a proposal&#41;. We would ideally target at least 2 of these models.</p> <p><strong>Skills:</strong> GPU Programming, Deep Learning, &#40;deep&#41; familiarity with the literature, familiarity with defining &#40;a lot of&#41; Custom Adjoints</p> <p><strong>Mentors:</strong> <a href="https://github.com/DhairyaLGandhi/">Dhairya Gandhi</a>, <a href="https://github.com/jpsamaroo">Julian Samaroo</a>, <a href="https://github.com/avik-pal">Avik Pal</a></p> <h2 id=deep_learning_for_source_code_analysis ><a href="#deep_learning_for_source_code_analysis">Deep Learning for source code analysis</a></h2> <p><strong>Difficulty</strong>: Easy to Medium</p> <p>The use of deep learning tools to source code is an active area of research. With the runtime being able to easily introspect into Julia code &#40;for example, with a clean, accessible AST format&#41;, using theses techniques on Julia code would be a fruitful exercise.</p> <div class=tight-list ><ul> <li><p>Use of RNNs for syntax error correction: https://arxiv.org/abs/1603.06129</p> <li><p>Implement Code2Vec for Julia: https://arxiv.org/abs/1803.09473</p> </ul></div> <p><strong>Skills:</strong> Familiarity with compiler techniques as well as deep learning tools will be required. The &quot;domain expertise&quot; in this task is Julia programming, so it will need someone who has a reasonable experience of the Julia programming language.</p> <p><strong>Expected Outcome:</strong> Packages for each technique that is usable by general programmers.</p> <p><strong>Mentors</strong>: <a href="https://github.com/aviks/">Avik Sengupta</a></p> <h1 id=graph_neural_networks_-_summer_of_code ><a href="#graph_neural_networks_-_summer_of_code">Graph Neural Networks - Summer of Code</a></h1> <p>Graph neural networks &#40;GNN&#41; are deep learning models well adapated to data given in the form of graphs and feature vectors associated to nodes and edges. GNNs are a growing are of research and find many applications in complex networks analysis, relational reasoning, combinatorial optimization, molecule generation, and many other fields. <a href="https://github.com/CarloLucibello/GraphNeuralNetworks.jl">GraphNeuralNetworks.jl</a> is a pure Julia package for GNNs equipped with features such as implementations of common graph convolutional layers, CUDA support and graph batching for fast parallel operations. There are a number of ways by which the package could be improved:</p> <ul> <li><p><strong>Adding graph convolutional layers &#40;duration: 175h, expected difficulty: easy to medium&#41;:</strong> While we implement a good variety of graph convolutional layers, there is still a vast zoology to be implemented yet.</p> <li><p><strong>Adding models and examples &#40;duration: 175h, expected difficulty: medium&#41;:</strong> As part of the documentation and for bootstrapping new projects, </p> </ul> <p>we want to add fully worked out examples and applications of graph neural networks.</p> <ul> <li><p><strong>Adding graph datasets &#40;duration: 175h, expected difficulty: easy&#41;:</strong> Provide julia friendly wrappers for common graph datasets in <a href="https://github.com/JuliaML/MLDatasets.jl"><code>MLDatasets.jl</code></a>.</p> <li><p><strong>Supporting heterogeneous graphs &#40;duration: 175h, expected difficulty: hard&#41;:</strong> In some complex networks, the relations expressed by edges can be of different types.</p> </ul> <p>We need to implement an heteroeneous graph type and implement convolutional layers supporting them. </p> <ul> <li><p><strong>Training on very large graphs &#40;duration: 175h, expected difficulty: medium to hard&#41;:</strong> Graph containing several milions of nodes are too large for gpu memory. Mini-batch training si performed on subgraphs, as in the GraphSAGE algorithm.</p> <li><p><strong>Supporting temporal graph neural networks &#40;duration: 350h, expected difficulty: hard&#41;:</strong> We aim at implementing temporal graph convolutions for time-varying graph and/or node features.</p> <li><p><strong>Improving perfomance using sparse linear algebra &#40;duration: 175h, expected difficulty: medium to hard&#41;:</strong> Many graph convolutional layers can be expressed as non-materializing algebraic operations involving the adjacency matrix instead of the slower and more memory consuming gather/scatter mechanism. We aim at extending as far as possible and in a gpu-friendly way these <em>fused</em> implementation.</p> </ul> <p><strong>Recommended skills:</strong> Familiarity with graph neural networks and Flux.jl.</p> <p><strong>Expected results:</strong> New features added to the package along with tests and relevant documentation.</p> <p><strong>Mentors:</strong> <a href="https://github.com/CarloLucibello">Carlo Lucibello</a> &#40;author of <a href="https://github.com/CarloLucibello/GraphNeuralNetworks.jl">GraphNeuralNetworks.jl</a>&#41;. For linear algebra, co-mentoring by <a href="https://github.com/Wimmerer">Will Kimmerer</a> &#40;lead developer of <a href="https://github.com/JuliaSparse/SuiteSparseGraphBLAS.jl">SuiteSparseGraphBLAS.jl</a>&#41;.</p> <p><strong>Contact:</strong> Feel free to contact us on the <a href="https://julialang.slack.com/">Julia Slack Workspace</a> or by opening an issue in the GitHub repo.</p> <h1 id=gui_projects_summer_of_code ><a href="#gui_projects_summer_of_code">GUI projects – Summer of Code</a></h1> <h2 id=qml_and_makie_integration ><a href="#qml_and_makie_integration">QML and Makie integration</a></h2> <p>The <a href="https://github.com/barche/QML.jl">QML.jl</a> package provides Julia bindings for <a href="https://doc.qt.io/qt-5/qtqml-index.html">Qt QML</a> on Windows, OS X and Linux. In the current state, basic GUI functionality exists, and rough integration with <a href="https://github.com/JuliaPlots/Makie.jl">Makie.jl</a> is available, allowing overlaying QML GUI elements over Makie visualizations.</p> <h3 id=expected_results ><a href="#expected_results">Expected results</a></h3> <ol> <li><p><em>Split off the QML code for Makie into a separate package.</em> This will allow specifying proper package compatibility between QML and Makie, without making Makie a mandatory dependency for QML &#40;currently we use <a href="https://github.com/JuliaPackaging/Requires.jl">Requires.jl</a> for that&#41;</p> <li><p><em>Improve the integration.</em> Currently, connections between Makie and QML need to be set up mostly manually. We need to implement some commonly used functionality, such as the registration of clicks in a viewport with proper coordinate conversion and navigation of 3D viewports.</p> </ol> <p><strong>Recommended Skills</strong>: Familiarity with both Julia and the Qt framework, some basic C&#43;&#43; skills, affinity with 3D graphics and OpenGL.</p> <p><strong>Duration: 175h, expected difficulty: medium</strong></p> <p><strong>Mentors</strong>: <a href="https://github.com/barche">Bart Janssens</a> and <a href="https://github.com/SimonDanisch">Simon Danish</a></p> <h2 id=web_apps_in_makie_and_jsserve ><a href="#web_apps_in_makie_and_jsserve">Web apps in Makie and JSServe</a></h2> <p><a href="https://github.com/JuliaPlots/Makie.jl">Makie.jl</a> is a visualization ecosystem for the Julia programming language, with a focus on interactivity and performance. <a href="https://github.com/SimonDanisch/JSServe.jl">JSServe.jl</a> is the core infrastructure library that makes Makie&#39;s web-based backend possible.</p> <p>At the moment, all the necessary ingredients exist for designing web-based User Interfaces &#40;UI&#41; in Makie, but the process itself is quite low-level and time-consuming. The aim of this project is to streamline that process via the following steps:</p> <ul> <li><p>implementing novel UI components and refining existing ones,</p> <li><p>introducing data structures suitable for representing complex UIs,</p> <li><p>adding simpler syntaxes for common scenarios, akin to Interact&#39;s <a href="https://github.com/JuliaGizmos/Interact.jl#manipulate"><code>@manipulate</code></a> macro,</p> <li><p>improving documentation and tutorials,</p> <li><p>streamlining the deployment process.</p> </ul> <p><strong>Bonus tasks.</strong> If time allows, one of the following directions could be pursued.</p> <ol> <li><p>Making Makie web-based plots more suitable for general web apps &#40;move more computation to the client side, improve interactivity and responsiveness&#41;.</p> <li><p>Generalize the UI infrastructure to native widgets, which are already implemented in Makie but with a different interface.</p> </ol> <p><strong>Desired skills.</strong> Familiarity with HTML, JavaScript, and CSS, as well as reactive programming. Experience with the Julia visualization and UI ecosystem.</p> <p><strong>Duration.</strong> 350h.</p> <p><strong>Difficulty.</strong> Medium.</p> <p><strong>Mentors.</strong> <a href="https://github.com/piever">Pietro Vertechi</a> and <a href="https://github.com/SimonDanisch">Simon Danisch</a>.</p> <h1 id=high_performance_and_parallel_computing_projects_summer_of_code ><a href="#high_performance_and_parallel_computing_projects_summer_of_code">High Performance and Parallel Computing Projects – Summer of Code</a></h1> <p>Julia is emerging as a serious tool for technical computing and is ideally suited for the ever-growing needs of big data analytics. This set of proposed projects addresses specific areas for improvement in analytics algorithms and distributed data management.</p> <p><strong>Difficulty:</strong> Medium</p> <h2 id=scheduling_algorithms_for_distributed_algorithms ><a href="#scheduling_algorithms_for_distributed_algorithms">Scheduling algorithms for Distributed algorithms</a></h2> <p>Dagger.jl is a native Julia framework and scheduler for distributed execution of Julia code and general purpose data parallelism, using dynamic, runtime-generated task graphs which are flexible enough to describe multiple classes of parallel algorithms. This project proposes to implement different scheduling algorithms for Dagger to optimize scheduling of certain classes of distributed algorithms, such as MapReduce and MergeSort, and properly utilizing heterogeneous compute resources. Students will be expected to find published distributed scheduling algorithms and implement them on top of the Dagger framework, benchmarking scheduling performance on a variety of micro-benchmarks and real problems.</p> <p>Mentors: <a href="https://github.com/jpsamaroo">Julian Samaroo</a>, <a href="https://github.com/vchuravy">Valentin Churavy</a></p> <h2 id=distributed_training ><a href="#distributed_training">Distributed Training</a></h2> <p><strong>Difficulty:</strong> Hard</p> <p>Add a distributed training API for Flux models built on top of <a href="https://github.com/JuliaParallel/Dagger.jl">Dagger.jl</a>. More detailed milestones include building Dagger.jl abstractions for <a href="https://github.com/JuliaParallel/UCX.jl">UCX.jl</a>, then building tools to map Flux models into data parallel Dagger DAGs. The final result should demonstrate a Flux model training with multiple devices in parallel via the Dagger.jl APIs. A stretch goal will include mapping operations with a model to a DAG to facilitate model parallelism as well.</p> <p><strong>Skills:</strong> Familiarity with UCX, representing execution models as DAGs, Flux.jl, and data/model parallelism in machine learning</p> <p><strong>Mentors:</strong> <a href="https://github.com/darsnack">Kyle Daruwalla</a>, <a href="https://github.com/jpsamaroo">Julian Samaroo</a>, and <a href="https://github.com/ToucheSir">Brian Chen</a></p> <h1 id=juliaimages_projects_summer_of_code ><a href="#juliaimages_projects_summer_of_code">JuliaImages Projects – Summer of Code</a></h1> <p><a href="https://github.com/JuliaImages">JuliaImages</a> &#40;see the <a href="https://juliaimages.github.io">documentation</a>&#41; is a framework in Julia for multidimensional arrays, image processing, and computer vision &#40;CV&#41;. It has an active development community and offers many features that unify CV and biomedical 3D/4D image processing, support big data, and promote interactive exploration.</p> <p>Often the best ideas are the ones that candidate SoC students come up with on their own. We are happy to <a href="https://github.com/JuliaImages/Images.jl/discussions/new?category&#61;jsoc">discuss such ideas</a> and help you refine your proposal. Below are some potential project ideas that might help spur some thoughts. In general, anything that is missing in JuliaImages, and worths three-months&#39; development can be considered as potential GSoC ideas. See the bottom of this page for information about mentors.</p> <h2 id=benchmarking_against_other_frameworks_medium ><a href="#benchmarking_against_other_frameworks_medium">Benchmarking against other frameworks &#40;medium&#41;</a></h2> <h3 id=description__6 ><a href="#description__6">Description</a></h3> <p>JuliaImages provides high-quality implementations of many algorithms; however, as yet there is no set of benchmarks that compare our code against that of other image-processing frameworks. Developing such benchmarks would allow us to advertise our strengths and/or identify opportunities for further improvement. See also the OpenCV project below.</p> <h3 id=skills ><a href="#skills">Skills</a></h3> <p>JuliaImages experiences is required. Some familiarities with other image processing frameworks is preferred.</p> <h3 id=expected_outcomes ><a href="#expected_outcomes">Expected outcomes</a></h3> <p>Benchmarks for several performance-sensitive packages &#40;e.g., ImageFiltering, ImageTransformations, ImageMorphology, ImageContrastAdjustment, ImageEdgeDetection, ImageFeatures, and/or ImageSegmentation&#41; against frameworks like Scikit-image and OpenCV, and optionally others like ITK, ImageMagick, and Matlab/Octave. See also the <a href="https://github.com/JuliaImages/image_benchmarks">image benchmarks</a> repository.</p> <p>This task splits into at least two pieces:</p> <ul> <li><p>developing frameworks for collecting the data, and</p> <li><p>visualizing the results.</p> </ul> <p>One should also be aware of the fact that differences in implementation &#40;which may include <a href="https://github.com/JuliaImages/Images.jl/pull/855">differences in quality</a>&#41; may complicate the interpretation of some benchmarks.</p> <h3 id=mentors ><a href="#mentors">Mentors</a></h3> <p><a href="https://github.com/timholy">Tim Holy</a> and <a href="https://github.com/johnnychen94">Johnny Chen</a></p> <h2 id=gpu_support_for_many_algorithms_hard ><a href="#gpu_support_for_many_algorithms_hard">GPU support for many algorithms &#40;hard&#41;</a></h2> <h3 id=description__7 ><a href="#description__7">Description</a></h3> <p>JuliaImages supports many common algorithms, but targets only the CPU. With Julia now possessing <a href="https://github.com/JuliaGPU">first-in-class support for GPUs</a>, now is the time to provide GPU implementations of many of the same algorithms.</p> <p><a href="https://github.com/JuliaGPU/KernelAbstractions.jl">KernelAbstractions</a> may make it easier to support both CPU and GPU with a common implementation.</p> <h3 id=skills__2 ><a href="#skills__2">Skills</a></h3> <p>Familiarity with CUDA programming in Julia, i.e., <a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a> is required.</p> <h3 id=expected_outcomes__2 ><a href="#expected_outcomes__2">Expected outcomes</a></h3> <p>Fairly widespread GPU support for a single nontrivial package. <a href="https://github.com/JuliaImages/ImageFiltering.jl">ImageFiltering</a> would be a good choice.</p> <h3 id=mentors__2 ><a href="#mentors__2">Mentors</a></h3> <p><a href="https://github.com/timholy">Tim Holy</a> and <a href="https://github.com/johnnychen94">Johnny Chen</a></p> <h2 id=better_imageio_supports_mediumhard ><a href="#better_imageio_supports_mediumhard">Better ImageIO supports &#40;medium/hard&#41;</a></h2> <h3 id=description__8 ><a href="#description__8">Description</a></h3> <p>ImageIO is the default IO backend shipped with Images.jl. It already supports a lot of image formats, yet there still exists some formats that are missing &#40;e.g., GIF, JPEG 2000&#41;. Potential applicant needs to support new formats by either 1&#41; wrapping available C libraries via BinaryBuilder, or 2&#41; re-implement the functionality with pure Julia. See also the EXIF project below.</p> <h3 id=skills__3 ><a href="#skills__3">Skills</a></h3> <p>Experiences with Julia is required. For library wrapping projects, experiences with cross-compiling in Linux system is required, and familiarity with the source language &#40;e.g., C&#41; is preferred. The difficulty almost totally depends on how the complicate the format is, and if there exists an easy-to-wrap C library.</p> <h3 id=expected_outcomes__3 ><a href="#expected_outcomes__3">Expected outcomes</a></h3> <p>Add at least one image format support.</p> <h3 id=mentors__3 ><a href="#mentors__3">Mentors</a></h3> <p><a href="https://github.com/johnnychen94">Johnny Chen</a>, <a href="https://github.com/Gnimuc">Yupei Qi</a> and <a href="https://github.com/IanButterworth">Ian Butterworth</a></p> <h2 id=exif_viewer_medium ><a href="#exif_viewer_medium">EXIF viewer &#40;medium&#41;</a></h2> <p><a href="https://en.wikipedia.org/wiki/Exif">Exchangeable image file format &#40;EXIF&#41;</a> is a widely used specification to store camera information. Potential applicant needs to provide a package to support read/write EXIF data of image file. This can be implemented in pure Julia, or wrapping the C package <a href="https://github.com/libexif/libexif">libexif</a>.</p> <h3 id=mentors__4 ><a href="#mentors__4">Mentors</a></h3> <p><a href="https://github.com/johnnychen94">Johnny Chen</a> and <a href="https://github.com/Gnimuc">Yupei Qi</a></p> <h2 id=interactivity_and_visualization_tools_open-ended ><a href="#interactivity_and_visualization_tools_open-ended">Interactivity and visualization tools &#40;open-ended&#41;</a></h2> <h3 id=description__9 ><a href="#description__9">Description</a></h3> <p>Image processing often involves tight interaction between algorithms and visualization. While there are a number of older tools available, leveraging GLVisualize seems to hold the greatest promise. This project might implement a number of interactive tools for region-of-interest selection, annotation, measurement, and modification. Software suites like OpenCV, ImageJ/Fiji, scikit-image, and Matlab might serve as inspiration.</p> <p>JuliaImages also provides several non-GUI visualization tools, e.g., <a href="https://github.com/JuliaImages/ImageDraw.jl">ImageDraw.jl</a>, <a href="https://github.com/JuliaImages/ImageInTerminal.jl">ImageInTerminal.jl</a>, <a href="https://github.com/JuliaImages/ImageShow.jl">ImageShow.jl</a> and <a href="https://github.com/JuliaArrays/MosaicViews.jl">MosaicViews.jl</a>. Improving these packages are also good project ideas.</p> <h3 id=skills__4 ><a href="#skills__4">Skills</a></h3> <p>For <a href="https://github.com/JuliaImages/ImageView.jl">ImageViews.jl</a> and similar GUI projects, familiarity with GUI programming is required. For non-GUI projects, familiarity with Julia array interfaces are preferred.</p> <h3 id=mentors__5 ><a href="#mentors__5">Mentors</a></h3> <p><a href="https://github.com/timholy">Tim Holy</a>. For non-GUI projects, <a href="https://github.com/johnnychen94">Johnny Chen</a> is also available.</p> <h2 id=better_qr_code_support_medium ><a href="#better_qr_code_support_medium">Better QR Code support &#40;medium&#41;</a></h2> <p><a href="https://github.com/jiegillet/QRCode.jl">QRCode.jl</a> is a legacy package that supports encoding data to QR code. Students are required to revive this package to co-exist with the latest JuliaImages ecosystem, and also adding support to decode QR code into julia data.</p> <h3 id=skills__5 ><a href="#skills__5">Skills</a></h3> <p>Experiences in JuliaImages are required. The ability to read and understand the QR code specification.</p> <h3 id=mentors__6 ><a href="#mentors__6">Mentors</a></h3> <p><a href="https://github.com/johnnychen94">Johnny Chen</a></p> <h2 id=integration_of_opencv_and_juliaimages_medium ><a href="#integration_of_opencv_and_juliaimages_medium">Integration of OpenCV and JuliaImages &#40;medium&#41;</a></h2> <h3 id=description__10 ><a href="#description__10">Description</a></h3> <p>OpenCV is one of the pre-eminent image-processing frameworks. During the summer of 2020 and 2021, significant progress was made on a <a href="https://docs.opencv.org/master/d8/da4/tutorial_julia.html">Julia wrapper</a> and <a href="https://github.com/archit120/OpenCV.jl">OpenCV.jl</a>. An important remaining task is to fill the missing documentation, testset, and also benchmark results.</p> <h3 id=skills__6 ><a href="#skills__6">Skills</a></h3> <p>C&#43;&#43; experiences are required. Some familiarity with the Julia and <a href="https://github.com/JuliaPackaging/BinaryBuilder.jl">BinaryBuilder.jl</a> and <a href="https://github.com/JuliaInterop/CxxWrap.jl">CxxWrap.jl</a> are preferred.</p> <h3 id=expected_outcomes__4 ><a href="#expected_outcomes__4">Expected outcomes</a></h3> <p>An OpenCV package that can be installed across all major platforms with <code>Pkg.add&#40;&quot;OpenCV&quot;&#41;</code>.</p> <h3 id=mentors__7 ><a href="#mentors__7">Mentors</a></h3> <p><a href="https://github.com/timholy">Tim Holy</a></p> <h2 id=contributions_to_a_stereo_matching_package_medium ><a href="#contributions_to_a_stereo_matching_package_medium">Contributions to a Stereo Matching Package &#40;medium&#41;</a></h2> <h3 id=description__11 ><a href="#description__11">Description</a></h3> <p>When two images are taken of a scene with a calibrated stereo rig it is possible to construct a three-dimensional model of the scene provided that one can determine the coordinates of corresponding points in the two images. The task of determining the coordinates of corresponding points is frequently called <em>stereo matching</em> or <em>disparity estimation</em>. Numerous algorithms for this task have been proposed over the years and new ones continue to be developed.</p> <p>This project will implement several stereo matching algorithms. Emphasis will be placed on <em>efficient</em> implementations which leverage all of Julia&#39;s features for writing fast code.</p> <p>Example algorithms:</p> <div class=tight-list ><ol> <li><p>Bleyer, Michael, Christoph Rhemann, and Carsten Rother. &quot;PatchMatch Stereo-Stereo Matching with Slanted Support Windows.&quot; Bmvc. Vol. 11. 2011.</p> <li><p>Hirschmuller, Heiko. &quot;Accurate and efficient stereo processing by semi-global matching and mutual information.&quot; Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on. Vol. 2. IEEE, 2005.</p> <li><p>Gehrig, Stefan K., and Clemens Rabe. &quot;Real-time semi-global matching on the CPU.&quot; Computer Vision and Pattern Recognition Workshops &#40;CVPRW&#41;, 2010 IEEE Computer Society Conference on. IEEE, 2010.</p> </ol></div> <h3 id=skills__7 ><a href="#skills__7">Skills</a></h3> <p>Experiences in JuliaImages are required. Familiarity with the algorithms are preferred.</p> <h3 id=expected_outcomes__5 ><a href="#expected_outcomes__5">Expected outcomes</a></h3> <p>A library of stereo matching algorithms with usage tutorials and documentation.</p> <h3 id=mentors__8 ><a href="#mentors__8">Mentors</a></h3> <p><a href="https://github.com/zygmuntszpak">Zygmunt Szpak</a></p> <h2 id=contributions_to_a_calibration_target_package_medium ><a href="#contributions_to_a_calibration_target_package_medium">Contributions to a Calibration Target package &#40;medium&#41;</a></h2> <h3 id=description__12 ><a href="#description__12">Description</a></h3> <p>Camera calibration involves determining a camera&#39;s intrinsic parameters from a series of images of a so-called &quot;calibration target&quot;. Knowledge of the intrinsic parameters facilitates three-dimensional reconstruction from images or video. The most frequently used calibration target is a checkerboard pattern. A key step in camera calibration involves automatically detecting the checkerboard and identifying landmarks such as the corners of each checkerboard square.</p> <p>This project will implement a recent automatic checkerboard detection and feature extraction algorithm.</p> <p>Example algorithm:</p> <div class=tight-list ><ol> <li><p>Y. Yan, P. Yang, L. Yan, J. Wan, Y. Sun, and K. Tansey, “Automatic checkerboard detection for camera calibration using self-correlation,” Journal of Electronic Imaging, vol. 27, no. 03, p. 1, May 2018.</p> </ol></div> <h3 id=skills__8 ><a href="#skills__8">Skills</a></h3> <p>Experiences in JuliaImages are required. Familiarity with the algorithms are preferred.</p> <h3 id=expected_outcomes__6 ><a href="#expected_outcomes__6">Expected outcomes</a></h3> <p>A checkeboard detection algorithm which can provide the necessary inputs to a camera calibration routine.</p> <h3 id=mentors__9 ><a href="#mentors__9">Mentors</a></h3> <p><a href="https://github.com/zygmuntszpak">Zygmunt Szpak</a></p> <h3 id=where_to_go_for_discussion_and_to_find_mentors ><a href="#where_to_go_for_discussion_and_to_find_mentors">Where to go for discussion and to find mentors</a></h3> <p>Interested students are encouraged to <a href="https://github.com/JuliaImages/Images.jl/discussions/new">open an discussion in Images.jl</a> to introduce themselves and discuss the detailed project ideas. To increase the chance of getting useful feedback, please provide detailed plans and ideas &#40;don&#39;t just copy the contents here&#41;.</p> <h3 id=expected_working_hours ><a href="#expected_working_hours">Expected working hours</a></h3> <p>As per <a href="https://developers.google.com/open-source/gsoc/faq#how_much_time_does_gsoc_participation_take">GSoC guideline</a> indicates, projects with medium difficulty are expected to be finished within 175 hours and that with hard difficulty 350 hours. However, this is a quite rough estimation. Applicants are expected to discuss this and other details with their potential mentors before submitting their proposal.</p> <h1 id=language_interoperability_summer_of_code ><a href="#language_interoperability_summer_of_code">Language interoperability – Summer of Code</a></h1> <h2 id=c ><a href="#c">C&#43;&#43;</a></h2> <h3 id=cxxwrap_stl ><a href="#cxxwrap_stl">CxxWrap STL</a></h3> <p>The <a href="https://github.com/JuliaInterop/CxxWrap.jl">CxxWrap.jl</a> package provides a way to load compiled C&#43;&#43; code into Julia. It exposes a small fraction of the C&#43;&#43; standard library to Julia, but many more functions and containers &#40;e.g. <code>std::map</code>&#41; still need to be exposed. The objective of this project is to improve C&#43;&#43; standard library coverage.</p> <h4 id=expected_outcome ><a href="#expected_outcome">Expected outcome</a></h4> <ol> <li><p>Add missing STL container types &#40;easy&#41;</p> <li><p>Add support for STL algorithms &#40;intermediate&#41;</p> <li><p>Investigate improvement of compile times and selection of included types &#40;advanced&#41;</p> </ol> <p><strong>Recommended Skills</strong>: Familiarity with both Julia and C&#43;&#43;</p> <p><strong>Duration: 175h, expected difficulty: hard</strong></p> <p><strong>Mentor</strong>: <a href="https://github.com/barche">Bart Janssens</a></p> <h2 id=rust ><a href="#rust">Rust</a></h2> <p>Take a look at the <a href=pluto >hyper.rs project, listed on the &quot;Pluto&quot; page</a> about wrapping a Rust HTTP server in a Julia package.</p> <h1 id=javis_projects_summer_of_code ><a href="#javis_projects_summer_of_code">Javis Projects – Summer of Code</a></h1> <blockquote> <p><a href="https://github.com/JuliaAnimators/Javis.jl">Javis: <strong>J</strong>ulia <strong>A</strong>nimations and <strong>Vis</strong>ualizations</a></p> </blockquote> <p>Are you ready to create the next amazing visualization? With Javis you can&#33; <a href="https://github.com/JuliaAnimators/Javis.jl">Javis.jl</a> is a general purpose Julia library to easily construct informative, performant, and winsome animated graphics. It uses a object-action relationship for users to make such visuals.</p> <p>Javis has found application in diverse areas such as <a href="https://juliaanimators.github.io/Javis.jl/dev/examples/">teaching, art and more</a>. To learn more about Javis and what it is capable of, check out our <a href="https://www.youtube.com/watch?v&#61;ckvsc6ukdOc">2021 JuliaCon talk</a>&#33; It builds on top of the drawing framework <a href="https://github.com/JuliaGraphics/Luxor.jl">Luxor.jl</a> by adding functions to simplify the creation of objects and their actions.</p> <p>Below you can find a list of potential projects that can be tackled during Google Summer of Code. If interested in exploring any of these projects, please reach out to any of the following mentors:</p> <ul> <li><p><strong><a href="http://jacobzelko.com/">Jacob Zelko</a></strong> - <a href="mailto:jacobszelko@gmail.com">email</a>, <a href="https://julialang.org/slack/">Slack</a> &#40;username: TheCedarPrince&#41;, or <a href="https://julialang.zulipchat.com/">Zulip</a> &#40;username: TheCedarPrince&#41;</p> <li><p><strong><a href="https://opensourc.es/about/">Ole Kröger</a></strong> - <a href="https://julialang.org/slack/">Slack</a> &#40;username: Wikunia&#41;, or <a href="https://julialang.zulipchat.com/">Zulip</a> &#40;username: Wikunia&#41;</p> <li><p><strong><a href="https://gpucce.github.io">Giovanni Puccetti</a></strong> - <a href="https://julialang.zulipchat.com/">Zulip</a> &#40;username: Giovanni&#41;</p> <li><p><strong><a href="https://sov-trotter.github.io/blog/">Arsh Sharma</a></strong> - <a href="https://julialang.zulipchat.com/">Zulip</a> &#40;username: Arsh Sharma&#41;</p> </ul> <p>Thanks for your interest&#33; 🎉</p> <h2 id=improve_javis_performance ><a href="#improve_javis_performance">Improve Javis Performance</a></h2> <p><strong>Mentors:</strong> Ole Kröger, Arsh Sharma</p> <p><strong>Recommended Skills:</strong> Familiarity with profiling, caching approaches, and performance testing</p> <p><strong>Difficulty:</strong> Medium</p> <p>As Javis&#39;s interface is largely stabilized and Javis is finding use in different applications, it is now time to deal with one of Javis&#39;s greatest pain points: slowness and high memory usage for large animations. While creating an animation in Javis, there is much room for performance improvements such as in the area of creating Objects and Actions, managing the data structures for Objects and Actions, rendering an animation, and handling different media formats &#40;such as gif and mp4&#41;. For this specific project, a student will work with Ole and Arsh to create a profiling scheme for Javis to identify performance bottlenecks and measure allocations, determine caching and memory flexible modes of rendering animations with tools such as <a href="https://github.com/JuliaIO/FFMPEG.jl">FFMPEG.jl</a>, and finish implementing live streaming of animations. The goal for this project will not be to fully fix all identified performance issues but rather to identify and catalogue them for further development by Javis maintainers and contributors.</p> <h2 id=building_novel_animation_abilities_for_javis ><a href="#building_novel_animation_abilities_for_javis">Building Novel Animation Abilities for Javis</a></h2> <p><strong>Mentors:</strong> Jacob Zelko, Giovanni Puccetti</p> <p><strong>Recommended Skills:</strong> General understanding of Luxor and the underlying structure of Javis</p> <p><strong>Difficulty:</strong> Medium</p> <p>Javis&#39;s interface has matured to a great point - but we believe Javis can do even more&#33; Although Javis can do complex transformations such as morphing one polygon to another, Javis is capable of more than that. In this project, a student will work with Jacob and Giovanni to create new animation abilities for Javis to handle different coordinate systems, developing new types of shorthand expressions for object creation known as JObjects, further developing morphing, building out the flexibility of layers, and more. A student is encouraged to come to this project with new ideas for what animations Javis can do and to reach out to Jacob and Giovanni to begin discussions early.</p> <h1 id=dynamical_systems_complex_systems_nonlinear_dynamics_summer_of_code ><a href="#dynamical_systems_complex_systems_nonlinear_dynamics_summer_of_code">Dynamical systems, complex systems &amp; nonlinear dynamics – Summer of Code</a></h1> <h2 id=agentsjl ><a href="#agentsjl">Agents.jl</a></h2> <p><strong>Difficulty</strong>: Medium to Hard.</p> <p><strong>Length</strong>: 350 hours.</p> <p><a href="https://juliadynamics.github.io/Agents.jl/stable/">Agents.jl</a> is a pure Julia framework for agent-based modeling &#40;ABM&#41;. It has an extensive list of features, excellent performance and is easy to learn, use, and extend. Comparisons with other popular frameworks written in Python or Java &#40;NetLOGO, MASON, Mesa&#41;, show that Agents.jl outperforms all of them in computational speed, list of features and usability.</p> <p>In this project students will be paired with lead developers of Agents.jl to improve Agents.jl with more features, better performance, and overall higher polish. Possible features to implement are:</p> <div class=tight-list ><ul> <li><p>Automatic performance increase of mixed-agent models by eliminating dynamic dispatch on the stepping function</p> <li><p>GPU support in Agents.jl</p> <li><p>New type of space representing a planet, which can be used in climate policy or human evolution modelling, and new interface for an overarching ABM composed of several smaller ABMs</p> </ul></div> <p><strong>Recommended Skills</strong>: Familiarity with agent based modelling, Agents.jl and Julia&#39;s Type System. Background in complex systems, sociology, or nonlinear dynamics is not required but would be advantageous.</p> <p><strong>Expected Results</strong>: Well-documented, well-tested useful new features for Agents.jl.</p> <p><strong>Mentors</strong>: <a href="https://github.com/Datseris">George Datseris</a>.</p> <h2 id=dynamicalsystemsjl ><a href="#dynamicalsystemsjl">DynamicalSystems.jl</a></h2> <p><strong>Difficulty:</strong> Easy to Medium, depending on the algorithms chosen to implement.</p> <p><strong>Length</strong>: 175 hours.</p> <p><a href="https://juliadynamics.github.io/DynamicalSystems.jl/latest/">DynamicalSystems.jl</a> is an <a href="https://dsweb.siam.org/The-Magazine/Article/winners-of-the-dsweb-2018-software-contest">award-winning</a> Julia software library for dynamical systems, nonlinear dynamics, deterministic chaos and nonlinear timeseries analysis. It has an impressive list of features, but one can never have enough. In this project students will be able to enrich DynamicalSystems.jl with new algorithms and enrich their knowledge of nonlinear dynamics and computer-assisted exploration of complex systems.</p> <p>Possible projects are summarized in the <a href="https://github.com/issues?q&#61;is&#37;3Aopen&#43;is&#37;3Aissue&#43;repo&#37;3AJuliaDynamics&#37;2FChaosTools.jl&#43;repo&#37;3AJuliaDynamics&#37;2FDynamicalSystemsBase.jl&#43;repo&#37;3AJuliaDynamics&#37;2FDelayEmbeddings.jl&#43;repo&#37;3AJuliaDynamics&#37;2FRecurrenceAnalysis.jl&#43;repo&#37;3AJuliaDynamics&#37;2FDynamicalSystems.jl&#43;label&#37;3A&#37;22wanted&#43;feature&#37;22&#43;">wanted-features of the library</a></p> <p><strong>Recommended Skills</strong>: Familiarity with nonlinear dynamics and/or differential equations and the Julia language.</p> <p><strong>Expected Results</strong>: Well-documented, well-tested new algorithms for DynamicalSystems.jl.</p> <p><strong>Mentors</strong>: <a href="https://github.com/Datseris">George Datseris</a></p> <h1 id=music_data_analysis_-_summer_of_code ><a href="#music_data_analysis_-_summer_of_code">Music data analysis - Summer of Code</a></h1> <p><a href="https://github.com/JuliaMusic">JuliaMusic</a> is an organization providing packages and functionalities that allow analyzing the properties of music performances. </p> <h2 id=midification_of_music_from_wave_files ><a href="#midification_of_music_from_wave_files">MIDIfication of music from wave files</a></h2> <p><strong>Difficulty</strong>: Medium.</p> <p><strong>Length</strong>: 350 hours.</p> <p>It is easy to analyze timing and intensity fluctuations in music that is the form of MIDI data. This format is already digitilized, and packages such as MIDI.jl and MusicManipulations.jl allow for seamless data processing. But arguably the most interesting kind of music to analyze is the live one. Live music performances are recorded in wave formats. Some algorithms exist that can detect the &quot;onsets&quot; of music hits, but they are typically focused only on the timing information and hence forfeit detecting e.g., the intensity of the played note. Plus, there are very few code implementations online for this problem, almost all of which are old and unmaintained. We would like to implement an algorithm in MusicProcessing.jl that given a recording of a single instrument, it can &quot;MIDIfy&quot; it, which means to digitalize it into the MIDI format.</p> <p><strong>Recommended Skills</strong>: Background in music, familiarity with digital signal processing.</p> <p><strong>Expected results</strong>: A well-tested, well-documented function <code>midify</code> in MusicProcessing.jl.</p> <p><strong>Mentors</strong>: <a href="https://github.com/Datseris/">George Datseris</a>.</p> <h1 id=juliareach_-_summer_of_code ><a href="#juliareach_-_summer_of_code">JuliaReach - Summer of Code</a></h1> <p><a href="https://github.com/JuliaReach">JuliaReach</a> is the Julia ecosystem for reachability computations of dynamical systems.</p> <h2 id=efficient_low-dimensional_symbolic-numeric_set_computations ><a href="#efficient_low-dimensional_symbolic-numeric_set_computations">Efficient low-dimensional symbolic-numeric set computations</a></h2> <p><strong>Difficulty</strong>: Medium.</p> <p><strong>Description.</strong> <a href="https://github.com/JuliaReach/LazySets.jl">LazySets</a> is a Julia library for computing with geometric sets, whose focus is on lazy set representations and efficient high-dimensional processing. The main interest in this project is to develop algorithms that leverage the structure of the sets. The special focus will be on low-dimensional &#40;typically 2D and 3D&#41; cases.</p> <p><strong>Expected Results.</strong> The goal is to implement certain efficient algorithms from the literature. The code is to be documented, tested, and evaluated in benchmarks. Specific tasks may include: efficient vertex enumeration of <a href="https://juliareach.github.io/LazySets.jl/dev/lib/sets/Zonotope/#LazySets.Zonotope">zonotopes</a>; operations on <a href="http://archive.www6.in.tum.de/www6/Main/Publications/Althoff2011f.pdf">zonotope bundles</a>; efficient disjointness checks between different set types; <a href="https://ieeexplore.ieee.org/document/7525593">complex zonotopes</a>.</p> <p><strong>Expected Length.</strong> 175 hours.</p> <p><strong>Recommended Skills.</strong> Familiarity with Julia and Git/GitHub is mandatory. Familiarity with <a href="https://github.com/JuliaReach/LazySets.jl">LazySets</a> is recommended. Basic knowledge of geometric terminology is appreciated but not required.</p> <p><strong>Mentors</strong>: <a href="github.com/mforets">Marcelo Forets</a>, <a href="github.com/schillic">Christian Schilling</a>.</p> <h2 id=reachability_with_sparse_polynomial_zonotopes ><a href="#reachability_with_sparse_polynomial_zonotopes">Reachability with sparse polynomial zonotopes</a></h2> <p><strong>Difficulty</strong>: Hard.</p> <p><strong>Description.</strong> Sparse polynomial zonotopes are a new non-convex set representation that are well-suited for reachability analysis of nonlinear dynamical systems. The task is to add efficient Julia implementations of:</p> <p>&#40;1&#41; sparse polynomial zonotopes in <a href="https://github.com/JuliaReach/LazySets.jl">LazySets</a>,</p> <p>&#40;2&#41; the corresponding reachability algorithm for dynamical systems in <a href="https://github.com/JuliaReach/ReachabilityAnalysis.jl">ReachabilityAnalysis</a>.</p> <p><strong>Expected Results.</strong> The goal is to efficiently implement sparse polynomial zonotopes and the corresponding reachability algorithms. The code is to be documented, tested, and evaluated extensively in benchmarks. If the candidate is interested, it is possible to change task &#40;2&#41; with </p> <p>&#40;3&#41; an integration of the new set representation for neural-network control systems in <a href="https://github.com/JuliaReach/NeuralNetworkAnalysis.jl">NeuralNetworkAnalysis</a>.</p> <p><strong>Expected Length.</strong> 350 hours.</p> <p><strong>Recommended Skills.</strong> Familiarity with Julia and Git/GitHub is mandatory. Familiarity with the mentioned Julia packages is appreciated but not required. The project does not require theoretical contributions, but it requires reading a research article &#40;see below&#41;; hence a certain level of academic experience is recommended.</p> <p><strong>Literature and related packages.</strong> <a href="https://www.youtube.com/watch?v&#61;iMtq6YeIsjA">This video</a> explains the concept of polynomial zonotopes &#40;slides <a href="https://github.com/JuliaReach/juliareach-days-3-reachathon/blob/master/Challenge_5/Challenge5_PolynomialZonotopes.pdf">here</a>&#41;. The relevant theory is described in <a href="https://arxiv.org/pdf/1901.01780">this research article</a>. There exists a Matlab implementation in <a href="https://tumcps.github.io/CORA/">CORA</a> &#40;the implementation of polynomial zonotopes can be found in <a href="https://github.com/TUMcps/CORA/tree/master/contSet/&#37;40polyZonotope">this folder</a>&#41;.</p> <p><strong>Mentors</strong>: <a href="github.com/mforets">Marcelo Forets</a>, <a href="github.com/schillic">Christian Schilling</a>.</p> <h2 id=improving_the_hybrid_systems_reachability_api ><a href="#improving_the_hybrid_systems_reachability_api">Improving the hybrid systems reachability API</a></h2> <p><strong>Difficulty</strong>: Hard.</p> <p><strong>Description.</strong> <a href="https://github.com/JuliaReach/ReachabilityAnalysis.jl">ReachabilityAnalysis</a> is a Julia library for set propagation of dynamical systems. One of the main aims is to handle systems with mixed discrete-continuous behaviors &#40;known as hybrid systems in the literature&#41;. This project will focus on enhancing the capabilities of the library and overall improvement of the ecosystem for users.</p> <p><strong>Expected Results.</strong> Specific tasks may include: problem-specific heuristics for hybrid systems; API for time-varying input sets; flowpipe underapproximations. The code is to be documented, tested, and evaluated in benchmarks. Integration with <a href="https://github.com/SciML/ModelingToolkit.jl">ModelingToolkit.jl</a> can also be considered if there is interest.</p> <p><strong>Expected Length.</strong> 350 hours.</p> <p><strong>Recommended Skills.</strong> Familiarity with Julia and Git/GitHub is mandatory. Familiarity with <a href="https://github.com/JuliaReach/LazySets.jl">LazySets</a> and <a href="https://github.com/JuliaReach/ReachabilityAnalysis.jl">ReachabilityAnalysis</a> is also required.</p> <p><strong>Mentors</strong>: <a href="github.com/mforets">Marcelo Forets</a>, <a href="github.com/schillic">Christian Schilling</a>.</p> <h1 id=stochastic_differential_equations_and_continuous_time_signal_processing_summer_of_code ><a href="#stochastic_differential_equations_and_continuous_time_signal_processing_summer_of_code">Stochastic differential equations and continuous time signal processing – Summer of Code</a></h1> <h2 id=smoothing_non-linear_continuous_time_systems ><a href="#smoothing_non-linear_continuous_time_systems">Smoothing non-linear continuous time systems</a></h2> <p>The student implements a state of the art smoother for continuous-time systems with additive Gaussian noise. The system&#39;s dynamics can be described as an ordinary differential equation with locally additive Gaussian random fluctuations, in other words a stochastic ordinary differential equation.</p> <p>Given a series of measurements observed over time, containing statistical noise and other inaccuracies, the task is to produce an estimate of the unknown trajectory of the system that led to the observations.</p> <p><em>Linear</em> continuous-time systems are smoothed with the fixed-lag Kalman-Bucy smoother &#40;related to the <a href="https://en.wikipedia.org/wiki/Kalman_filter#Kalman–Bucy_filter">Kalman–Bucy_filter</a>&#41;. It relies on coupled ODEs describing how mean and covariance of the conditional distribution of the latent system state evolve over time. A versatile implementation in Julia is missing.</p> <p><strong>Expected Results</strong>: Build efficient implementation of non-linear smoothing of continuous stochastic dynamical systems.</p> <p><strong>Recommended Skills</strong>: Gaussian random variables, Bayes&#39; formula, Stochastic Differential Equations</p> <p><strong>Mentors</strong>: <a href="https://github.com/mschauer">Moritz Schauer</a></p> <p><strong>Rating</strong>: Hard</p> <h1 id=loop_optimization_projects ><a href="#loop_optimization_projects">Loop Optimization projects</a></h1> <p>LoopModels.jl uses an internal representation of loops that represents the iteration space of each constituent operation as well as their dependencies. The iteration spaces of inner loops are allowed to be affine functions of the outer loops, and multiple loops are allowed to exist at each level of a loopnest. LoopModels.jl aims to support optimizations including fusion, splitting, permuting loops, unrolling, and vectorization to maximize throughput. Broadly, this functionality can be divided into four pieces:</p> <ol> <li><p>The internal representation of the loops &#40;Loop IR&#41;.</p> <li><p>Means of creating the internal representation from Julia code. This must be able to deconstruct and simplify user provided types into the primitive types representable by IR, e.g. decompose <code>ForwardDiff.Dual&#123;T1,ForwardDiff.Dual&#123;T2,Float64,N&#125;,M&#125;</code> operations into operations on the underlying <code>Float64</code>.</p> <li><p>Analyze the representation to determine an optimal, correct, and target-specific schedule.</p> <li><p>Generate runnable code according to the schedule.</p> </ol> <p>Open projects on this effort include:</p> <h2 id=improving_and_refining_the_ir_and_analysis ><a href="#improving_and_refining_the_ir_and_analysis">Improving and refining the IR and analysis</a></h2> <p>This can include refining the search, dependency analysis, and cost modeling.</p> <h2 id=develop_the_frontend_to_infer_the_loop_structure ><a href="#develop_the_frontend_to_infer_the_loop_structure">Develop the frontend to infer the loop structure</a></h2> <p>Develop the front end that infers the loop structure from Julia code, and creates the Loop IR. This will likely live as a compiler plugin, but infrastrcture such as GPUCompiler.jl to interface more directly on the LLVM level is worth exploring, as this would allow taking advantage of LLVM&#39;s existing infrastrcture.</p> <h2 id=generating_code ><a href="#generating_code">Generating code</a></h2> <p>Code would be generated through LLVM.jl. It must be able to follow the schedule determined by the optimization. The schedule is abstract, so care must still be taken to generate optimal code when following the schedule, e.g. to optimally keep track of the loop bounds, handle remainders, and indexing the arrays.</p> <p>Mentors: <a href="https://github.com/chriselrod">Chris Elrod</a>.</p> <h1 id=machine_learning_projects_-_summer_of_code ><a href="#machine_learning_projects_-_summer_of_code">Machine Learning Projects - Summer of Code</a></h1> <h3 id=cuda_hacking ><a href="#cuda_hacking">CUDA Hacking</a></h3> <p>Are you a performance nut? Help us implement cutting-edge CUDA kernels in Julia for operations important across deep learning, scientific computing and more. We also need help developing our wrappers for machine learning, sparse matrices and more, as well as CI and infrastructure. Contact us to develop a project plan.</p> <p>Mentors: <a href="https://github.com/maleadt">Tim Besard</a>, <a href="https://github.com/DhairyaLGandhi">Dhairya Gandhi</a>.</p> <h3 id=reinforcement_learning_environments ><a href="#reinforcement_learning_environments">Reinforcement Learning Environments</a></h3> <p>Develop a series of reinforcement learning environments, in the spirit of the <a href="https://gym.openai.com">OpenAI Gym</a>. Although we have wrappers for the gym available, it is hard to install &#40;due to the Python dependency&#41; and, since it&#39;s written in Python and C code, we can&#39;t do more interesting things with it &#40;such as differentiate through the environments&#41;. A pure-Julia version that supports a similar API and visualisation options would be valuable to anyone doing RL with Flux.</p> <p>Mentors: <a href="https://github.com/DhairyaLGandhi/">Dhairya Gandhi</a>.</p> <h3 id=reinforcement_learning_algorithms ><a href="#reinforcement_learning_algorithms">Reinforcement Learning Algorithms</a></h3> <p>Recent advances in reinforcement learning led to many breakthroughs in artificial intelligence. Some of the latest deep reinforcement learning algorithms have been implemented in <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl">ReinforcementLearning.jl</a> with Flux. We&#39;d like to have more interesting and practical algorithms added to enrich the whole community, including but not limited to the following directions:</p> <ul> <li><p><strong>&#91;Easy&#40;175h&#41;&#93; Recurrent version of existing algorithms</strong>. Students with a basic understanding of Q-learning and recurrent neural networks are preferred. We&#39;d like to have a general implementation to easily extend existing algorithms to the sequential version.</p> <li><p><strong>&#91;Medium&#40;175h&#41;&#93; Multi-agent reinforcement learning algorithms</strong>. Currently, we only have some CFR， MADDPG and NFSP related algorithms implemented. We&#39;d like to see more implemented, including <a href="https://arxiv.org/abs/1705.08926">COMA</a> and its variants, <a href="https://arxiv.org/abs/1711.00832">PSRO</a>.</p> <li><p><strong>&#91;Medium&#40;350h&#41;&#93; Model-based reinforcement learning algorithms</strong>. Students interested in this topic may refer <a href="https://arxiv.org/abs/2006.16712">Model-based Reinforcement Learning: A Survey</a> and design some general interfaces to implement typical model based algorithms.</p> <li><p><strong>&#91;Hard&#40;350h&#41;&#93; Distributed reinforcement learning framework</strong>. Inspired by <a href="https://arxiv.org/abs/2006.00979">Acme</a>, a similar design is proposed in <a href="https://github.com/JuliaReinforcementLearning/DistributedReinforcementLearning.jl">DistributedReinforcementLearning.jl</a>. However, it is still in a very early stage. Students interested in this direction are required to have a basic understanding of distributed computing in Julia. Ideally we&#39;d like to see some distributed reinforcement learning algorithms implemented under this framework, like <a href="https://openreview.net/forum?id&#61;r1lyTjAqYX&amp;utm_campaign&#61;RL&#37;20Weekly&amp;utm_medium&#61;email&amp;utm_source&#61;Revue&#37;20newsletter">R2D2</a>, <a href="https://arxiv.org/abs/1804.08617v1">D4PG</a>.</p> </ul> <h4 id=expected_outcomes__7 ><a href="#expected_outcomes__7">Expected outcomes</a></h4> <p>For each new algorithm, at least two experiments are expected to be added into <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningZoo.jl">ReinforcementLearningZoo.jl</a>. A simple one to make sure it works on some toy games with CPU only and another more practical one to produce comparable results on the original paper with GPU enabled. Besides, a technical report on the implementation details and speed/performance comparison with other baselines is preferred.</p> <p>Mentors: <a href="https://github.com/findmyway">Jun Tian</a></p> <h3 id=alphazerojl ><a href="#alphazerojl">AlphaZero.jl</a></h3> <p>The philosophy of the <a href="https://github.com/jonathan-laurent/AlphaZero.jl">AlphaZero.jl</a> project is to provide an implementation of AlphaZero that is simple enough to be widely accessible for students and researchers, while also being sufficiently powerful and fast to enable meaningful experiments on limited computing resources &#40;our latest release is consistently between one and two orders of magnitude faster than competing Python implementations&#41;.</p> <p>Here are a few project ideas that build on AlphaZero.jl. Please contact us for additional details and let us know about your experience and interests so that we can build a project that best suits your profile.</p> <ul> <li><p>&#91;Easy&#93; Integrate AlphaZero.jl with the <a href="https://github.com/JuliaReinforcementLearning/OpenSpiel.jl">OpenSpiel</a> game library and benchmark it on a series of simple board games.</p> <li><p>&#91;Medium&#93; Use AlphaZero.jl to train a chess agent. In order to save computing resources and allow faster bootstrapping, you may train an initial policy using supervised learning.</p> <li><p>&#91;Hard&#93; Build on AlphaZero.jl to implement the <a href="https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules">MuZero</a> algorithm.</p> <li><p>&#91;Hard&#93; Explore applications of AlphaZero beyond board games &#40;e.g. theorem proving, chip design, chemical synthesis...&#41;.</p> </ul> <h4 id=expected_outcomes__8 ><a href="#expected_outcomes__8">Expected outcomes</a></h4> <p>In all these projects, the goal is not only to showcase the current Julia ecosystem and test its limits, but also to push it forward through concrete contributions that other people can build on. Such contributions include:</p> <ul> <li><p>Improvements to existing Julia packages &#40;e.g. AlphaZero, ReinforcementLearning, CommonRLInterface, Dagger, Distributed, CUDA...&#41; through code, documentation or benchmarks.</p> <li><p>A well-documented and replicable artifact to be added to <a href="https://github.com/jonathan-laurent/AlphaZero.jl/tree/master/games">AlphaZero.Examples</a>, <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningZoo.jl">ReinforcementLearningZoo</a> or released in its own package.</p> <li><p>A blog post that details your experience, discusses the challenges you went through and identifies promising areas for future work.</p> </ul> <p><strong>Mentors</strong>: <a href="https://github.com/jonathan-laurent">Jonathan Laurent</a></p> <h3 id=nlp_tools_and_models ><a href="#nlp_tools_and_models">NLP Tools and Models</a></h3> <p><strong>Difficulty</strong>: Medium to Hard</p> <p>Build deep learning models for Natural Language Processing in Julia. <a href="https://github.com/juliatext/TextAnalysis.jl">TextAnalysis</a> and <a href="https://github.com/JuliaText/WordTokenizers.jl">WordTokenizers</a> contains the basic algorithms and data structures to work with textual data in Julia. On top of that base, we want to build modern deep learning models based on recent research. The following tasks can span multiple students and projects.</p> <p>It is important to note that we want practical, usable solutions to be created, not just research models. This implies that a large part of the effort will need to be in finding and using training data, and testing the models over a wide variety of domains. Pre-trained models must be available to users, who should be able to start using these without supplying their own training data.</p> <div class=tight-list ><ul> <li><p>Implement GPT/GPT-2 in Julia</p> <li><p>Implement <a href="https://arxiv.org/abs/1909.03186">extractive summarisation based on Transformers</a></p> <li><p>Implement practical models for</p> <ul> <li><p>Dependency Tree Parsing</p> <li><p>Morphological extractions</p> <li><p>Translations &#40;using Transformers&#41;</p> </ul> <li><p>Indic language support – validate and test all models for Indic languages</p> <ul> <li><p>ULMFiT models for Indic languages</p> </ul> <li><p>Chinese tokenisation and parsing</p> </ul></div> <p><strong>Mentors</strong>: <a href="https://github.com/aviks/">Avik Sengupta</a></p> <h3 id=automated_music_generation ><a href="#automated_music_generation">Automated music generation</a></h3> <p><strong>Difficulty</strong>: Hard</p> <p>Neural network based models can be used for music analysis and music generation &#40;composition&#41;. A suite of tools in Julia to enable research in this area would be useful. This is a large, complex project that is suited for someone with an interest in music and machine learning. This project will need a mechanism to read music files &#40;primarily MIDI&#41;, a way to synthesise sounds, and finally a model to learn composition. All of this is admittedly a lot of work, so the exact boundaries of the project can be flexible, but this can be an exciting project if you are interested in both music and machine learning.</p> <p><strong>Recommended Skills</strong>: Music notation, some basic music theory, MIDI format, Transformer and LSTM architectures</p> <p><strong>Resources</strong>: <a href="https://magenta.tensorflow.org/music-transformer">Music Transformer</a>, <a href="https://magenta.tensorflow.org/maestro-wave2midi2wave">Wave2MIDI2Wave</a>, <a href="https://github.com/JuliaMusic/MIDI.jl">MIDI.jl</a>, <a href="https://github.com/JuliaMusic/Mplay.jl">Mplay.jl</a></p> <p><strong>Mentors</strong>: <a href="https://github.com/aviks/">Avik Sengupta</a></p> <h1 id=molecular_simulation_-_summer_of_code ><a href="#molecular_simulation_-_summer_of_code">Molecular Simulation - Summer of Code</a></h1> <p>Much of science can be explained by the movement and interaction of molecules. Molecular dynamics &#40;MD&#41; is a computational technique used to explore these phenomena, from noble gases to biological macromolecules. <a href="https://github.com/JuliaMolSim/Molly.jl">Molly.jl</a> is a pure Julia package for MD, and for the simulation of physical systems more broadly. The package is currently under development for research with a focus on proteins and differentiable molecular simulation. There are a number of ways that the package could be improved:</p> <ul> <li><p><strong>Adding simulators &#40;duration: 175h, expected difficulty: easy to medium&#41;:</strong> a variety of standard approaches to simulating molecules can be added including Langevin dynamics, FIRE minimisation, pressure coupling &#40;NPT ensemble&#41; and enhanced sampling approaches such as replica-exchange MD &#40;REMD&#41;.</p> <li><p><strong>Adding constraint algorithms &#40;duration: 175h, expected difficulty: medium&#41;:</strong> many simulations keep fast degrees of freedom such as bond lengths and bond angles fixed using approaches such as SHAKE, RATTLE and SETTLE. A fast implementation of these algorithms would be a valuable contribution.</p> <li><p><strong>Adding electrostatic summation &#40;duration: 175h, expected difficulty: medium to hard&#41;:</strong> methods such as particle-mesh Ewald &#40;PME&#41; are in wide use for molecular simulation. Developing fast, flexible implementations and exploring compatibility with GPU acceleration and automatic differentiation would be an <a href="https://discourse.julialang.org/t/electrostatics-in-julia/41633">important contribution</a>.</p> </ul> <p><strong>Recommended skills:</strong> familiarity with computational chemistry, structural bioinformatics or simulating physical systems.</p> <p><strong>Expected results:</strong> new features added to the package along with tests and relevant documentation.</p> <p><strong>Mentor:</strong> <a href="https://github.com/jgreener64">Joe Greener</a></p> <p><strong>Contact:</strong> feel free to ask questions via <a href="http://jgreener64.github.io">email</a> or the <a href="https://join.slack.com/t/juliamolsim/shared_invite/zt-tc060co0-HgiKApazzsQzBHDlQ58A7g">JuliaMolSim Slack</a>.</p> <h1 id=numerical_projects_summer_of_code ><a href="#numerical_projects_summer_of_code">Numerical Projects – Summer of Code</a></h1> <div class=franklin-toc ><ol><li><a href="#view_all_gsocjsoc_projects">View all GSoC/JSoC Projects</a><li><a href="#projects">Projects</a><li><a href="#new_geostatistical_clustering_methods">New geostatistical clustering methods</a><li><a href="#new_geostatistical_simulation_methods">New geostatistical simulation methods</a><li><a href="#migrate_from_plotsjl_to_makiejl_recipes">Migrate from Plots.jl to Makie.jl recipes</a><li><a href="#how_to_get_started">How to get started?</a><li><a href="#time_series_forecasting_at_scale_-_speed_up_via_julia">Time series forecasting at scale - speed up via Julia</a><ol><li><a href="#prerequisites">Prerequisites</a><li><a href="#your_contribution">Your contribution</a><li><a href="#references">References</a></ol><li><a href="#interpretable_machine_learning_in_julia">Interpretable Machine Learning in Julia</a><ol><li><a href="#description">Description</a><li><a href="#prerequisites__2">Prerequisites</a><li><a href="#your_contribution__2">Your contribution</a><li><a href="#references__2">References</a></ol><li><a href="#model_visualization_in_mlj">Model visualization in MLJ</a><ol><li><a href="#description__2">Description</a><li><a href="#prerequisites__3">Prerequisites</a><li><a href="#your_contribution__3">Your contribution</a><li><a href="#references__3">References</a></ol><li><a href="#deeper_bayesian_integration">Deeper Bayesian Integration</a><ol><li><a href="#description__3">Description</a><li><a href="#your_contributions">Your contributions</a><li><a href="#references__4">References</a><li><a href="#difficulty_medium_to_hard">Difficulty: Medium to Hard</a></ol><li><a href="#mlj_and_mlflow_integration">MLJ and MLFlow integration</a><ol><li><a href="#description__4">Description</a><li><a href="#prerequisites__4">Prerequisites</a><li><a href="#your_contribution__4">Your contribution</a><li><a href="#references__5">References</a></ol><li><a href="#speed_demons_only_need_apply">Speed demons only need apply</a><ol><li><a href="#description__5">Description</a><li><a href="#prerequisites__5">Prerequisites</a><li><a href="#your_contribution__5">Your contribution</a><li><a href="#references__6">References</a></ol><li><a href="#improving_test_coverage">Improving test coverage</a><li><a href="#multi-threading_improvement_projects">Multi-threading Improvement Projects</a><li><a href="#automation_of_testing_performance_benchmarking">Automation of testing / performance benchmarking</a><li><a href="#towards_deepchemjl_combining_machine_learning_with_chemical_knowledge">Towards DeepChem.jl: Combining Machine Learning with Chemical Knowledge</a><li><a href="#bringing_dftk_to_graphics-processing_units_gpus">Bringing DFTK to graphics-processing units &#40;GPUs&#41;</a><li><a href="#native_julia_ode_sde_dae_dde_and_spde_solvers">Native Julia ODE, SDE, DAE, DDE, and &#40;S&#41;PDE Solvers</a><li><a href="#improvements_to_physics-informend_neural_networks_pinn_for_solving_differential_equations">Improvements to Physics-Informend Neural networks &#40;PINN&#41; for solving differential equations</a><li><a href="#performance_enhancements_for_differential_equation_solvers">Performance enhancements for differential equation solvers</a><li><a href="#discretizations_of_partial_differential_equations">Discretizations of partial differential equations</a><li><a href="#tools_for_global_sensitivity_analysis">Tools for global sensitivity analysis</a><li><a href="#parameter_identifiability_analysis">Parameter identifiability analysis</a><li><a href="#model_order_reduction">Model Order Reduction</a><li><a href="#automated_symbolic_manipulations_of_differential_equation_systems">Automated symbolic manipulations of differential equation systems</a><li><a href="#documenterjl">Documenter.jl</a><li><a href="#docsystem_api">Docsystem API</a><ol><li><a href="#metalheadjl_developement">Metalhead.jl Developement</a><li><a href="#fastaijl_time_series_development">FastAI.jl Time Series Development</a><li><a href="#fastaijl_text_development">FastAI.jl Text Development</a><li><a href="#differentiable_computer_vision_hard">Differentiable Computer Vision &#91;HARD&#93;</a><li><a href="#ferminets_generative_synthesis_for_automating_the_choice_of_neural_architectures">FermiNets: Generative Synthesis for Automating the Choice of Neural Architectures</a><li><a href="#differentiable_rendering_hard">Differentiable Rendering &#91;HARD&#93;</a></ol><li><a href="#deep_learning_for_source_code_analysis">Deep Learning for source code analysis</a><li><a href="#qml_and_makie_integration">QML and Makie integration</a><ol><li><a href="#expected_results">Expected results</a></ol><li><a href="#web_apps_in_makie_and_jsserve">Web apps in Makie and JSServe</a><li><a href="#scheduling_algorithms_for_distributed_algorithms">Scheduling algorithms for Distributed algorithms</a><li><a href="#distributed_training">Distributed Training</a><li><a href="#benchmarking_against_other_frameworks_medium">Benchmarking against other frameworks &#40;medium&#41;</a><ol><li><a href="#description__6">Description</a><li><a href="#skills">Skills</a><li><a href="#expected_outcomes">Expected outcomes</a><li><a href="#mentors">Mentors</a></ol><li><a href="#gpu_support_for_many_algorithms_hard">GPU support for many algorithms &#40;hard&#41;</a><ol><li><a href="#description__7">Description</a><li><a href="#skills__2">Skills</a><li><a href="#expected_outcomes__2">Expected outcomes</a><li><a href="#mentors__2">Mentors</a></ol><li><a href="#better_imageio_supports_mediumhard">Better ImageIO supports &#40;medium/hard&#41;</a><ol><li><a href="#description__8">Description</a><li><a href="#skills__3">Skills</a><li><a href="#expected_outcomes__3">Expected outcomes</a><li><a href="#mentors__3">Mentors</a></ol><li><a href="#exif_viewer_medium">EXIF viewer &#40;medium&#41;</a><ol><li><a href="#mentors__4">Mentors</a></ol><li><a href="#interactivity_and_visualization_tools_open-ended">Interactivity and visualization tools &#40;open-ended&#41;</a><ol><li><a href="#description__9">Description</a><li><a href="#skills__4">Skills</a><li><a href="#mentors__5">Mentors</a></ol><li><a href="#better_qr_code_support_medium">Better QR Code support &#40;medium&#41;</a><ol><li><a href="#skills__5">Skills</a><li><a href="#mentors__6">Mentors</a></ol><li><a href="#integration_of_opencv_and_juliaimages_medium">Integration of OpenCV and JuliaImages &#40;medium&#41;</a><ol><li><a href="#description__10">Description</a><li><a href="#skills__6">Skills</a><li><a href="#expected_outcomes__4">Expected outcomes</a><li><a href="#mentors__7">Mentors</a></ol><li><a href="#contributions_to_a_stereo_matching_package_medium">Contributions to a Stereo Matching Package &#40;medium&#41;</a><ol><li><a href="#description__11">Description</a><li><a href="#skills__7">Skills</a><li><a href="#expected_outcomes__5">Expected outcomes</a><li><a href="#mentors__8">Mentors</a></ol><li><a href="#contributions_to_a_calibration_target_package_medium">Contributions to a Calibration Target package &#40;medium&#41;</a><ol><li><a href="#description__12">Description</a><li><a href="#skills__8">Skills</a><li><a href="#expected_outcomes__6">Expected outcomes</a><li><a href="#mentors__9">Mentors</a><li><a href="#where_to_go_for_discussion_and_to_find_mentors">Where to go for discussion and to find mentors</a><li><a href="#expected_working_hours">Expected working hours</a></ol><li><a href="#c">C&#43;&#43;</a><ol><li><a href="#cxxwrap_stl">CxxWrap STL</a><ol><li><a href="#expected_outcome">Expected outcome</a></ol></ol><li><a href="#rust">Rust</a><li><a href="#improve_javis_performance">Improve Javis Performance</a><li><a href="#building_novel_animation_abilities_for_javis">Building Novel Animation Abilities for Javis</a><li><a href="#agentsjl">Agents.jl</a><li><a href="#dynamicalsystemsjl">DynamicalSystems.jl</a><li><a href="#midification_of_music_from_wave_files">MIDIfication of music from wave files</a><li><a href="#efficient_low-dimensional_symbolic-numeric_set_computations">Efficient low-dimensional symbolic-numeric set computations</a><li><a href="#reachability_with_sparse_polynomial_zonotopes">Reachability with sparse polynomial zonotopes</a><li><a href="#improving_the_hybrid_systems_reachability_api">Improving the hybrid systems reachability API</a><li><a href="#smoothing_non-linear_continuous_time_systems">Smoothing non-linear continuous time systems</a><li><a href="#improving_and_refining_the_ir_and_analysis">Improving and refining the IR and analysis</a><li><a href="#develop_the_frontend_to_infer_the_loop_structure">Develop the frontend to infer the loop structure</a><li><a href="#generating_code">Generating code</a><ol><li><a href="#cuda_hacking">CUDA Hacking</a><li><a href="#reinforcement_learning_environments">Reinforcement Learning Environments</a><li><a href="#reinforcement_learning_algorithms">Reinforcement Learning Algorithms</a><ol><li><a href="#expected_outcomes__7">Expected outcomes</a></ol><li><a href="#alphazerojl">AlphaZero.jl</a><ol><li><a href="#expected_outcomes__8">Expected outcomes</a></ol><li><a href="#nlp_tools_and_models">NLP Tools and Models</a><li><a href="#automated_music_generation">Automated music generation</a></ol><li><a href="#numerical_linear_algebra">Numerical Linear Algebra</a><ol><li><a href="#matrix_functions">Matrix functions</a></ol><li><a href="#better_bignums_integration">Better Bignums Integration</a><ol><li><a href="#special_functions">Special functions</a><li><a href="#a_julia-native_ccsa_optimization_algorithm">A Julia-native CCSA optimization algorithm</a></ol><li><a href="#pluto_as_a_vs_code_notebook">Pluto as a VS Code notebook</a><li><a href="#tools_for_education">Tools for education</a><li><a href="#wrapping_a_rust_http_server_in_julia">Wrapping a Rust HTTP server in Julia</a><ol><li><a href="#introduction">Introduction</a><li><a href="#details">Details</a></ol><li><a href="#machine_learning_time_series_regression">Machine Learning Time Series Regression</a><li><a href="#machine_learning_for_nowcasting_and_forecasting">Machine learning for nowcasting and forecasting</a><li><a href="#time_series_forecasting_at_scales">Time series forecasting at scales</a><li><a href="#gpu_accelerated_simulator_of_clifford_circuits">GPU accelerated simulator of Clifford Circuits.</a><ol><li><a href="#physics-informed_neural_networks_pinns_and_solving_differential_equations_with_deep_learning">Physics-Informed Neural Networks &#40;PINNs&#41; and Solving Differential Equations with Deep Learning</a><li><a href="#improvements_to_neural_and_universal_differential_equations">Improvements to Neural and Universal Differential Equations</a><li><a href="#accelerating_optimization_via_machine_learning_with_surrogate_models">Accelerating optimization via machine learning with surrogate models</a><li><a href="#parameter_estimation_for_nonlinear_dynamical_models">Parameter estimation for nonlinear dynamical models</a></ol><li><a href="#integration_of_fenicsjl_with_dolfin-adjoint_zygotejl_for_finite_element_scientific_machine_learning">Integration of FEniCS.jl with dolfin-adjoint &#43; Zygote.jl for Finite Element Scientific Machine Learning</a><li><a href="#multi-start_optimization_methods">Multi-Start Optimization Methods</a><li><a href="#groebner_basis_and_symbolic_root_finding">Groebner basis and Symbolic root finding</a><li><a href="#symbolic_integration">Symbolic Integration</a><li><a href="#implement_flashfill_in_julia">Implement Flashfill in Julia </a><li><a href="#parquetjl_enhancements">Parquet.jl enhancements</a><li><a href="#statistical_transforms">Statistical transforms</a><li><a href="#utility_transforms">Utility transforms</a><li><a href="#how_to_get_started__2">How to get started?</a><li><a href="#machine_learning_in_topology_optimisation">Machine learning in topology optimisation</a><li><a href="#multi-material_design_representation">Multi-material design representation</a><li><a href="#optimisation_on_a_uniform_rectilinear_grid">Optimisation on a uniform rectilinear grid</a><li><a href="#adaptive_mesh_refinement_for_topology_optimisation">Adaptive mesh refinement for topology optimisation</a><li><a href="#heat_transfer_design_optimisation">Heat transfer design optimisation</a><li><a href="#more_real-world_bayesian_models_in_turing_julia">More real-world Bayesian models in Turing / Julia</a><li><a href="#improving_the_integration_between_turing_and_turings_mcmc_inference_packages">Improving the integration between Turing and Turing&#39;s MCMC inference packages</a><li><a href="#directed-graphical_model_support_for_the_abstract_probabilistic_programming_library">Directed-graphical model support for the abstract probabilistic programming library</a><li><a href="#a_modular_tape_caching_mechanism_for_reversediff">A modular tape caching mechanism for ReverseDiff</a><li><a href="#benchmarking_improving_performance_of_the_juliagaussianprocesses_libraries">Benchmarking &amp; improving performance of the JuliaGaussianProcesses libraries</a><li><a href="#iterative_methods_for_inference_in_gaussian_processes">Iterative methods for inference in Gaussian Processes</a><li><a href="#approximate_inference_methods_for_non-gaussian_likelihoods_in_gaussian_processes">Approximate inference methods for non-Gaussian likelihoods in Gaussian Processes</a><li><a href="#gpu_integration_in_the_juliagps_ecosystem">GPU integration in the JuliaGPs ecosystem</a><li><a href="#vs_code_extension">VS Code extension</a><li><a href="#package_installation_ui">Package installation UI</a><li><a href="#code_generation_improvements_and_async_abi">Code generation improvements and async ABI</a><li><a href="#wasm_threading">Wasm threading</a><li><a href="#high_performance_low-level_integration_of_js_objects">High performance, Low-level integration of js objects</a><li><a href="#dom_integration">DOM Integration</a><li><a href="#porting_existing_web-integration_packages_to_the_wasm_platform">Porting existing web-integration packages to the wasm platform</a><li><a href="#native_dependencies_for_the_web">Native dependencies for the web</a><li><a href="#distributed_computing_with_untrusted_parties">Distributed computing with untrusted parties</a><li><a href="#deployment">Deployment</a></ol></div> <h2 id=numerical_linear_algebra ><a href="#numerical_linear_algebra">Numerical Linear Algebra</a></h2> <h3 id=matrix_functions ><a href="#matrix_functions">Matrix functions</a></h3> <p>Matrix functions map matrices onto other matrices, and can often be interpreted as generalizations of ordinary functions like sine and exponential, which map numbers to numbers. Once considered a niche province of numerical algorithms, matrix functions now appear routinely in applications to cryptography, aircraft design, nonlinear dynamics, and finance.</p> <p>This project proposes to implement state of the art algorithms that extend the currently available matrix functions in Julia, as outlined in issue <a href="https://github.com/JuliaLang/julia/issues/5840">#5840</a>. In addition to matrix generalizations of standard functions such as real matrix powers, surds and logarithms, students will be challenged to design generic interfaces for lifting general scalar-valued functions to their matrix analogues for the efficient computation of arbitrary &#40;well-behaved&#41; matrix functions and their derivatives.</p> <p><strong>Recommended Skills</strong>: A strong understanding of calculus and numerical analysis.</p> <p><strong>Expected Results</strong>: New and faster methods for evaluating matrix functions.</p> <p><strong>Mentors:</strong> <a href="https://github.com/jiahao">Jiahao Chen</a>, <a href="https://github.com/stevengj">Steven Johnson</a>.</p> <p><strong>Difficulty:</strong> Hard</p> <h2 id=better_bignums_integration ><a href="#better_bignums_integration">Better Bignums Integration</a></h2> <p>Julia currently supports big integers and rationals, making use of the GMP. However, GMP currently doesn&#39;t permit good integration with a garbage collector.</p> <p>This project therefore involves exploring ways to improve BigInt, possibly including:</p> <div class=tight-list ><ul> <li><p>Modifying GMP to support high-performance garbage-collection</p> <li><p>Reimplementation of aspects of BigInt in Julia</p> <li><p>Lazy graph style APIs which can rewrite terms or apply optimisations</p> </ul></div> <p>This experimentation could be carried out as a package with a new implementation, or as patches over the existing implementation in Base.</p> <p><strong>Expected Results</strong>: An implementation of BigInt in Julia with increased performance over the current one.</p> <p><strong>Require Skills</strong>: Familiarity with extended precision numerics OR performance considerations. Familiarity either with Julia or GMP.</p> <p><strong>Mentors</strong>: <a href="https://github.com/vtjnash">Jameson Nash</a></p> <p><strong>Difficulty:</strong> Hard</p> <h3 id=special_functions ><a href="#special_functions">Special functions</a></h3> <p>As a technical computing language, Julia provides a huge number of <a href="https://en.wikipedia.org/wiki/Special_functions">special functions</a>, both in Base as well as packages such as <a href="https://github.com/JuliaStats/StatsFuns.jl">StatsFuns.jl</a>. At the moment, many of these are implemented in external libraries such as <a href="https://github.com/JuliaLang/Rmath-julia">Rmath</a> and <a href="https://github.com/JuliaLang/openspecfun">openspecfun</a>. This project would involve implementing these functions in native Julia &#40;possibly utilising the work in <a href="https://github.com/nolta/SpecialFunctions.jl">SpecialFunctions.jl</a>&#41;, seeking out opportunities for possible improvements along the way, such as supporting <code>Float32</code> and <code>BigFloat</code>, exploiting fused multiply-add operations, and improving errors and boundary cases.</p> <p><strong>Recommended Skills</strong>: A strong understanding of calculus.</p> <p><strong>Expected Results</strong>: New and faster methods for evaluating properties of special functions.</p> <p><strong>Mentors:</strong> <a href="https://github.com/stevengj">Steven Johnson</a>, <a href="https://github.com/oscardssmith">Oscar Smith</a>. Ask on Discourse or on slack</p> <h3 id=a_julia-native_ccsa_optimization_algorithm ><a href="#a_julia-native_ccsa_optimization_algorithm">A Julia-native CCSA optimization algorithm</a></h3> <p>The CCSA algorithm by <a href="https://epubs.siam.org/doi/10.1137/S1052623499362822">Svanberg &#40;2001&#41;</a> is a <a href="https://en.wikipedia.org/wiki/Nonlinear_programming">nonlinear programming algorithm</a> widely used in <a href="https://en.wikipedia.org/wiki/Topology_optimization">topology optimization</a> and for other large-scale optimization problems: it is a robust algorithm that can handle arbitrary nonlinear inequality constraints and huge numbers of degrees of freedom. Moreover, the relative simplicity of the algorithm makes it possible to easily incorporate sparsity in the Jacobian matrix &#40;for handling huge numbers of constraints&#41;, approximate-Hessian preconditioners, and as special-case optimizations for affine terms in the objective or constraints. However, currently it is only available in Julia via the <a href="https://github.com/JuliaOpt/NLopt.jl">NLopt.jl</a> interface to an external C implementation, which greatly limits its flexibility.</p> <p><strong>Recommended Skills</strong>: Experience with nonlinear optimization algorithms and understanding of <a href="https://en.wikipedia.org/wiki/Duality_&#40;optimization&#41;">Lagrange duality</a>, familiarity with sparse matrices and other Julia data structures.</p> <p><strong>Expected Results</strong>: A package implementing a native-Julia CCSA algorithm.</p> <p><strong>Mentors:</strong> <a href="https://github.com/stevengj">Steven Johnson</a>.</p> <h1 id=plutojl_projects ><a href="#plutojl_projects">Pluto.jl projects</a></h1> <h2 id=pluto_as_a_vs_code_notebook ><a href="#pluto_as_a_vs_code_notebook">Pluto as a VS Code notebook</a></h2> <p>VS Code is an extensible editor, and one of its most recent features is a notebook GUI, with a corresponding <a href="https://code.visualstudio.com/api/extension-guides/notebook">Notebook API</a>, allowing extension developers to write their own <em>notebook backend</em>. We want to combine two popular Julia IDEs: VS Code and Pluto.jl, and use it to provide a mature editing and debugging experience combined with Pluto&#39;s reactivity.</p> <p><strong>Expected Results:</strong> Reactive notebook built on top of VSCode&#39;s notebook API.</p> <p><strong>Recommended skills:</strong> JavaScript/TypeScript, some Julia experience</p> <p><strong>Mentors:</strong> <a href="https://github.com/pfitzseb">Sebastian Pfitzner</a> &#40;core maintainer of julia-vscode&#41;, <a href="https://github.com/pankgeorg">Panagiotis Georgakopoulos</a> and <a href="https://github.com/fonsp">Fons van der Plas</a> &#40;core maintainers of Pluto.jl&#41; and friends</p> <p><em>Also see the other <a href="https://julialang.org/jsoc/gsoc/vscode/">VS Code projects</a>&#33;</em></p> <h2 id=tools_for_education ><a href="#tools_for_education">Tools for education</a></h2> <p>Pluto&#39;s primary use case is education, and we recently started using Pluto notebooks as an &#39;interactive textbook&#39;: https://computationalthinking.mit.edu/ . If you are interested in design and interactive visualization, there are lots of cool JS projects in this area. Examples include:</p> <ul> <li><p>Linking video content to dynamic content, better integration between exercise and lecture material.</p> <li><p>Experiment with playing back the edits to a notebook session, like a video, but on a scrollable page. &#40;<a href="https://www.notion.so/malyvsen/Replay-notebook-computations-8bcd4787842e40a199806ebe1c368acb">link</a>&#41;.</p> <li><p>Syntax analysis to automatically review &#39;code style&#39;</p> <li><p>Improved live check and autograding tools</p> <li><p>And so on&#33; Take a look at our <a href="https://www.notion.so/malyvsen/Pluto-jl-a9982e79b7bb4c658e6216c15a9d4cab">project board</a> and get in touch if you have further ideas: fons@plutojl.org</p> </ul> <p><strong>Expected Results:</strong> <em>One</em> of the items above&#33; When finished, your work will be used in future editions of the Computational Thinking course and more&#33;</p> <p><strong>Recommended skills:</strong> JavaScript, CSS, you can learn Julia as part of the project.</p> <p><strong>Mentors:</strong> <a href="https://github.com/fonsp">Fons van der Plas</a>, <a href="https://github.com/ctrekker">Connor Burns</a> and fellow Pluto.jl maintainers, with feedback from <a href="https://math.mit.edu/directory/profile.php?pid&#61;63">Alan Edelman</a></p> <h2 id=wrapping_a_rust_http_server_in_julia ><a href="#wrapping_a_rust_http_server_in_julia">Wrapping a Rust HTTP server in Julia</a></h2> <h3 id=introduction ><a href="#introduction">Introduction</a></h3> <p>Context: <em>Pluto is a notebook system written in Julia, which means that it runs an HTTP/WS web server in Julia. We currently use the <a href="https://github.com/JuliaWeb/HTTP.jl">HTTP.jl</a> for this, an ambitious project to write an HTTP server and client in pure Julia. While HTTP.jl works well in most scenarios, we still find that Pluto&#39;s connection is not always reliable. This is because people use Pluto on such a wide range of systems, with all kinds of network configurations, proxies, firewalls, browser interactions etc.</em> </p> <p>Looking for alternatives, we believe that, instead of using a pure-Julia implementation of HTTP, we should wrap around an existing, high-production web server like <a href="http://hyper.rs/">hyper.rs</a>. Julia has a rich history of wrapping libraries written in C, C&#43;&#43;, Python, Go, JS and more, and the package manager has first-class support for external binaries.</p> <h3 id=details ><a href="#details">Details</a></h3> <p>As a participant of this project, you will build on top of the Julia and Rust ecosystems. A potential starting point would be looking at the Deno <a href="https://github.com/denoland/deno/blob/2dc5dba8baf148a525cbb7987cdad0ba6398c5e4/ext/http/lib.rs">http server</a> implementation also built on top of hyper.rs. Initially, the goal would be to start using the <a href="https://docs.rs/hyper/latest/hyper/ffi/index.html">hyper C API</a> to interoperate with Julia &#40;there is already a <a href="https://github.com/JuliaBinaryWrappers/hyper_jll.jl">hyper_jll</a> package ❤ &#33;&#33;&#41;. Depending on the progress, another area of exploration is to investigate rustier tools like <a href="https://github.com/Taaitaaiger/jlrs">jlrs</a>.</p> <p><strong>Expected Results:</strong> A prototype of wrapping the <code>hyper</code> library in Julia, with a focus on reliability and efficiency, forming the basis of the package.</p> <p><strong>Recommended skills:</strong> Rust, some Julia experience, some previous experience with language interoperability or inter-process communication.</p> <p><strong>Mentors:</strong> <a href="https://github.com/pangoraw">Paul Berg</a> and <a href="https://github.com/fonsp">Fons van der Plas</a></p> <h1 id=pythia_summer_of_code ><a href="#pythia_summer_of_code">Pythia – Summer of Code</a></h1> <h2 id=machine_learning_time_series_regression ><a href="#machine_learning_time_series_regression">Machine Learning Time Series Regression</a></h2> <p><a href="https://github.com/ababii/Pythia.jl">Pythia</a> is a package for scalable machine learning time series forecasting and nowcasting in Julia.</p> <p>The project mentors are <a href="https://ababii.github.io/">Andrii Babii</a> and <a href="https://www.turing.ac.uk/people/researchers/sebastian-vollmer/">Sebastian Vollmer</a>.</p> <h2 id=machine_learning_for_nowcasting_and_forecasting ><a href="#machine_learning_for_nowcasting_and_forecasting">Machine learning for nowcasting and forecasting</a></h2> <p>This project involves developing scalable machine learning time series regressions for nowcasting and forecasting. Nowcasting in economics is the prediction of the present, the very near future, and the very recent past state of an economic indicator. The term is a contraction of &quot;now&quot; and &quot;forecasting&quot; and originates in meteorology. </p> <p>The objective of this project is to introduce scalable regression-based nowcasting and forecasting methodologies that demonstrated the empirical success in data-rich environment recently. Examples of existing popular packages for regression-based nowcasting on other platforms include the &quot;MIDAS Matlab Toolbox&quot;, as well as the &#39;midasr&#39; and &#39;midasml&#39; packages in R. The starting point for this project is porting the &#39;midasml&#39; package from R to Julia. Currently Pythia has the sparse-group LASSO regression functionality for forecasting. </p> <p>The following functions are of interest: in-sample and out-of sample forecasts/nowcasts, regularized MIDAS with Legendre polynomials, visualization of nowcasts, AIC/BIC and time series cross-validation tuning, forecast evaluation, pooled and fixed effects panel data regressions for forecasting and nowcasting, HAC-based inference for sparse-group LASSO, high-dimensional Granger causality tests. Other widely used existing functions from R/Python/Matlab are also of interest.</p> <p><strong>Recommended skills:</strong> Graduate-level knowledge of time series analysis, machine learning, and optimization is helpful.</p> <p><strong>Expected output:</strong> The student is expected to produce code, documentation, visualization, and real-data examples.</p> <p><strong>References:</strong> Contact project mentors for references.</p> <h2 id=time_series_forecasting_at_scales ><a href="#time_series_forecasting_at_scales">Time series forecasting at scales</a></h2> <p>Modern business applications often involve forecasting hundreds of thousands of time series. Producing such a gigantic number of reliable and high-quality forecasts is computationally challenging, which limits the scope of potential methods that can be used in practice, see, e.g., the &#39;forecast&#39;, &#39;fable&#39;, or &#39;prophet&#39; packages in R. Currently, Julia lacks the scalable time series forecasting functionality and this project aims to develop the automated data-driven and scalable time series forecasting methods. </p> <p>The following functionality is of interest: forecasting intermittent demand &#40;Croston, adjusted Croston, INARMA&#41;, scalable seasonal ARIMA with covariates, loss-based forecasting &#40;gradient boosting&#41;, unsupervised time series clustering, forecast combinations, unit root tests &#40;ADF, KPSS&#41;. Other widely used existing functions from R/Python/Matlab are also of interest.</p> <p><strong>Recommended skills:</strong> Graduate-level knowledge of time series analysis is helpful.</p> <p><strong>Expected output:</strong> The student is expected to produce code, documentation, visualization, and real-data examples.</p> <p><strong>References:</strong> Contact project mentors for references.</p> <h1 id=tools_for_simulation_of_quantum_clifford_circuits ><a href="#tools_for_simulation_of_quantum_clifford_circuits">Tools for simulation of Quantum Clifford Circuits</a></h1> <p>Clifford circuits are a class of quantum circuits that can be simulated efficiently on a classical computer. As such, they do not provide the computational advantage expected of universal quantum computers. Nonetheless, they are extremely important, as they underpin most techniques for quantum error correction and quantum networking. Software that efficiently simulates such circuits, at the scale of thousands or more qubits, is essential to the design of quantum hardware. The <a href="https://github.com/Krastanov/QuantumClifford.jl">QuantumClifford.jl</a> Julia project enables such simulations.</p> <h2 id=gpu_accelerated_simulator_of_clifford_circuits ><a href="#gpu_accelerated_simulator_of_clifford_circuits">GPU accelerated simulator of Clifford Circuits.</a></h2> <p>Simulation of Clifford circuits involves significant amounts of linear algebra with boolean matrices. This enables the use of many standard computation accelerators like GPUs, as long as these accelerators support bit-wise operations. The main complications is that the elements of the matrices under consideration are usually packed in order to increase performance and lower memory usage, i.e. a vector of 64 elements would be stored as a single 64 bit integer instead of as an array of 64 bools. A Summer of Code project could consist of implement the aforementioned linear algebra operations in GPU kernels, and then seamlessly integrating them in the rest of the QuantumClifford library. At a minimum that would include <a href="https://github.com/Krastanov/QuantumClifford.jl/blob/v0.4.0/src/QuantumClifford.jl#L725">Pauli-Pauli products</a> and certain <a href="https://github.com/Krastanov/QuantumClifford.jl/blob/v0.4.0/src/symbolic_cliffords.jl">small Clifford operators</a>, but could extend to general <a href="https://github.com/Krastanov/QuantumClifford.jl/blob/v0.4.0/src/QuantumClifford.jl#L1385">stabilizer tableau multiplication</a> and even <a href="https://github.com/Krastanov/QuantumClifford.jl/blob/v0.4.0/src/QuantumClifford.jl#L985">tableau diagonalization</a>. </p> <p><strong>Recommended skills:</strong> Basic knowledge of the <a href="https://krastanov.github.io/QuantumClifford.jl/dev/references/">stabilizer formalism</a> used for simulating Clifford circuits. Familiarity with performance profiling tools in Julia and Julia&#39;s GPU stack, including <a href="https://github.com/JuliaGPU/KernelAbstractions.jl">KernelAbstractions</a> and <a href="https://github.com/mcabbott/Tullio.jl">Tullio</a>.</p> <p><strong>Mentors:</strong> <a href="https://github.com/Krastanov">Stefan Krastanov</a></p> <p><strong>Expected duration:</strong> 175 hours &#40;but applicants can scope it to a longer project by including work on GPU-accelerated Gaussian elimination used in the canonicalization routines&#41;</p> <p><strong>Difficulty:</strong> Easy/medium if the applicant is familiar with Julia, even without understanding of Quantum Information Science &#40;but applicants can scope it to &quot;hard&quot; by including the afforementioned additional topics&#41;</p> <h1 id=scientific_machine_learning_sciml_projects ><a href="#scientific_machine_learning_sciml_projects">Scientific Machine Learning &#40;SciML&#41; Projects</a></h1> <p>These projects are hosted by the <a href="https://sciml.ai/">SciML Open Source Scientific Machine Learning Software Organzation</a>.</p> <h3 id=physics-informed_neural_networks_pinns_and_solving_differential_equations_with_deep_learning ><a href="#physics-informed_neural_networks_pinns_and_solving_differential_equations_with_deep_learning">Physics-Informed Neural Networks &#40;PINNs&#41; and Solving Differential Equations with Deep Learning</a></h3> <p>Neural networks can be used as a method for efficiently solving difficult partial differential equations. Recently this strategy has been dubbed <a href="https://www.sciencedirect.com/science/article/pii/S0021999118307125">physics-informed neural networks</a> and has seen a resurgence because of its efficiency advantages over classical deep learning. Efficient implementations from recent papers are being explored as part of the <a href="https://github.com/SciML/NeuralNetDiffEq.jl">NeuralNetDiffEq.jl</a> package. The <a href="https://github.com/SciML/NeuralNetDiffEq.jl/issues">issue tracker</a> contains links to papers which would be interesting new neural network based methods to implement and benchmark against classical techniques. Project work in this area includes:</p> <ul> <li><p><a href="https://github.com/SciML/NeuralNetDiffEq.jl/issues/71">Improved training strategies</a> for PINNs.</p> <li><p>Implementing new neural architectures that impose physical constraints like <a href="https://arxiv.org/pdf/2002.00021.pdf">divergence-free criteria</a>.</p> <li><p>Demonstrating large-scale problems solved by PINN training.</p> <li><p>Improving the speed and parallelization of PINN training routines.</p> </ul> <p>This project is good for both software engineers interested in the field of scientific machine learning and those students who are interested in perusing graduate research in the field.</p> <p><strong>Recommended Skills</strong>: Background knowledge in numerical analysis and machine learning.</p> <p><strong>Expected Results</strong>: New neural network based solver methods.</p> <p><strong>Mentors</strong>: <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a></p> <h3 id=improvements_to_neural_and_universal_differential_equations ><a href="#improvements_to_neural_and_universal_differential_equations">Improvements to Neural and Universal Differential Equations</a></h3> <p><a href="https://arxiv.org/abs/1806.07366">Neural ordinary differential equations</a> have been shown to be a way to use machine learning to learn differential equation models. Further improvements to the methodology, like <a href="https://arxiv.org/abs/2001.04385">universal differential equations</a> have incorporated physical and biological knowledge into the system in order to make it a data and compute efficient learning method. However, there are many computational aspects left to explore. The purpose of this project is to enhance the universal differential equation approximation abilities of <a href="https://github.com/SciML/DiffEqFlux.jl">DiffEqFlux.jl</a>, adding features like:</p> <ul> <li><p>Improved adjoints for DAEs and SDEs</p> <li><p><a href="https://github.com/SciML/DiffEqFlux.jl/issues/173">Non-neural network universal approximators</a></p> <li><p>Various <a href="https://github.com/SciML/DiffEqFlux.jl/issues/133">improvements to</a> <a href="https://github.com/SciML/DiffEqFlux.jl/issues/118">minibatching</a></p> <li><p>Support for <a href="https://github.com/SciML/DiffEqFlux.jl/issues/48">second order ODEs &#40;i.e. symplectic integrators&#41;</a></p> <li><p><a href="https://github.com/SciML/DiffEqFlux.jl/issues/46">Continuous normalizing flows</a> and <a href="https://github.com/SciML/DiffEqFlux.jl/issues/47">FFJORD</a></p> </ul> <p>See the <a href="https://github.com/SciML/DiffEqFlux.jl/issues">DiffEqFlux.jl issue tracker</a> for full details.</p> <p>This project is good for both software engineers interested in the field of scientific machine learning and those students who are interested in perusing graduate research in the field.</p> <p><strong>Recommended Skills</strong>: Background knowledge in numerical analysis and machine learning.</p> <p><strong>Expected Results</strong>: New and improved methods for neural and universal differential equations.</p> <h3 id=accelerating_optimization_via_machine_learning_with_surrogate_models ><a href="#accelerating_optimization_via_machine_learning_with_surrogate_models">Accelerating optimization via machine learning with surrogate models</a></h3> <p>In many cases, when attempting to optimize a function <code>f&#40;p&#41;</code> each calculation of <code>f</code> is very expensive. For example, evaluating <code>f</code> may require solving a PDE or other applications of complex linear algebra. Thus, instead of always directly evaluating <code>f</code>, one can develop a surrogate model <code>g</code> which is approximately <code>f</code> by training on previous data collected from <code>f</code> evaluations. This technique of using a trained surrogate in place of the real function is called surrogate optimization and mixes techniques from machine learning to accelerate optimization.</p> <p>Advanced techniques <a href="https://www.cambridge.org/core/journals/acta-numerica/article/kernel-techniques-from-machine-learning-to-meshless-methods/00686923110F799A1537C4F02BBAAE8E">utilize radial basis functions</a> and Gaussian processes in order to interpolate to new parameters to estimate <code>f</code> in areas which have not been sampled. <a href="http&#37;3A&#37;2F&#37;2Fwww.ressources-actuarielles.net&#37;2FEXT&#37;2FISFA&#37;2F1226.nsf&#37;2F9c8e3fd4d8874d60c1257052003eced6&#37;2Fe7dc33e4da12c5a9c12576d8002e442b&#37;2F&#37;24FILE&#37;2FJones01.pdf">Adaptive training techniques</a> explore how to pick new areas to evaluate <code>f</code> to better hone in on global optima. The purpose of this project is to explore these techniques and build a package which performs surrogate optimizations.</p> <p><strong>Recommended Skills</strong>: Background knowledge of standard machine learning, statistical, or optimization techniques. Strong knowledge of numerical analysis is helpful but not required.</p> <p><strong>Expected Results</strong>: Library functions for performing surrogate optimization with tests on differential equation models.</p> <p><strong>Mentors</strong>: <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a>, <a href="https://github.com/MartinuzziFrancesco">Francesco Martinuzzi</a></p> <h3 id=parameter_estimation_for_nonlinear_dynamical_models ><a href="#parameter_estimation_for_nonlinear_dynamical_models">Parameter estimation for nonlinear dynamical models</a></h3> <p>Machine learning has become a popular tool for understanding data, but scientists typically understand the world through the lens of physical laws and their resulting dynamical models. These models are generally differential equations given by physical first principles, where the constants in the equations such as chemical reaction rates and planetary masses determine the overall dynamics. The inverse problem to simulation, known as parameter estimation, is the process of utilizing data to determine these model parameters.</p> <p>The purpose of this project is to utilize the growing array of statistical, optimization, and machine learning tools in the Julia ecosystem to build library functions that make it easy for scientists to perform this parameter estimation with the most high-powered and robust methodologies. Possible projects include improving methods for Bayesian estimation of parameters via Stan.jl and Julia-based libraries like Turing.jl, or global optimization-based approaches. Novel techniques like classifying model outcomes via support vector machines and deep neural networks can also be considered. Research and benchmarking to attempt to find the most robust methods will take place in this project. Additionally, the implementation of methods for estimating structure, such as <a href="https://www.pnas.org/content/111/52/18507">topological sensitivity analysis</a> along with performance enhancements to existing methods will be considered.</p> <p>Some work in this area can be found in <a href="https://github.com/SciML/DiffEqParamEstim.jl">DiffEqParamEstim.jl</a> and <a href="https://github.com/SciML/DiffEqBayes.jl">DiffEqBayes.jl</a>. Examples can be found <a href=" https://docs.sciml.ai/dev/analysis/parameter_estimation">in the DifferentialEquations.jl documentation</a>.</p> <p><strong>Recommended Skills</strong>: Background knowledge of standard machine learning, statistical, or optimization techniques. It&#39;s recommended but not required that one has basic knowledge of differential equations and DifferentialEquations.jl. Using the differential equation solver to get outputs from parameters can be learned on the job, but you should already be familiar &#40;but not necessarily an expert&#41; with the estimation techniques you are looking to employ.</p> <p><strong>Expected Results</strong>: Library functions for performing parameter estimation and inferring properties of differential equation solutions from parameters. Notebooks containing benchmarks determining the effectiveness of various methods and classifying when specific approaches are appropriate will be developed simultaneously.</p> <p><strong>Mentors</strong>: <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a>, <a href="https://github.com/Vaibhavdixit02">Vaibhav Dixit</a></p> <h2 id=integration_of_fenicsjl_with_dolfin-adjoint_zygotejl_for_finite_element_scientific_machine_learning ><a href="#integration_of_fenicsjl_with_dolfin-adjoint_zygotejl_for_finite_element_scientific_machine_learning">Integration of FEniCS.jl with dolfin-adjoint &#43; Zygote.jl for Finite Element Scientific Machine Learning</a></h2> <p>Scientific machine learning requires mixing scientific computing libraries with machine learning. <a href="https://www.stochasticlifestyle.com/the-essential-tools-of-scientific-machine-learning-scientific-ml/">This blog post highlights how the tooling of Julia is fairly advanced in this field</a> compared to alternatives such as Python, but one area that has not been completely worked out is integration of automatic differentiation with partial differential equations. <a href="https://github.com/SciML/FEniCS.jl">FEniCS.jl</a> is a wrapper to the <a href="https://fenicsproject.org/">FEniCS</a> project for finite element solutions of partial differential equations. We would like to augment the Julia wrappers to allow for integration with Julia&#39;s automatic differentiation libraries like <a href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a> by using <a href="http://www.dolfin-adjoint.org/en/release/">dolfin-adjoint</a>. This would require setting up this library for automatic installation for Julia users and writing adjoint passes which utilize this adjoint builder library. It would result in the first total integration between PDEs and neural networks.</p> <p><strong>Recommended Skills</strong>: A basic background in differential equations and Python. Having previous Julia knowledge is preferred but not strictly required.</p> <p><strong>Expected Results</strong>: Efficient and high-quality implementations of adjoints for Zygote.jl over FEniCS.jl functions.</p> <p><strong>Mentors</strong>: <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a></p> <h2 id=multi-start_optimization_methods ><a href="#multi-start_optimization_methods">Multi-Start Optimization Methods</a></h2> <p>While standard machine learning can be shown to be &quot;safe&quot; for local optimization, scientific machine learning can sometimes require the use of globalizing techniques to improve the optimization process. Hybrid methods, known as multistart optimization methods, glue together a local optimization technique together with a parameter search over a large space of possible initial points. The purpose of this project would be to take a <a href="https://github.com/tpapp/MultistartOptimization.jl">MultistartOptimization.jl</a> as a starting point and create a fully featured set of multistart optimization tools for use with <a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a></p> <p><strong>Recommended Skills</strong>: A basic background in optimization. Having previous Julia knowledge is preferred but not strictly required.</p> <p><strong>Expected Results</strong>: Efficient and high-quality implementations of multistart optimization methods.</p> <p><strong>Mentors</strong>: <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a> and <a href="https://github.com/pkofod">Patrick Kofod Mogensen</a></p> <h1 id=symbolic_computation_project_ideas ><a href="#symbolic_computation_project_ideas">Symbolic computation project ideas</a></h1> <h2 id=groebner_basis_and_symbolic_root_finding ><a href="#groebner_basis_and_symbolic_root_finding">Groebner basis and Symbolic root finding</a></h2> <p>Implement solving polynomial equation systems symbolically. &#40;I.e. finding the variety of a set of polynomials&#41;. This involves first computing the Groebner basis for a set of polynomials. Groebner basis computation is NP complete so it is essential that the implementation is practical. It should start by studying the literature on state-of-the art Groebner basis solvers.</p> <p><strong>Recommended Skills</strong>: Calculus and discrete mathematics. Prior knowledge of computational algebra and ring theory is preferred.</p> <p><strong>Expected Results</strong>: Working Groebner basis and rootfinding algorithms to be deployed in the Symbolics.jl package, along with documentation and tutorials.</p> <p><strong>Mentors</strong>: <a href="https://github.com/shashi">Shashi Gowda</a>, <a href="https://github.com/YingboMa">Yingbo Ma</a>, <a href="https://github.com/MasonProtter">Mason Protter</a></p> <h2 id=symbolic_integration ><a href="#symbolic_integration">Symbolic Integration</a></h2> <p>Implement the <a href="https://dspace.mit.edu/handle/1721.1/11997">heuristic approach to symbolic integration</a>. Then hook into a repository of rules such as <a href="https://rulebasedintegration.org/">RUMI</a></p> <p><strong>Recommended Skills</strong>: Calculus</p> <p><strong>Expected Results</strong>: A working implementation of symbolic integration in the Symbolics.jl library, along with documentation and tutorials demonstrating its use in scientific disciplines.</p> <p><strong>Mentors</strong>: <a href="https://github.com/shashi">Shashi Gowda</a>, <a href="https://github.com/YingboMa">Yingbo Ma</a>, <a href="https://github.com/MasonProtter">Mason Protter</a></p> <h1 id=tabular_data_summer_of_code ><a href="#tabular_data_summer_of_code">Tabular Data – Summer of Code</a></h1> <h2 id=implement_flashfill_in_julia ><a href="#implement_flashfill_in_julia">Implement Flashfill in Julia </a></h2> <p><strong>Difficulty</strong>: Medium</p> <p><em>FlashFill</em> is mechanism for creating data manipulation pipelines using programming by example &#40;PBE&#41;. As an example see this <a href="https://support.microsoft.com/en-us/office/using-flash-fill-in-excel-3f9bcf1e-db93-4890-94a0-1578341f73f7">implementation in Microsoft Excel</a>. We want a version of Flashfill that can work against Julia tabular data structures, such as DataFrames and Tables. </p> <p><strong>Resources</strong>:</p> <div class=tight-list ><ul> <li><p>A <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/04/pldi16-tutorial.pptx">presentation</a> by Sumit Gulwani of Microsoft Research</p> <li><p>A <a href="https://youtu.be/X1YXge3C8RI">video</a></p> <li><p><a href="https://www.microsoft.com/en-us/research/group/prose/">MSR Prose research group</a></p> <li><p><a href="https://www.microsoft.com/en-us/research/group/prose/#&#33;publications">Papers</a></p> </ul></div> <p><strong>Recommended Skills</strong>: Compiler techniques, DSL generation, Program synthesis</p> <p><strong>Expected Output</strong>: A practical flashfill implementation that can be used on any tabular data structure in Julia</p> <p><strong>Mentors</strong>: <a href="https://github.com/aviks/">Avik Sengupta</a></p> <h2 id=parquetjl_enhancements ><a href="#parquetjl_enhancements">Parquet.jl enhancements</a></h2> <p><strong>Difficulty</strong>: Medium</p> <p><a href="https://parquet.apache.org/">Apache Parquet</a> is a binary data format for tabular data. It has features for compression and memory-mapping of datasets on disk. A decent implementation of Parquet in Julia is likely to be highly performant. It will be useful as a standard format for distributing tabular data in a binary format. There exists a Parquet.jl package that has a Parquet reader and a writer. It currently conforms to the Julia Tabular file IO interface at a very basic level. It needs more work to add support for critical elements that would make Parquet.jl usable for fast large scale parallel data processing. One or more of the following goals can be targeted:</p> <div class=tight-list ><ul> <li><p>Lazy loading and support for out-of-core processing, with Arrow.jl and Tables.jl integration. Improved usability and performance of Parquet reader and writer for large files.</p> <li><p>Reading from and writing data on to cloud data stores, including support for partitioned data.</p> <li><p>Support for missing data types and encodings making the Julia implementation fully featured.</p> </ul></div> <p><strong>Resources:</strong></p> <div class=tight-list ><ul> <li><p>The <a href="https://parquet.apache.org/documentation/latest/">Parquet</a> file format &#40;also are many articles and talks on the Parquet storage format on the internet&#41;</p> <li><p><a href="https://quinnj.home.blog/2019/07/21/a-tour-of-the-data-ecosystem-in-julia/">A tour of the data ecosystem in Julia</a></p> <li><p><a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a></p> <li><p><a href="https://github.com/JuliaData/Arrow.jl">Arrow.jl</a></p> </ul></div> <p><strong>Recommended skills:</strong> Good knowledge of Julia language, Julia data stack and writing performant Julia code.</p> <p><strong>Expected Results:</strong> Depends on the specific projects we would agree on.</p> <p><strong>Mentors:</strong> <a href="https://github.com/shashi">Shashi Gowda</a>, <a href="https://github.com/tanmaykm">Tanmay Mohapatra</a></p> <h1 id=tabletransformsjl_-_summer_of_code ><a href="#tabletransformsjl_-_summer_of_code">TableTransforms.jl - Summer of Code</a></h1> <p><a href="https://github.com/JuliaML/TableTransforms.jl">TableTransforms.jl</a> provides transforms that are commonly used in statistics and machine learning. It was developed to address specific needs in feature engineering and works with general Tables.jl tables.</p> <p>Project mentors: <a href="https://github.com/juliohm">Júlio Hoffimann</a></p> <h2 id=statistical_transforms ><a href="#statistical_transforms">Statistical transforms</a></h2> <p>Statistical transforms such as PCA, Z-score, etc, can greatly improve the convergence of various statistical learning models, and are widely used in advanced machine learning pipelines. In this project the mentee will learn how to implement advanced transforms such as PPMT and other transforms for imputation of missing values.</p> <p><strong>Desired skills:</strong> Statistics, Machine Learning</p> <p><strong>Difficulty level:</strong> Medium</p> <p><strong>Expected duration:</strong> 350hrs</p> <p><strong>References:</strong></p> <div class=tight-list ><ul> <li><p><a href="https://geostatisticslessons.com/lessons/ppmt">Project Pursuit Multivariate Transform</a></p> </ul></div> <h2 id=utility_transforms ><a href="#utility_transforms">Utility transforms</a></h2> <p>Utility transforms such as standardization of column names and other string-based transforms are extremely important for digesting real-world data. In this project the mentee will learn good coding practices and will implement various utility transforms available in other languages &#40;e.g. Janitor package in R, pyjanitor in Python&#41;.</p> <p><strong>Desired skills:</strong> Text processing, Regex</p> <p><strong>Difficulty level:</strong> Easy</p> <p><strong>Expected duration:</strong> 175hrs</p> <p><strong>References:</strong></p> <div class=tight-list ><ul> <li><p><a href="https://garthtarr.github.io/meatR/janitor.html">Janitor R</a></p> <li><p><a href="https://github.com/pyjanitor-devs/pyjanitor">pyjanitor</a></p> </ul></div> <h2 id=how_to_get_started__2 ><a href="#how_to_get_started__2">How to get started?</a></h2> <p>Address <a href="https://github.com/JuliaML/TableTransforms.jl">open issues in the package</a>.</p> <p>Please contact <a href="https://github.com/juliohm">Júlio Hoffimann</a> on <a href="https://julialang.zulipchat.com">Zulip</a> if you have any questions.</p> <h1 id=topopt_projects_summer_of_code ><a href="#topopt_projects_summer_of_code">TopOpt Projects – Summer of Code</a></h1> <p><a href="https://github.com/JuliaTopOpt/TopOpt.jl">TopOpt.jl</a> is a <a href="https://en.wikipedia.org/wiki/Topology_optimization">topology optimisation</a> package written in pure Julia. Topology optimisation is an exciting field at the intersection of shape representation, physics simulations and mathematical optimisation, and the Julia language is a great fit for this field. To learn more about <code>TopOpt.jl</code>, check the following <a href="https://www.youtube.com/watch?v&#61;sBqdkxPXluU">JuliaCon talk</a>.</p> <p>The following is a tentative list of projects in topology optimisation that you could be working on in the coming Julia Season of Contributions or Google Summer of Code. If you are interested in exploring any of these topics or if you have other interests related to topology optimisation, please reach out to the main mentor <a href="https://github.com/mohamed82008">Mohamed Tarek</a> via email.</p> <h2 id=machine_learning_in_topology_optimisation ><a href="#machine_learning_in_topology_optimisation">Machine learning in topology optimisation</a></h2> <p><strong>Project difficulty</strong>: Easy to Medium</p> <p><strong>Description</strong>: There are numerous ways to use machine learning for design optimisation in topology optimisation. The following are all recent papers with applications of neural networks and machine learning in topology optimisation. There are also exciting research opportunities in this direction.</p> <ul> <li><p><a href="https://openreview.net/pdf?id&#61;DUy-qLzqvlU">DNN-based Topology Optimisation: Spatial Invariance and Neural Tangent Kernel</a></p> <li><p><a href="https://openreview.net/pdf?id&#61;bBHHU4dW88g">NTopo: Mesh-free Topology Optimization using Implicit Neural Representations</a></p> <li><p><a href="https://www.sciencedirect.com/science/article/pii/S004578252100414X?via&#37;3Dihub">TONR: An exploration for a novel way combining neural network with topology optimization</a></p> <li><p><a href="https://link.springer.com/article/10.1007/s00158-020-02748-4">TOuNN: Topology Optimization using Neural Networks</a></p> </ul> <p>In this project you will implement one of the algorithms discussed in any of these papers.</p> <p><strong>Knowledge prerequisites</strong>: neural networks, optimisation, Julia programming</p> <h2 id=multi-material_design_representation ><a href="#multi-material_design_representation">Multi-material design representation</a></h2> <p><strong>Project difficulty</strong>: Easy</p> <p><strong>Description</strong>: There are some topology optimisation formulations that enable the optimisation of the shape of the structure and the material selected simultaneously. In this project, you will implement some multi-material design optimisation formulations, e.g. <a href="https://link.springer.com/article/10.1007/s00158-011-0625-z">this paper</a> has a relatively simple approach to integrate in TopOpt.jl. Other methods include using mixed integer nonlinear programming from <a href="https://github.com/JuliaNonconvex/Nonconvex.jl">Nonconvex.jl</a> to select materials in different parts of the design.</p> <p><strong>Knowledge prerequisites</strong>: basic optimisation, Julia programming</p> <h2 id=optimisation_on_a_uniform_rectilinear_grid ><a href="#optimisation_on_a_uniform_rectilinear_grid">Optimisation on a uniform rectilinear grid</a></h2> <p><strong>Project difficulty</strong>: Medium</p> <p><strong>Description</strong>: Currently in TopOpt.jl, there are only unstructured meshes supported. This is a very flexible type of mesh but it&#39;s not as memory efficient as uniform rectilinear grids where all the elements are assumed to have the same shape. This is the most common grid used in topology optimisation in practice. Currently in TopOpt.jl, the uniform rectilinear grid will be stored as an unstructured mesh which is unnecessarily inefficient. In this project, you will optimise the finite element analysis and topology optimisation codes in TopOpt.jl for uniform rectilinear grids.</p> <p><strong>Knowledge prerequisites</strong>: knowledge of mesh types, Julia programming</p> <h2 id=adaptive_mesh_refinement_for_topology_optimisation ><a href="#adaptive_mesh_refinement_for_topology_optimisation">Adaptive mesh refinement for topology optimisation</a></h2> <p><strong>Project difficulty</strong>: Medium</p> <p><strong>Description</strong>: Topology optimisation problems with more mesh elements take longer to simulate and to optimise. In this project, you will explore the use of adaptive mesh refinement starting from a coarse mesh, optimising and only refining the elements that need further optimisation. This is an effective way to accelerate topology optimisation algorithms.</p> <p><strong>Knowledge prerequisites</strong>: adaptive mesh refinement, Julia programming</p> <h2 id=heat_transfer_design_optimisation ><a href="#heat_transfer_design_optimisation">Heat transfer design optimisation</a></h2> <p><strong>Project difficulty</strong>: Medium</p> <p><strong>Description</strong>: All of the examples in TopOpt.jl and problem types are currently of the linear elasticity, quasi-static class of problems. The goal of this project is to implement more problem types and examples from the field of heat transfer. Both steady-state heat transfer problems and linear elasticity problems make use of elliptic partial differential equations so the code from linear elasticity problems should be largely reusable for heat transfer problems with minimum changes.</p> <p><strong>Knowledge prerequisites</strong>: finite element analysis, heat equation, Julia programming</p> <h1 id=turing_projects_summer_of_code ><a href="#turing_projects_summer_of_code">Turing Projects – Summer of Code</a></h1> <p><a href="https://turing.ml/">Turing</a> is a universal probabilistic programming language embedded in Julia. Turing allows the user to write models in standard Julia syntax, and provide a wide range of sampling-based inference methods for solving problems across probabilistic machine learning, Bayesian statistics and data science etc. Since Turing is implemented in pure Julia code, its compiler and inference methods are amenable to hacking: new model families and inference methods can be easily added. Below is a list of ideas for potential projects, though you are welcome to propose your own to the Turing team.</p> <p>If you are interested in exploring any of these projects, please reach out to the listed project mentors. You can find their contact information at <a href="https://turing.ml/stable/team">turing.ml/team</a>.</p> <h2 id=more_real-world_bayesian_models_in_turing_julia ><a href="#more_real-world_bayesian_models_in_turing_julia">More real-world Bayesian models in Turing / Julia</a></h2> <p><strong>Mentors</strong>: Kai Xu, Tor E. Fjelde, Hong Ge</p> <p><strong>Project difficulty</strong>: Medium</p> <p><strong>Project length</strong>: 175 hrs or 350 hrs</p> <p><strong>Description</strong>: There are many real-world Bayesian models out there, and they deserve a Turing / Julia implementation.</p> <p>Examples include but not limited to </p> <ul> <li><p>Forecasting &#40;<a href="https://facebook.github.io/prophet/">Prophet</a>, <a href="https://github.com/facebook/prophet/tree/main/examples">datasets</a>&#41;</p> <li><p>Recommender system &#40;<a href="http://www.cs.utoronto.ca/~amnih/papers/pmf.pdf">probabilistic matrix factorisation</a>, <a href="https://grouplens.org/datasets/movielens/">dataset</a>&#41;</p> <li><p>Ranking &#40;<a href="https://en.wikipedia.org/wiki/TrueSkill">TrueSkill</a>, <a href="https://github.com/dotnet/mbmlbook/tree/main/src/3.&#37;20Meeting&#37;20Your&#37;20Match/Data">dataset</a>&#41;</p> <li><p>Bayesian revenue estimation &#40;<a href="https://www.smartly.io/blog/tutorial-how-we-productized-bayesian-revenue-estimation-with-stan">example</a>&#41;</p> <li><p>Political forecasting model &#40;<a href="https://github.com/sjwild/Canandian_Election_2021">example</a>&#41;</p> <li><p>Topic mining &#40;latent Dirichlet allocation and new variants&#41;</p> <li><p>Multiple Annotators/Combining Unreliable Observations &#40;Dawid and Skene, 1979&#41;</p> </ul> <p>For each model, we consider the following tasks</p> <ul> <li><p>Correctness test: correctness of the implementation can be tested by doing inference for prior samples, for which we know the ground truth latent variables.</p> <li><p>Performance benchmark: this includes &#40;i&#41; time per MCMC step and &#40;ii&#41; time per effective sample; if the model is differentiable, a further break-down of &#40;i&#41; into &#40;i.1&#41; time per forward pass and &#40;i.2&#41; time per gradient pass are needed.</p> <li><p>Real-world results: if available, the final step is to apply the model to a real-world dataset; if such an experiment has been done in the literature, consistency of inference results needs to be checked</p> </ul> <h2 id=improving_the_integration_between_turing_and_turings_mcmc_inference_packages ><a href="#improving_the_integration_between_turing_and_turings_mcmc_inference_packages">Improving the integration between Turing and Turing&#39;s MCMC inference packages</a></h2> <p><strong>Mentors</strong>: Cameron Pfiffer, Mohamed Tarek, David Widmann</p> <p><strong>Project difficulty</strong>: Easy</p> <p><strong>Project length</strong>: 175 hrs</p> <p><strong>Description</strong>: Turing.jl is based on a set of inference packages that maintained by the TuringLang group. This project is about making use of improvements in DynamicPPL to create a generic integration between Turing.jl and the AbstractMCMC.jl sampling API. The ultimate goal is to remove or substantially reduce algorithm-specific glue code inside Turing.jl. The project would also involve improving data structures for storing model parameters in DynamicPPL.</p> <h2 id=directed-graphical_model_support_for_the_abstract_probabilistic_programming_library ><a href="#directed-graphical_model_support_for_the_abstract_probabilistic_programming_library">Directed-graphical model support for the abstract probabilistic programming library</a></h2> <p><strong>Mentors</strong>: Philipp Gabler, Hong Ge</p> <p><strong>Project difficulty</strong>: Hard</p> <p><strong>Project length</strong>: 350 hrs</p> <p><strong>Description</strong>: We want to have a very light-weight representation of probabilistic models of static graphs &#40;similar to BUGS&#41;, which can serve as a representation target of other front-end DSLs or be dynamically built. The representation should consist of the model and node representations &#40;stochastic and deterministic, perhaps hyperparameters&#41; and conform to the AbstractPPL model interface, with basic functions &#40;evaluation of density, sampling, conditioning; at later stages some static analysis like extraction of Markov blankets&#41;. The model should also contain the state of the variables and implement the AbstractPPL trace interface &#40;dictionary functions, querying of variable names&#41;. The result should be able to work with existing sampling packages through the abstract interfaces.</p> <h2 id=a_modular_tape_caching_mechanism_for_reversediff ><a href="#a_modular_tape_caching_mechanism_for_reversediff">A modular tape caching mechanism for ReverseDiff</a></h2> <p><strong>Mentors</strong>: Qingliang Zhuo, Mohamed Tarek</p> <p><strong>Project difficulty</strong>: Medium</p> <p><strong>Project length</strong>: 175 hrs</p> <p><strong>Description</strong>: Tape caching often leads to significant performance improvements for gradient-based sampling algorithms &#40;e.g. HMC/NUTS&#41;. Tape caching is only possible at the complete computational level for ReverseDiff at the moment. This project is about implementing a more modular, i.e. function-as-a-caching-barrier, tape caching mechanism for ReverseDiff.jl.</p> <h2 id=benchmarking_improving_performance_of_the_juliagaussianprocesses_libraries ><a href="#benchmarking_improving_performance_of_the_juliagaussianprocesses_libraries">Benchmarking &amp; improving performance of the JuliaGaussianProcesses libraries</a></h2> <p><strong>Mentors</strong>: Theo Galy-Fajou, Will Tebbutt, ST John</p> <p><strong>Project difficulty</strong>: Medium</p> <p><strong>Project length</strong>: 350 hrs</p> <p><strong>Description</strong>: Although KernelFunctions.jl has extensive correctness testing, our performance testing is lacking. This project aims to resolve this, and resolve performance issues wherever they are found. The student would first need to extend our existing benchmarking coverage, and debug any obvious performance problems. The next phase of the work would be to construct end-to-end examples of KernelFunctions being used in practice, profile them to determine where performance problems lie, and fix them.</p> <h2 id=iterative_methods_for_inference_in_gaussian_processes ><a href="#iterative_methods_for_inference_in_gaussian_processes">Iterative methods for inference in Gaussian Processes</a></h2> <p><strong>Mentors</strong>: Will Tebbutt, S. T. John, Ross Viljoen</p> <p><strong>Project difficulty</strong>: Medium</p> <p><strong>Project length</strong>: 175 hrs</p> <p><strong>Description</strong>: There has recently been quite a bit of work on inference methods for GPs that use iterative methods rather than the Cholesky factorisation. They look quite promising, but no one has implemented any of these within the Julia GP ecosystem yet, but they should fit nicely within the AbstractGPs framework. If you&#39;re interested in improving the GP ecosystem in Julia, this project might be for you&#33;</p> <h2 id=approximate_inference_methods_for_non-gaussian_likelihoods_in_gaussian_processes ><a href="#approximate_inference_methods_for_non-gaussian_likelihoods_in_gaussian_processes">Approximate inference methods for non-Gaussian likelihoods in Gaussian Processes</a></h2> <p><strong>Mentors</strong>: S. T. John, Ross Viljoen, Theo Galy-Fajou</p> <p><strong>Project difficulty</strong>: Hard</p> <p><strong>Project length</strong>: 350 hrs</p> <p><strong>Description</strong>: Adding <a href="https://github.com/JuliaGaussianProcesses/JuliaGaussianProcesses.github.io/discussions/5#discussioncomment-1627101">approximate inference</a> methods for non-Gaussian likelihoods which are available in other GP packages but not yet within JuliaGPs. The project would start by determining which approximate inference method&#40;s&#41; to implement–-there&#39;s lots to do, and we&#39;re happy to work with a student on whichever method they are most interested in, or to suggest one if they have no strong preference.</p> <h2 id=gpu_integration_in_the_juliagps_ecosystem ><a href="#gpu_integration_in_the_juliagps_ecosystem">GPU integration in the JuliaGPs ecosystem</a></h2> <p><strong>Mentors</strong>: Ross Viljoen, Theo Galy-Fajou, Will Tebbutt</p> <p><strong>Project difficulty</strong>: Medium</p> <p><strong>Project length</strong>: 350 hrs</p> <p><strong>Description</strong>: This would involve first ensuring that common models are able to run fully on the GPU, then identifying and improving GPU-specific performance bottlenecks. This would begin by implementing a limited end-to-end example involving a GP with a standard kernel, and profiling it to debug any substantial performance bottlenecks. From there, support for a wider range of the functionality available in KernelFunctions.jl and AbstractGPs.jl can be added. Stretch goal: extension of GPU support to some functionality in ApproximateGPs.jl.</p> <h1 id=vs_code_projects ><a href="#vs_code_projects">VS Code projects</a></h1> <h2 id=vs_code_extension ><a href="#vs_code_extension">VS Code extension</a></h2> <p>We are generally looking for folks that want to help with the <a href="https://www.julia-vscode.org/">Julia VS Code extension</a>. We have a long list of open issues, and some of them amount to significant projects.</p> <p><strong>Required Skills</strong>: TypeScript, julia, web development.</p> <p><strong>Expected Results</strong>: Depends on the specific projects we would agree on.</p> <p><strong>Mentors</strong>: <a href="https://github.com/davidanthoff">David Anthoff</a></p> <h2 id=package_installation_ui ><a href="#package_installation_ui">Package installation UI</a></h2> <p>The VSCode extension for Julia could provide a simple way to browse available packages and view what&#39;s installed on a users system. To start with, this project could simply provide a GUI that reads in package data from a <code>Project.toml</code>/<code>Manifest.toml</code> and show some UI elements to add/remove/manage those packages.</p> <p>This could also be extended by having metadata about the package, such as a readme, github stars, activity and so on &#40;somewhat similar to the VSCode-native extension explorer&#41;.</p> <p><strong>Expected Results</strong>: A UI in VSCode for package operations.</p> <p><strong>Recommended Skills</strong>: Familiarity with TypeScript and Julia development.</p> <p><strong>Mentors</strong>: <a href="https://github.com/pfitzseb">Sebastian Pfitzner</a></p> <p><em>Also take a look at <a href="https://julialang.org/jsoc/gsoc/pluto/">Pluto - VS Code integration</a>&#33;</em></p> <h1 id=web_platform_projects_summer_of_code ><a href="#web_platform_projects_summer_of_code">Web Platform Projects – Summer of Code</a></h1> <p>Julia has early support for targeting WebAssembly and running in the web browser. Please note that this is a rapidly moving area &#40;see the <a href="https://github.com/Keno/julia-wasm">project repository</a> for a more detailed overview&#41;, so if you are interested in this work, please make sure to inform yourself of the current state and talk to us to scope out an appropriate project. The below is intended as a set of possible starting points.</p> <p>Mentor for these projects is <a href="https://github.com/Keno">Keno Fischer</a> unless otherwise stated.</p> <h2 id=code_generation_improvements_and_async_abi ><a href="#code_generation_improvements_and_async_abi">Code generation improvements and async ABI</a></h2> <p>Because Julia relies on an asynchronous task runtime and WebAssembly currently lacks native support for stack management, Julia needs to explicitly manage task stacks in the wasm heap and perform a compiler transformation to use this stack instead of the native WebAssembly stack. The overhead of this transformation directly impacts the performance of Julia on the wasm platform. Additionally, since all code Julia uses &#40;including arbitrary C/C&#43;&#43; libraries&#41; must be compiled using this transformation, it needs to cover a wide variety of inputs and be coordinated with other users having similar needs &#40;e.g. the Pyodide project to run python on the web&#41;. The project would aim to improve the quality, robustness and flexibility of this transformation.</p> <p><strong>Recommended Skills</strong>: Experience with LLVM.</p> <h2 id=wasm_threading ><a href="#wasm_threading">Wasm threading</a></h2> <p>WebAssembly is in the process of standardizing <a href="https://github.com/WebAssembly/threads">threads</a>. Simultaneously, work is ongoing to introduce a new threading runtime in julia &#40;see <a href="https://github.com/JuliaLang/julia/pull/22631">#22631</a> and replated PRs&#41;. This project would investigate enabling threading support for Julia on the WebAssembly platform, implementing runtime parallel primitives on the web assembly platform and ensuring that high level threading constructs are correctly mapped to the underlying platform. Please note that both the WebAssembly and julia threading infrastructure is still in active development and may continue to change over the duration of the project. An informed understanding of the state of these projects is a definite prerequisite for this project.</p> <p><strong>Recommended Skills</strong>: Experience with C and multi-threaded programming.</p> <h2 id=high_performance_low-level_integration_of_js_objects ><a href="#high_performance_low-level_integration_of_js_objects">High performance, Low-level integration of js objects</a></h2> <p>WebAssembly is in the process of adding <a href="https://github.com/WebAssembly/reference-types">first class references to native objects</a> to their specification. This capability should allow very high performance integration between julia and javascript objects. Since it is not possible to store references to javascript objects in regular memory, adding this capability will require several changes to the runtime system and code generation &#40;possibly including at the LLVM level&#41; in order to properly track these references and emit them either as direct references to as indirect references to the reference table.</p> <p><strong>Recommended Skills</strong>: Experience with C.</p> <h2 id=dom_integration ><a href="#dom_integration">DOM Integration</a></h2> <p>While julia now runs on the web platform, it is not yet a language that&#39;s suitable for first-class development of web applications. One of the biggest missing features is integration with and abstraction over more complicated javascript objects and APIs, in particular the DOM. Inspiration may be drawn from similar projects in <a href="https://github.com/koute/stdweb">Rust</a> or other languages.</p> <p><strong>Recommended Skills</strong>: Experience with writing libraries in Julia, experience with JavaScript Web APIs.</p> <h2 id=porting_existing_web-integration_packages_to_the_wasm_platform ><a href="#porting_existing_web-integration_packages_to_the_wasm_platform">Porting existing web-integration packages to the wasm platform</a></h2> <p>Several Julia libraries &#40;e.g. WebIO.jl, Escher.jl&#41; provide input and output capabilities for the web platform. Porting these libraries to run directly on the wasm platform would enable a number of existing UIs to automatically work on the web.</p> <p><strong>Recommended Skills</strong>: Experience with writing libraries in Julia.</p> <h2 id=native_dependencies_for_the_web ><a href="#native_dependencies_for_the_web">Native dependencies for the web</a></h2> <p>The Julia project uses <a href="https://github.com/JuliaPackaging/BinaryBuilder.jl">BinaryBuilder</a> to provide binaries of native dependencies of julia packages. Experimental support exists to extend this support to the wasm platform, but few packages have been ported. This project would consist of attempting to port a significant fraction of the binary dependencies of the julia ecosystem to the web platform by improving the toolchain support in BinaryBuilder or &#40;if necessary&#41;, porting upstream packages to fix assumptions not applicable on the wasm platform.</p> <p><strong>Recommended Skills</strong>: Experience with building native libraries in Unix environments.</p> <h2 id=distributed_computing_with_untrusted_parties ><a href="#distributed_computing_with_untrusted_parties">Distributed computing with untrusted parties</a></h2> <p>The Distributed computing abstractions in julia provide convenient abstraction for implementing programs that span many communicating julia processes on different machines. However, the existing abstractions generally assume that all communicating processes are part of the same trust domain &#40;e.g. they allow messages to execute arbitrary code on the remote&#41;. With some of the nodes potentially running in the web browser &#40;or multiple browser nodes being part of the same distributed computing cluster via WebRPC&#41;, this assumption no longer holds true and new interfaces need to be designed to support multiple trust domains without overly restricting usability.</p> <p><strong>Recommended Skills</strong>: Experience with distributed computing and writing libraries in Julia.</p> <h2 id=deployment ><a href="#deployment">Deployment</a></h2> <p>Currently supported use cases for julia on the web platform are primarily geared towards providing interactive environments to support exploration of the full language. Of course, this leads to significantly larger binaries than would be required for using Julia as part of a production deployment. By disabling dynamic language features &#40;e.g. eval&#41; one could generate small binaries suitable for deployment. Some progress towards this exists in packages like <a href="https://github.com/JuliaLang/PackageCompiler.jl">PackageCompiler.jl</a>, though significant work remains to be done.</p> <p><strong>Recommended Skills</strong>: Interest in or experience with Julia internals.</p> </div><br><br> <script src="/libs/highlight/highlight.min.js"></script> <script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: ' '});</script> <footer class="container-fluid footer-copy"> <div class=container > <div class="row footrow"> <ul> <li><a href="/project">About</a> <li><a href="/about/help">Get Help</a> <li><a href="/governance/">Governance</a> <li><a href="/research/#publications">Publications</a> <li><a href="/research/#sponsors">Sponsors</a> </ul> <ul> <li><a href="/downloads/">Downloads</a> <li><a href="/downloads/">All Releases</a> <li><a href="https://github.com/JuliaLang/julia">Source Code</a> <li><a href="/downloads/#current_stable_release">Current Stable Release</a> <li><a href="/downloads/#long_term_support_release">Longterm Support Release</a> <li><a href="https://status.julialang.org/">PkgServer Status</a> </ul> <ul> <li><a href="https://docs.julialang.org/en/v1/">Documentation</a> <li><a href="https://juliaacademy.com">JuliaAcademy</a> <li><a href="https://www.youtube.com/user/JuliaLanguage">YouTube</a> <li><a href="/learning/getting-started/">Getting Started</a> <li><a href="https://docs.julialang.org/en/v1/manual/faq/">FAQ</a> <li><a href="/learning/books">Books</a> </ul> <ul> <li><a href="/community/">Community</a> <li><a href="/community/standards/">Code of Conduct</a> <li><a href="/diversity/">Diversity</a> <li><a href="https://juliacon.org">JuliaCon</a> <li><a href="/community/#julia_user_and_developer_survey">User/Developer Survey</a> <li><a href="/shop/">Shop Merchandise</a> </ul> <ul> <li><a href="https://github.com/JuliaLang/julia/blob/master/CONTRIBUTING.md">Contributing</a> <li><a href="/contribute">Contributor's Guide</a> <li><a href="https://github.com/JuliaLang/julia/issues">Issue Tracker</a> <li><a href="https://github.com/JuliaLang/julia/security/policy">Report a Security Issue</a> <li><a href="https://github.com/search?q=is%3Aopen+is%3Aissue+language%3AJulia+label%3A%22help+wanted%22">Help Wanted Issues</a> <li><a href="https://github.com/search?q=is%3Aopen+is%3Aissue+language%3AJulia+label%3A%22good+first+issue%22">Good First Issue</a> <li><a href="https://docs.julialang.org/en/v1/devdocs/reflection/">Dev Docs</a> </ul> </div> <div id=footer-bottom  class=row > <div class="col-md-10 py-2"> <p>This site is powered by <a href="https://www.netlify.com">Netlify</a>, <a href="https://franklinjl.org">Franklin.jl</a>, and the <a href="https://julialang.org">Julia Programming Language</a>. We thank <a href="https://www.fastly.com">Fastly</a> for their generous infrastructure support.</p> <p>©2021 JuliaLang.org <a href="https://github.com/JuliaLang/www.julialang.org/graphs/contributors">contributors</a>. The content on this website is made available under the <a href="https://github.com/JuliaLang/www.julialang.org/blob/master/LICENSE.md">MIT license</a>. </div> <div class="col-md-2 py-2"> <span class=float-sm-right > <a class=github-button  href="https://github.com/sponsors/julialang" data-icon=octicon-heart  data-size=large  aria-label="Sponsor @julialang on GitHub">Sponsor</a> </span> </div> </div> </div> </footer> <script src="/libs/jquery/jquery.min.js"></script> <script src="/libs/bootstrap/bootstrap.min.js"></script>