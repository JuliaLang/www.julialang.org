<!doctype html>
<html lang="en">
<head>
	<!-- parts for all pages -->
	<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="author" content="Jeff Bezanson, Stefan Karpinski, Viral Shah, Alan Edelman, et al.">
<meta name="description" content="The official website for the Julia Language. Julia is a language that is fast, dynamic, easy to use, and open source. Click here to learn more.">
<meta name="robots" content="max-image-preview:large">
<meta name="twitter:site:id" content="1237720952"> <!-- @JuliaLanguage -->
<meta name="google-site-verification" content="9VDSjBtchQj6PQYIVwugTPY7pVCfLYgvkXiRHjc_Bzw" /> <!-- Google News Feed -->


	<link rel="icon" href="/assets/infra/julia.ico">

  <!-- Franklin stylesheets for generated pages -->
  
  

	<!-- NOTE: specific stylesheets -->
<link rel="stylesheet" href="/libs/bootstrap/bootstrap.min.css">
<link rel="stylesheet" href="/css/app.css">
<link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/fonts.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:400,400i,500,500i,700,700i" rel="stylesheet">
<link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
<script async defer src="/libs/buttons.js"></script>
<script src="/libs/clipboard.min.js"></script>
<script src="/libs/detectdark.js"></script>


<script defer data-domain="julialang.org" src="https://plausible.io/js/script.js"></script>

<!-- scripts for map rendering -->
<link rel="stylesheet" href="https://unpkg.com/leaflet@1.7.1/dist/leaflet.css"
integrity="sha512-xodZBNTC5n17Xt2atTPuE1HxjVMSvLVW9ocqUKLsCC5CXdbqCmblAshOMAS6/keqq/sMZMZ19scR4PsZChSR7A=="
crossorigin=""/>

<script src="https://unpkg.com/leaflet@1.7.1/dist/leaflet.js"
 integrity="sha512-XQoYMqMTK8LvdxXYG3nZ448hOEQiglfqkJs1NOQV44cWnUrBc8PkAOcXy20w0vlaXaVUearIOBhiXZ5V3ynxwA=="
 crossorigin=""></script>

<!-- https://github.com/Leaflet/Leaflet.markercluster -->
<script src="https://cdn.jsdelivr.net/npm/leaflet.markercluster@1.4.1/dist/leaflet.markercluster-src.min.js"></script>

<script src="https://kit.fontawesome.com/f030d443fe.js" crossorigin="anonymous"></script>


   <title>View all GSoC/JSoC Projects</title>   

  

  <!-- Specific style for blog pages (except the /blob/index) -->
  

  <!-- OGP Metadata -->
	<meta property="og:title" content="View all GSoC/JSoC Projects">
<meta property="og:description" content="">
<meta property="og:image" content="/assets/images/julia-open-graph.png">


</head>

<body>

<div class="container py-3 py-lg-0">
  <nav class="navbar navbar-expand-lg navbar-light bg-light" id="main-menu">
    <!-- LOGO -->
    <a class="navbar-brand" href="/">
      <img class="julialogo" src="/assets/infra/logo.svg" alt="JuliaLang Logo">
    </a>

    <button class="navbar-toggler ml-auto hidden-sm-up float-xs-left" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

      <!-- MENU: DOWNLOAD | DOCUMENTATION | BLOG ... -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mx-auto">
        <li class="nav-item   flex-md-fill text-md-center">
          <a class="nav-link" href="/downloads/">Download</a>
        </li>
        <li class="nav-item flex-md-fill text-md-center">
          <a class="nav-link" href="https://docs.julialang.org">Documentation</a>
        </li>
        <li class="nav-item   flex-md-fill text-md-center">
          <a class="nav-link" href="/learning/">Learn</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="/blog/">Blog</a>
        </li>
        <li class="nav-item   flex-md-fill text-md-center">
          <a class="nav-link" href="/community/">Community</a>
        </li>
        <li class="nav-item   flex-md-fill text-md-center">
          <a class="nav-link" href="/contribute/">Contribute</a>
        </li>
        <li class="nav-item  active flex-md-fill text-md-center">
          <a class="nav-link" href="/jsoc/">JSoC</a>
        </li>
      </ul>
      <span class="navbar-right">
        <a class="github-button" href="https://github.com/JuliaLang/julia" data-size="large" data-show-count="true" aria-label="Star JuliaLang/julia on GitHub">Star</a>
        <a class="github-button" href="https://github.com/sponsors/julialang" data-icon="octicon-heart" data-size="large" aria-label="Sponsor @julialang on GitHub">Sponsor</a>
      </span>
    </div>

  </nav>
</div>


<br><br>




<a href="https://github.com/JuliaLang/www.julialang.org/blob/master/jsoc/allprojects.md" title="Edit this page on GitHub" class="edit-float">
</a>


<!-- Content appended here -->
<div class="container main"><h2 id="view_all_gsocjsoc_projects"><a href="#view_all_gsocjsoc_projects" class="header-anchor">View all GSoC/JSoC Projects</a></h2>
<p>This page is designed to improve discoverability of projects. You can, for example, search this page for specific keywords and find all of the relevant projects.</p>
<h2 id="projects"><a href="#projects" class="header-anchor">Projects</a></h2>
<h1 id="mljjl_projects_summer_of_code"><a href="#mljjl_projects_summer_of_code" class="header-anchor">MLJ.jl Projects – Summer of Code</a></h1>
<p><a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a> is a machine learning framework for Julia aiming to provide a convenient way to use and combine a multitude of tools and models available in the Julia ML/Stats ecosystem.</p>
<h3 id="list_of_projects"><a href="#list_of_projects" class="header-anchor">List of projects</a></h3>
<p>MLJ is released under the MIT license and sponsored by the Alan Turing Institute.</p>
<div class="franklin-toc"><ol><li><a href="#view_all_gsocjsoc_projects">View all GSoC/JSoC Projects</a></li><li><a href="#projects">Projects</a><ol><li><a href="#list_of_projects">List of projects</a></li></ol></li><li><a href="#categorical_variable_encoding">Categorical variable encoding</a><ol><li><a href="#description">Description</a></li><li><a href="#prerequisites">Prerequisites</a></li><li><a href="#your_contribution">Your contribution</a></li><li><a href="#references">References</a></li></ol></li><li><a href="#machine_learning_in_predictive_survival_analysis">Machine Learning in Predictive Survival Analysis</a><ol><li><a href="#description__2">Description</a></li><li><a href="#prerequisites__2">Prerequisites</a></li><li><a href="#your_contribution__2">Your contribution</a></li><li><a href="#references__2">References</a></li></ol></li><li><a href="#deeper_bayesian_integration">Deeper Bayesian Integration</a><ol><li><a href="#description__3">Description</a></li><li><a href="#your_contributions">Your contributions</a></li><li><a href="#references__3">References</a></li><li><a href="#difficulty_medium_to_hard">Difficulty: Medium to Hard</a></li></ol></li><li><a href="#tracking_and_sharing_mlj_workflows_using_mlflow">Tracking and sharing MLJ workflows using MLflow</a><ol><li><a href="#description__4">Description</a></li><li><a href="#prerequisites__3">Prerequisites</a></li><li><a href="#your_contribution__3">Your contribution</a></li><li><a href="#references__4">References</a></li></ol></li><li><a href="#speed_demons_only_need_apply">Speed demons only need apply</a><ol><li><a href="#description__5">Description</a></li><li><a href="#prerequisites__4">Prerequisites</a></li><li><a href="#your_contribution__4">Your contribution</a></li><li><a href="#references__5">References</a></li></ol></li><li><a href="#improving_test_coverage_175_hours">Improving test coverage &#40;175 hours&#41;</a></li><li><a href="#multi-threading_improvement_projects_175_hours_each">Multi-threading Improvement Projects &#40;175 hours each&#41;</a></li><li><a href="#automation_of_testing_performance_benchmarking_350_hours">Automation of testing / performance benchmarking &#40;350 hours&#41;</a></li><li><a href="#documenterjl">Documenter.jl</a></li><li><a href="#fluid-structure_interaction_example">Fluid-Structure Interaction Example</a></li><li><a href="#investigation_of_performant_assembly_strategies">Investigation of Performant Assembly Strategies</a><ol><li><a href="#training_on_very_large_graphs">Training on very large graphs  </a></li><li><a href="#adding_graph_convolutional_layers">Adding graph convolutional layers </a></li><li><a href="#adding_models_and_examples">Adding models and examples</a></li><li><a href="#adding_graph_datasets">Adding graph datasets</a></li><li><a href="#implement_layers_for_heterogeneous_graphs">Implement layers for heterogeneous graphs</a></li><li><a href="#improving_performance_using_sparse_linear_algebra">Improving performance using sparse linear algebra </a></li><li><a href="#support_for_amgdpu_and_apple_silicon">Support for AMGDPU and Apple Silicon</a></li><li><a href="#implement_layers_for_temporal_graphs">Implement layers for Temporal Graphs</a></li></ol></li><li><a href="#recommended_skills">Recommended skills</a></li><li><a href="#mentors">Mentors </a></li><li><a href="#qml_and_makie_integration">QML and Makie integration</a><ol><li><a href="#expected_results">Expected results</a></li></ol></li><li><a href="#web_apps_in_makie_and_jsserve">Web apps in Makie and JSServe</a><ol><li><a href="#expected_results__2">Expected results</a></li></ol></li><li><a href="#scheduling_algorithms_for_dagger">Scheduling Algorithms for Dagger</a></li><li><a href="#distributed_training">Distributed Training</a></li><li><a href="#distributed_arrays_over_dagger">Distributed Arrays over Dagger</a></li><li><a href="#benchmarking_against_other_frameworks">Benchmarking against other frameworks</a></li><li><a href="#where_to_go_for_discussion_and_to_find_mentors">Where to go for discussion and to find mentors</a></li><li><a href="#c">C&#43;&#43;</a><ol><li><a href="#cxxwrap_stl">CxxWrap STL</a><ol><li><a href="#expected_outcome">Expected outcome</a></li></ol></li></ol></li><li><a href="#rust">Rust</a><ol><li><a href="#general_goal_of_juliaconstraints">General goal of JuliaConstraints</a></li></ol></li><li><a href="#constraint_programming-based_design_for_kumi_kumi_slope">Constraint Programming-Based Design for Kumi Kumi Slope</a><ol><li><a href="#core_objectives">Core Objectives</a></li></ol></li><li><a href="#agentsjl">Agents.jl</a></li><li><a href="#dynamicalsystemsjl">DynamicalSystems.jl</a></li><li><a href="#large_language_model_projects">Large Language Model Projects</a><ol><li><a href="#project_1_enhancing_llama2jl_with_gpu_support">Project 1: Enhancing llama2.jl with GPU Support</a></li><li><a href="#project_2_llamajl_-_low-level_c_interface">Project 2: Llama.jl - Low-level C interface</a></li><li><a href="#project_3_supercharging_the_knowledge_base_of_aihelpmejl">Project 3: Supercharging the Knowledge Base of AIHelpMe.jl</a></li><li><a href="#project_4_enhancing_julias_ai_ecosystem_with_colbert_v2_for_efficient_document_retrieval">Project 4: Enhancing Julia&#39;s AI Ecosystem with ColBERT v2 for Efficient Document Retrieval</a></li><li><a href="#project_5_enhancing_promptingtoolsjl_with_advanced_schema_support_and_functionality">Project 5: Enhancing PromptingTools.jl with Advanced Schema Support and Functionality</a></li><li><a href="#project_6_expanding_the_julia_large_language_model_leaderboard">Project 6: Expanding the Julia Large Language Model Leaderboard</a></li><li><a href="#project_7_counterfactuals_for_llms_model_explainability_and_generative_ai">Project 7: Counterfactuals for LLMs &#40;<em>Model Explainability</em> and <em>Generative AI</em>&#41;</a></li></ol></li><li><a href="#how_to_contact_us">How to Contact Us</a></li><li><a href="#observational_health_subecosystem_projects">Observational Health Subecosystem Projects</a><ol><li><a href="#project_1_developing_tooling_for_observational_health_research_in_julia">Project 1: Developing Tooling for Observational Health Research in Julia</a></li><li><a href="#project_2_developing_patient_level_prediction_tooling_within_julia">Project 2: Developing Patient Level Prediction Tooling within Julia</a></li></ol></li><li><a href="#medical_imaging_subecosystem_projects">Medical Imaging Subecosystem Projects</a><ol><li><a href="#project_3_adding_functionalities_to_medical_imaging_visualizations">Project 3: Adding functionalities to medical imaging visualizations</a></li><li><a href="#project_4_adding_dataset-wide_functions_and_integrations_of_augmentations">Project 4: Adding dataset-wide functions and integrations of augmentations</a></li><li><a href="#project_5_highly-efficient_mri_simulations_with_multi-vendor_gpu_support">Project 5: Highly-efficient MRI Simulations with Multi-Vendor GPU Support</a></li></ol></li><li><a href="#midification_of_music_from_wave_files">MIDIfication of music from wave files</a></li><li><a href="#efficient_symbolic-numeric_set_computations">Efficient symbolic-numeric set computations</a></li><li><a href="#reachability_with_sparse_polynomial_zonotopes">Reachability with sparse polynomial zonotopes</a></li><li><a href="#improving_the_hybrid_systems_reachability_api">Improving the hybrid systems reachability API</a></li><li><a href="#panel_data_analysis">Panel data analysis</a><ol><li><a href="#description__6">Description</a></li><li><a href="#prerequisites__5">Prerequisites</a></li><li><a href="#your_contribution__5">Your contribution</a></li><li><a href="#references__6">References</a></li></ol></li><li><a href="#distributionsjl_expansion">Distributions.jl Expansion</a><ol><li><a href="#prerequisites__6">Prerequisites</a></li><li><a href="#your_contribution__6">Your contribution</a></li></ol></li><li><a href="#hypothesistestingjl_expansion">HypothesisTesting.jl Expansion</a><ol><li><a href="#prerequisites__7">Prerequisites</a></li><li><a href="#your_contribution__7">Your contribution</a></li><li><a href="#references__7">References</a></li></ol></li><li><a href="#crraojl">CRRao.jl</a><ol><li><a href="#description__7">Description</a></li><li><a href="#prerequisites__8">Prerequisites</a></li><li><a href="#your_contribution__8">Your contribution</a></li></ol></li><li><a href="#juliastats_improvements">JuliaStats Improvements </a><ol><li><a href="#description__8">Description</a></li><li><a href="#prerequisites__9">Prerequisites</a></li><li><a href="#your_contribution__9">Your contribution</a></li></ol></li><li><a href="#surveyjl">Survey.jl</a><ol><li><a href="#prerequisites__10">Prerequisites</a></li><li><a href="#your_contribution__10">Your contribution</a></li><li><a href="#references__8">References</a></li></ol></li><li><a href="#smoothing_non-linear_continuous_time_systems">Smoothing non-linear continuous time systems</a><ol><li><a href="#reinforcement_learning_environments">Reinforcement Learning Environments</a><ol><li><a href="#expected_outcome__2">Expected outcome</a></li></ol></li><li><a href="#alphazerojl">AlphaZero.jl</a><ol><li><a href="#expected_outcomes">Expected Outcomes</a></li></ol></li></ol></li><li><a href="#numerical_linear_algebra">Numerical Linear Algebra</a><ol><li><a href="#matrix_functions">Matrix functions</a></li></ol></li><li><a href="#better_bignums_integration">Better Bignums Integration</a><ol><li><a href="#special_functions">Special functions</a></li><li><a href="#a_julia-native_ccsa_optimization_algorithm">A Julia-native CCSA optimization algorithm</a></li></ol></li><li><a href="#massive_parallel_factorized_bouncy_particle_sampler">Massive parallel factorized bouncy particle sampler</a></li><li><a href="#machine_learning_time_series_regression">Machine Learning Time Series Regression</a></li><li><a href="#machine_learning_for_nowcasting_and_forecasting">Machine learning for nowcasting and forecasting</a></li><li><a href="#time_series_forecasting_at_scales">Time series forecasting at scales</a></li><li><a href="#gpu_accelerated_simulator_of_clifford_circuits">GPU accelerated simulator of Clifford Circuits.</a></li><li><a href="#a_zoo_of_quantum_error_correcting_codes_andor_decoders">A Zoo of Quantum Error Correcting codes and/or decoders</a></li><li><a href="#leftright_multiplications_with_small_gates">Left/Right multiplications with small gates.</a></li><li><a href="#generation_of_fault_tolerant_ecc_circuits_flag_qubit_circuits_and_more">Generation of Fault Tolerant ECC Circuits, Flag Qubit Circuits and more</a></li><li><a href="#measurement-based_quantum_computing_mbqc_compiler">Measurement-Based Quantum Computing &#40;MBQC&#41; compiler</a></li><li><a href="#implementing_a_graph_state_simulator">Implementing a Graph State Simulator</a></li><li><a href="#simulation_of_slightly_non-clifford_circuits_and_states">Simulation of Slightly Non-Clifford Circuits and States</a></li><li><a href="#magic_state_modeling_-_distillation_injection_etc">Magic State Modeling - Distillation, Injection, Etc</a></li><li><a href="#gpu_accelerated_operators_and_ode_solvers">GPU accelerated operators and ODE solvers</a></li><li><a href="#autodifferentiation">Autodifferentiation</a></li><li><a href="#closer_integration_with_the_sciml_ecosystem">Closer Integration with the SciML Ecosystem</a></li><li><a href="#efficient_tensor_differentiation">Efficient Tensor Differentiation</a></li><li><a href="#symbolic_root_finding">Symbolic root finding</a></li><li><a href="#symbolic_integration_in_symbolicsjl">Symbolic Integration in Symbolics.jl</a></li><li><a href="#xla-style_optimization_from_symbolic_tracing">XLA-style optimization from symbolic tracing</a></li><li><a href="#automatically_improving_floating_point_accuracy_herbie">Automatically improving floating point accuracy &#40;Herbie&#41;</a></li><li><a href="#parquetjl_enhancements">Parquet.jl enhancements</a></li><li><a href="#dataframesjl_join_enhancements">DataFrames.jl join enhancements</a></li><li><a href="#project_1_conformal_prediction_meets_bayes_predictive_uncertainty">Project 1: Conformal Prediction meets Bayes &#40;<em>Predictive Uncertainty</em>&#41;</a></li><li><a href="#project_2_counterfactual_regression_model_explainability">Project 2: Counterfactual Regression &#40;<em>Model Explainability</em>&#41;</a></li><li><a href="#project_3_counterfactuals_for_llms_model_explainability_and_generative_ai">Project 3: Counterfactuals for LLMs &#40;<em>Model Explainability</em> and <em>Generative AI</em>&#41;</a></li><li><a href="#project_4_from_counterfactuals_to_interventions_recourse_through_minimal_causal_interventions">Project 4: From Counterfactuals to Interventions &#40;Recourse through Minimal Causal Interventions&#41;</a></li><li><a href="#about_us">About Us</a></li><li><a href="#how_to_contact_us__2">How to Contact Us</a></li><li><a href="#testing_and_benchmarking_of_topoptjl">Testing and benchmarking of TopOpt.jl</a></li><li><a href="#machine_learning_in_topology_optimization">Machine learning in topology optimization</a></li><li><a href="#optimization_on_a_uniform_rectilinear_grid">Optimization on a uniform rectilinear grid</a></li><li><a href="#adaptive_mesh_refinement_for_topology_optimization">Adaptive mesh refinement for topology optimization</a></li><li><a href="#heat_transfer_design_optimization">Heat transfer design optimization</a></li><li><a href="#compiler-based_automatic_differentiation_with_enzymejl">Compiler-based automatic differentiation with Enzyme.jl</a></li><li><a href="#advanced_visualization_and_in-situ_visualization_with_paraview">Advanced visualization and in-situ visualization with ParaView</a></li><li><a href="#implementing_models_from_posteriordb_in_turing_julia">Implementing models from PosteriorDB in Turing / Julia</a></li><li><a href="#improving_the_integration_between_turing_and_turings_mcmc_inference_packages">Improving the integration between Turing and Turing’s MCMC inference packages</a></li><li><a href="#gpu_support_for_normalizingflowsjl_and_bijectorsjl">GPU support for NormalizingFlows.jl and Bijectors.jl</a></li><li><a href="#batched_support_for_normalizingflowsjl_and_bijectorsjl">Batched support for NormalizingFlows.jl and Bijectors.jl</a></li><li><a href="#targets_for_benchmarking_samplers_with_vectorization_gpu_and_high-order_derivative_supports">Targets for Benchmarking Samplers with vectorization, GPU and high-order derivative supports</a></li><li><a href="#vs_code_extension">VS Code extension</a></li><li><a href="#package_installation_ui">Package installation UI</a></li><li><a href="#code_generation_improvements_and_async_abi">Code generation improvements and async ABI</a></li><li><a href="#wasm_threading">Wasm threading</a></li><li><a href="#high_performance_low-level_integration_of_js_objects">High performance, Low-level integration of js objects</a></li><li><a href="#dom_integration">DOM Integration</a></li><li><a href="#porting_existing_web-integration_packages_to_the_wasm_platform">Porting existing web-integration packages to the wasm platform</a></li><li><a href="#native_dependencies_for_the_web">Native dependencies for the web</a></li><li><a href="#distributed_computing_with_untrusted_parties">Distributed computing with untrusted parties</a></li><li><a href="#deployment">Deployment</a></li></ol></div>
<h2 id="categorical_variable_encoding"><a href="#categorical_variable_encoding" class="header-anchor">Categorical variable encoding</a></h2>
<p>Extend the categorical variable encoding of MLJ.</p>
<p><strong>Difficulty.</strong> Moderate. <strong>Duration.</strong> 350 hours</p>
<h3 id="description"><a href="#description" class="header-anchor">Description</a></h3>
<p>MLJ provides basic one-hot encoding of categorical variables but no sophisticated encoding techniques. One-hot encoding is rather limited, in particular when a categorical has a very large number of classes. Many other techniques exists, and this project aims to make some of these available to the MLJ user.</p>
<p><strong>Mentors.</strong> <a href="https://ablaom.github.io/">Anthony Blaom</a> &#40;best contact: direct message on Julia slack&#41;</p>
<h3 id="prerequisites"><a href="#prerequisites" class="header-anchor">Prerequisites</a></h3>
<ul>
<li><p>Julia language fluency is essential.</p>
</li>
<li><p>Git-workflow familiarity is strongly preferred.</p>
</li>
<li><p>Experience with machine learning and data science workflows.</p>
</li>
<li><p>Familiarity with MLJ&#39;s API a plus.</p>
</li>
</ul>
<h3 id="your_contribution"><a href="#your_contribution" class="header-anchor">Your contribution</a></h3>
<p>In this project you will survey popular existing methods for one-hot encoding categorical variables. In collaboration with the mentor, you will make a plan for integrating some of these techniques into MLJ. You will begin work on the plan, initially focusing on simple methods, providing MLJ interfaces to existing julia packages, or new implementations where needed. If the project advances well, you will implement more advanced techniques, such as <a href="https://arxiv.org/abs/1604.06737">entity embedding</a> via MLJFlux.jl &#40;MLJ&#39;s neural network interface&#41;.</p>
<h3 id="references"><a href="#references" class="header-anchor">References</a></h3>
<ul>
<li><p>Existing encoding in MLJ: <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/models/OneHotEncoder_MLJModels/#OneHotEncoder_MLJModels">OneHotEncoder</a>; <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/models/ContinuousEncoder_MLJModels/#ContinuousEncoder_MLJModels">ContinuousEncoder</a>; <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/models/UnivariateTimeTypeToContinuous_MLJModels/#UnivariateTimeTypeToContinuous_MLJModels">UnivariateContinuousTimeEncoder</a></p>
</li>
<li><p>StatsModels.jl <a href="https://juliastats.org/StatsModels.jl/stable/contrasts/">encoders</a></p>
</li>
<li><p>MLJ <a href="https://github.com/JuliaAI/MLJModels.jl/issues/534">feature request</a></p>
</li>
<li><p>Guo and Berkhahn &#91;&#40;2016&#93;&#93;&#40;https://arxiv.org/abs/1604.06737&#41; &quot;Entity Embeddings of Categorical Variables&quot;</p>
</li>
<li><p><a href="https://github.com/FluxML/MLJFlux.jl">MLJFlux.jl</a></p>
</li>
</ul>
<h2 id="machine_learning_in_predictive_survival_analysis"><a href="#machine_learning_in_predictive_survival_analysis" class="header-anchor">Machine Learning in Predictive Survival Analysis</a></h2>
<p>Implement survival analysis models for use in the MLJ machine learning platform.</p>
<p><strong>Difficulty.</strong> Moderate - hard. <strong>Duration.</strong> 350 hours</p>
<h3 id="description__2"><a href="#description__2" class="header-anchor">Description</a></h3>
<p>Survival/time-to-event analysis is an important field of Statistics concerned with understanding the distribution of events over time. Survival analysis presents a unique challenge as we are also interested in events that do not take place, which we refer to as &#39;censoring&#39;. Survival analysis methods are important in many real-world settings, such as health care &#40;disease prognosis&#41;, finance and economics &#40;risk of default&#41;, commercial ventures &#40;customer churn&#41;, engineering &#40;component lifetime&#41;, and many more. This project aims to implement models for performing survivor analysis with the MLJ machine learning framework.</p>
<p><strong><a href="https://mlr3proba.mlr-org.com">mlr3proba</a> is currently the most complete survival analysis interface, let&#39;s get SurvivalAnalysisA.jl to the same standard - but learning from mistakes along the way.</strong></p>
<p><strong>Mentors.</strong> <a href="https://sebastian.vollmer.ms">Sebastian Vollmer</a>, <a href="https://ablaom.github.io/">Anthony Blaom</a>,</p>
<h3 id="prerequisites__2"><a href="#prerequisites__2" class="header-anchor">Prerequisites</a></h3>
<ul>
<li><p>Julia language fluency is essential.</p>
</li>
<li><p>Git-workflow familiarity is strongly preferred.</p>
</li>
<li><p>Some experience with survival analysis.</p>
</li>
<li><p>Familiarity with MLJ&#39;s API a plus.</p>
</li>
<li><p>A passing familiarity with machine learning goals and workflow is</p>
</li>
</ul>
<p>preferred.</p>
<h3 id="your_contribution__2"><a href="#your_contribution__2" class="header-anchor">Your contribution</a></h3>
<p>You will work towards creating a survival analysis package with a range of metrics, capable of making distribution predictions for classical and ML models. You will bake in competing risks in early, as well as prediction transformations, and include both left and interval censoring.  You will code up basic models &#40;Cox PH and AFT&#41;, as well as one ML model as a proof of concept &#40;probably decision tree is simplest or Coxnet&#41;.</p>
<p>Specifically, you will:</p>
<ul>
<li><p>Familiarize yourself with the training and evaluation machine</p>
</li>
</ul>
<p>learning models in MLJ.</p>
<ul>
<li><p>For SurvivalAnalysis.jl, implement the <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/adding_models_for_general_use/">MLJ model interface</a>.</p>
</li>
<li><p>Consider Explainability of SurvivalAnalysis through SurvSHAP&#40;t&#41;</p>
</li>
<li><p>Develop a proof of concept for newer advanced survival analysis</p>
</li>
</ul>
<p>models not currently implemented in Julia.</p>
<h3 id="references__2"><a href="#references__2" class="header-anchor">References</a></h3>
<ul>
<li><p>Mateusz Krzyziński et al., <a href="https://doi.org/10.1016/j.knosys.2022.110234">SurvSHAP&#40;t&#41;: Time-Dependent Explanations of Machine Learning Survival Models</a>, Knowledge-Based Systems 262 &#40;February 2023&#41;: 110234</p>
</li>
<li><p>Kvamme, H., Borgan, Ø., &amp; Scheel, I. &#40;2019&#41;. <a href="https://arxiv.org/abs/1907.00825">Time-to-event prediction with neural networks and Cox regression</a>. Journal of Machine Learning Research, 20&#40;129&#41;, 1–30.</p>
</li>
<li><p>Lee, C., Zame, W. R., Yoon, J., &amp; van der Schaar, M. &#40;2018&#41;. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/11842/11701">Deephit: A deep learning approach to survival analysis with	competing risks.</a> In Thirty-Second AAAI Conference on Artificial	Intelligence.</p>
</li>
<li><p>Katzman, J. L., Shaham, U., Cloninger, A., Bates, J., Jiang, T., &amp; Kluger, Y. &#40;2018&#41;. <a href="https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-018-0482-1">DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network</a>. BMC Medical Research Methodology, 18&#40;1&#41;, 24.</p>
</li>
<li><p>Gensheimer, M. F., &amp; Narasimhan, B. &#40;2019&#41;. A scalable discrete-time survival model for neural networks.&#93;&#40;https://peerj.com/articles/6257/&#41; PeerJ, 7, e6257.</p>
</li>
<li><p><a href="https://github.com/RaphaelS1/SurvivalAnalysis.jl">SurvivalAnalysis.jl</a></p>
</li>
</ul>
<h2 id="deeper_bayesian_integration"><a href="#deeper_bayesian_integration" class="header-anchor">Deeper Bayesian Integration</a></h2>
<p>Bayesian methods and probabilistic supervised learning provide uncertainty quantification. This project aims increasing integration to combine Bayesian and non-Bayesian methods using Turing.</p>
<p><strong>Difficulty.</strong> Difficult. <strong>Duration.</strong> 350 hours.</p>
<h3 id="description__3"><a href="#description__3" class="header-anchor">Description</a></h3>
<p>As an initial step reproduce <a href="https://github.com/cscherrer/SossMLJ.jl">SOSSMLJ</a> in Turing. The bulk of the project is to implement methods that combine multiple predictive distributions.</p>
<h3 id="your_contributions"><a href="#your_contributions" class="header-anchor">Your contributions</a></h3>
<ul>
<li><p>Interface between Turing and MLJ</p>
</li>
<li><p>Comparisons of ensembling, stacking of predictive distribution</p>
</li>
<li><p>reproducible benchmarks across various settings.</p>
</li>
</ul>
<h3 id="references__3"><a href="#references__3" class="header-anchor">References</a></h3>
<p><a href="http://www.stat.columbia.edu/~gelman/research/published/stacking_paper_discussion_rejoinder.pdf">Bayesian Stacking</a> <a href="https://github.com/alan-turing-institute/skpro/blob/master/README.md">SKpro</a></p>
<h3 id="difficulty_medium_to_hard"><a href="#difficulty_medium_to_hard" class="header-anchor">Difficulty: Medium to Hard</a></h3>
<p><strong>Mentors</strong>: <a href="https://github.com/yebai">Hong Ge</a> <a href="https://sebastian.vollmer.ms">Sebastian Vollmer</a></p>
<h2 id="tracking_and_sharing_mlj_workflows_using_mlflow"><a href="#tracking_and_sharing_mlj_workflows_using_mlflow" class="header-anchor">Tracking and sharing MLJ workflows using MLflow</a></h2>
<p>Help data scientists using MLJ track and share their machine learning experiments using <a href="https://mlflow.org">MLflow</a>. The emphasis iin this phase of the project is:</p>
<ul>
<li><p>support <em>asynchronous</em> workflows, as appear in parallelized model tuning</p>
</li>
<li><p>support live logging while training <em>iterative</em> models, such as neural networks</p>
</li>
</ul>
<p><strong>Difficulty.</strong> Moderate. <strong>Duration.</strong> 350 hours.</p>
<h3 id="description__4"><a href="#description__4" class="header-anchor">Description</a></h3>
<p>MLflow is an open source platform for the machine learning life cycle. It allows the data scientist to upload experiment metadata and outputs to the platform for reproducing and sharing purposes. MLJ <a href="https://github.com/JuliaAI/MLJFlow.jl">already allows</a> users to report basic model performance evaluation to an MLflow service and this project seeks to greatly enhance this integration.</p>
<h3 id="prerequisites__3"><a href="#prerequisites__3" class="header-anchor">Prerequisites</a></h3>
<ul>
<li><p>Julia language fluency essential</p>
</li>
<li><p>Understanding of asynchronous programming principles</p>
</li>
<li><p>Git-workflow familiarity strongly preferred.</p>
</li>
<li><p>General familiarity with data science workflows</p>
</li>
</ul>
<h3 id="your_contribution__3"><a href="#your_contribution__3" class="header-anchor">Your contribution</a></h3>
<ul>
<li><p>You will familiarize yourself with MLJ, MLflow and MLflowClient.jl client APIs.</p>
</li>
<li><p>You will familiarize yourself with the MLJFlow.jl package providing MLJ &lt;–&gt; MLflow integration</p>
</li>
<li><p>Implement changes needed to allow correct <em>asynchronous</em> logging of model performance evaluations</p>
</li>
<li><p>Extend logging to &#40;parallelized&#41; model tuning &#40;MLJ&#39;s <code>TunedModel</code> wrapper&#41;</p>
</li>
<li><p>Extend logging to controlled training of iterative models &#40;MLJ&#39;s <code>IteratedModel</code> wrapper&#41;</p>
</li>
</ul>
<h3 id="references__4"><a href="#references__4" class="header-anchor">References</a></h3>
<ul>
<li><p><a href="https://mlflow.org">MLflow</a> website.</p>
</li>
<li><p><a href="https://mlflow.org/docs/latest/rest-api.html">MLflow REST API</a>.</p>
</li>
<li><p><a href="https://github.com/JuliaAI/MLJFlow.jl">MLJFlow.jl</a></p>
</li>
<li><p><a href="https://github.com/JuliaAI/MLFlowClient.jl">MLflowClient.jl</a></p>
</li>
<li><p><a href="https://github.com/JuliaAI/MLJIteration.jl">MLJIteration.jl</a></p>
</li>
<li><p><a href="https://github.com/JuliaAI/MLJFlow.jl/issues/26">Issue on asynchronous reporting</a></p>
</li>
</ul>
<p><strong>Mentors.</strong> <a href="https://ablaom.github.io/">Anthony Blaom</a></p>
<h2 id="speed_demons_only_need_apply"><a href="#speed_demons_only_need_apply" class="header-anchor">Speed demons only need apply</a></h2>
<p>Diagnose and exploit opportunities for speeding up common MLJ workflows.</p>
<p><strong>Difficulty.</strong> Moderate.  <strong>Duration.</strong> 350 hours.</p>
<h3 id="description__5"><a href="#description__5" class="header-anchor">Description</a></h3>
<p>In addition to investigating a number of known performance bottlenecks, you will have some free reign in this to identify opportunities to speed up common MLJ workflows, as well as making better use of memory resources.</p>
<h3 id="prerequisites__4"><a href="#prerequisites__4" class="header-anchor">Prerequisites</a></h3>
<ul>
<li><p>Julia language fluency essential.</p>
</li>
<li><p>Experience with multi-threading and multi-processor computing essential, preferably in Julia.</p>
</li>
<li><p>Git-workflow familiarity strongly preferred.</p>
</li>
<li><p>Familiarity with machine learning goals and  workflow preferred</p>
</li>
</ul>
<h3 id="your_contribution__4"><a href="#your_contribution__4" class="header-anchor">Your contribution</a></h3>
<p>In this project you will:</p>
<ul>
<li><p>familiarize yourself with the training, evaluation and tuning of machine learning models in MLJ</p>
</li>
<li><p>benchmark and profile common workflows to identify opportunities for further code optimizations, with a focus on the most popular models</p>
</li>
<li><p>work to address problems identified</p>
</li>
<li><p>roll out new data front-end for iterative models, to avoid unnecessary copying of data</p>
</li>
<li><p>experiment with adding multi-processor parallelism to the current learning networks scheduler</p>
</li>
<li><p>implement some of these optimizations</p>
</li>
</ul>
<h3 id="references__5"><a href="#references__5" class="header-anchor">References</a></h3>
<ul>
<li><p><a href="https://github.com/alan-turing-institute/MLJ.jl/blob/dev/ROADMAP.md#scalability">MLJ Roadmap</a>. See, in particular &quot;Scalability&quot; section.</p>
</li>
<li><p><a href="https://github.com/alan-turing-institute/MLJBase.jl/issues/309">Taking performance more seriously GitHub issue</a></p>
</li>
<li><p><a href="https://alan-turing-institute.github.io/MLJ.jl/dev/adding_models_for_general_use/#Implementing-a-data-front-end-1">Data front end</a> for MLJ models.</p>
</li>
</ul>
<p><strong>Mentors.</strong> <a href="https://ablaom.github.io">Anthony Blaom</a>, Okon Samuel.</p>
<h1 id="bayesianoptimization"><a href="#bayesianoptimization" class="header-anchor">BayesianOptimization</a></h1>
<p>Bayesian optimization is a global optimization strategy for &#40;potentially noisy&#41; functions with unknown derivatives. With well-chosen priors, it can find optima with fewer function evaluations than alternatives, making it well suited for the optimization of costly objective functions.</p>
<p>Well known examples include hyper-parameter tuning of machine learning models &#40;see e.g. <a href="https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf">Taking the Human Out of the Loop: A Review of Bayesian Optimization</a>&#41;. The Julia package <a href="https://github.com/jbrea/BayesianOptimization.jl">BayesianOptimization.jl</a> currently supports only basic Bayesian optimization methods. There are multiple directions to improve the package, including &#40;but not limited to&#41;</p>
<ul>
<li><p><strong>Hybrid Bayesian Optimization &#40;duration: 175h, expected difficulty: medium&#41;</strong> with discrete and continuous variables. Implement e.g. <a href="https://arxiv.org/abs/2106.04682v1">HyBO</a> see also <a href="https://github.com/jbrea/BayesianOptimization.jl/issues/26">here</a>.</p>
</li>
<li><p><strong>Scalable Bayesian Optimization &#40;duration: 175h, expected difficulty: medium&#41;</strong>: implement e.g. <a href="https://proceedings.neurips.cc/paper/2019/hash/6c990b7aca7bc7058f5e98ea909e924b-Abstract.html">TuRBO</a> or <a href="http://proceedings.mlr.press/v130/eriksson21a.html">SCBO</a>.</p>
</li>
<li><p><strong>Better Defaults &#40;duration: 175h, expected difficulty: easy&#41;</strong>: write an extensive test suite and implement better defaults; draw inspiration from e.g. <a href="https://github.com/dragonfly/dragonfly">dragonfly</a>.</p>
</li>
</ul>
<p><strong>Recommended Skills:</strong> Familiarity with Bayesian inference, non-linear optimization, writing Julia code and reading Python code.</p>
<p><strong>Expected Outcome:</strong> Well-tested and well-documented new features.</p>
<p><strong>Mentor:</strong> <a href="https://github.com/jbrea">Johanni Brea</a></p>
<h1 id="compiler_projects_summer_of_code"><a href="#compiler_projects_summer_of_code" class="header-anchor">Compiler Projects – Summer of Code</a></h1>
<p>There are a number of compiler projects that are currently being worked on. Please contact Jameson Nash for additional details and let us know what specifically interests you about this area of contribution. That way, we can tailor your project to better suit your interests and skillset.</p>
<ul>
<li><p><strong>LLVM AliasAnalysis &#40;175-350 hours&#41;</strong> The Julia language utilizes LLVM as a backend for code generation, so the quality of code generation is very important for performance. This means that there are plenty of opportunities for those with knowledge of or interest in LLVM to contribute via working on Julia&#39;s code generation process. We have recently encountered issues with memcpy information only accepting a single aliasing metadata argument, rather than separate information for the source and destination. There are other similar missing descriptive or optimization steps in the aliasing information we produce or consume by LLVM&#39;s passes.</p>
<p><strong>Expected Outcomes</strong>: Improve upon the alias information &quot;LLVM level&quot; of Julia codegen.<br /><strong>Skills</strong>: C/C&#43;&#43; programming<br /><strong>Difficulty</strong>: Hard</p>
</li>
<li><p><strong>Macro hygiene re-implementation, to eliminate incorrect predictions inherent in current approach &#40;350 hours&#41;</strong></p>
<p>This may be a good project for someone that wants to learn lisp/scheme&#33; Our current algorithm runs in multiple passes, which means sometimes we compute the wrong scope for a variable in the earlier pass than when we assign the actual scope to each value. See <a href="https://github.com/JuliaLang/julia/labels/macros">https://github.com/JuliaLang/julia/labels/macros</a>, and particularly issues such as <a href="https://github.com/JuliaLang/julia/issues/20241">https://github.com/JuliaLang/julia/issues/20241</a>, <a href="https://github.com/JuliaLang/julia/issues/53667">https://github.com/JuliaLang/julia/issues/53667</a>, <a href="https://github.com/JuliaLang/julia/issues/53673">https://github.com/JuliaLang/julia/issues/53673</a> and <a href="https://github.com/JuliaLang/julia/issues/34164">https://github.com/JuliaLang/julia/issues/34164</a>.</p>
<p><strong>Expected Outcomes</strong>: Ideally, re-implementation of hygienic macros. Realistically, resolving some or any of the <code>macros</code> issues.<br /><strong>Skills</strong>: Lisp/Scheme/Racket experience desired but not necessarily required.<br /><strong>Difficulty</strong>: Medium</p>
</li>
<li><p><strong>Better debug information output for variables &#40;175 hours&#41;</strong></p>
<p>We have part of the infrastructure in place for representing DWARF information for our variables, but only from limited places. We could do much better since there are numerous opportunities for improvement&#33;</p>
</li>
</ul>
<p><strong>Expected Outcomes</strong>: Ability to see more variable, argument, and object details in gdb <strong>Recommended Skills</strong>: Most of these projects involve algorithms work, requiring a willingness and interest in seeing how to integrate with a large system.<br /><strong>Difficulty</strong>: Medium<br /><strong>Mentors</strong>: <a href="https://github.com/vtjnash">Jameson Nash</a>, <a href="https://github.com/gbaraldi">Gabriel Baraldi </a></p>
<h2 id="improving_test_coverage_175_hours"><a href="#improving_test_coverage_175_hours" class="header-anchor">Improving test coverage &#40;175 hours&#41;</a></h2>
<p>Code coverage reports very good coverage of all of the Julia Stdlib packages, but it&#39;s not complete. Additionally, the coverage tools themselves &#40;–track-coverage and <a href="https://github.com/JuliaCI/Coverage.jl">https://github.com/JuliaCI/Coverage.jl</a>&#41; could be further enhanced, such as to give better accuracy of statement coverage, or more precision. A successful project may combine a bit of both building code and finding faults in others&#39; code.</p>
<p>Another related side-project might be to explore adding Type information to the coverage reports?</p>
<p><strong>Recommended Skills</strong>: An eye for detail, a thrill for filing code issues, and the skill of breaking things.<br /><strong>Contact:</strong> <a href="https://github.com/vtjnash">Jameson Nash</a></p>
<h2 id="multi-threading_improvement_projects_175_hours_each"><a href="#multi-threading_improvement_projects_175_hours_each" class="header-anchor">Multi-threading Improvement Projects &#40;175 hours each&#41;</a></h2>
<p>Continuous on-going work is being done to improve the correctness and threaded code. A few ideas to get you started on how to join this effort, in brief, include:</p>
<ul>
<li><p>Measure and optimize the performance of the scheduler <code>partr</code> algorithm, and add the ability to dynamically scale it by workload size. Or replace it with a <code>workstealing</code> implementation in Julia.</p>
</li>
<li><p>Automatic insertion, and subsequent optimization, of GC safe-points/regions, particularly around loops. Similarly for <code>ccall</code>, implement the ability to define a particular <code>ccall</code> as being a safe-region.</p>
</li>
<li><p>Solve various thread-safety and data-race bugs in the runtime. &#40;e.g. <a href="https://github.com/JuliaLang/julia/issues/49778">https://github.com/JuliaLang/julia/issues/49778</a> and <a href="https://github.com/JuliaLang/julia/pull/42810">https://github.com/JuliaLang/julia/pull/42810</a>&#41;</p>
</li>
</ul>
<p>Join the regularly scheduled multithreading call for discussion of any of these at <a href="https://calendar.google.com/event?action&#61;TEMPLATE&amp;tmeid&#61;MzQ1MnZxMGNucGt2NGQwYW1zZjA4MzM5dGtfMjAyMTAyMTdUMTYzMDAwWiBqdWxpYWxhbmcub3JnX2tvbWF1YXFldDE0ZW9nOW9pdjNwNm83cG1nQGc&amp;tmsrc&#61;julialang.org_komauaqet14eog9oiv3p6o7pmg&#37;40group.calendar.google.com&amp;scp&#61;ALL">#multithreading BoF calendar invite</a> on the Julia Language Public Events calendar.</p>
<p> <strong>Recommended Skills</strong>: Varies by project, but generally some multi-threading and C experience is needed<br /><strong>Contact:</strong> <a href="https://github.com/vtjnash">Jameson Nash</a></p>
<h2 id="automation_of_testing_performance_benchmarking_350_hours"><a href="#automation_of_testing_performance_benchmarking_350_hours" class="header-anchor">Automation of testing / performance benchmarking &#40;350 hours&#41;</a></h2>
<p>The Nanosoldier.jl project &#40;and related <a href="https://github.com/JuliaCI/BaseBenchmarks.jl">https://github.com/JuliaCI/BaseBenchmarks.jl</a>&#41; tests for performance impacts of some changes. However, there remains many areas that are not covered &#40;such as compile time&#41; while other areas are over-covered &#40;greatly increasing the duration of the test for no benefit&#41; and some tests may not be configured appropriately for statistical power. Furthermore, the current reports are very primitive and can only do a basic pair-wise comparison, while graphs and other interactive tooling would be more valuable. Thus, there would be many great projects for a summer contributor to tackle here&#33;</p>
<p><strong>Expected Outcomes</strong>: Improvement of Julia&#39;s automated testing/benchmarking framework. <strong>Skills</strong>: Interest in and/or experience with CI systems. <strong>Difficulty</strong>: Medium</p>
<p><strong>Contact:</strong> <a href="https://github.com/vtjnash">Jameson Nash</a>, <a href="https://github.com/maleadt">Tim Besard</a></p>
<h1 id="tensor_network_contraction_order_optimization_and_visualization"><a href="#tensor_network_contraction_order_optimization_and_visualization" class="header-anchor">Tensor network contraction order optimization and visualization</a></h1>
<p><a href="https://github.com/under-Peter/OMEinsum.jl">OMEinsum.jl</a> is a pure Julia package for tensor network computation,  which has been used in various projects, including</p>
<ul>
<li><p><a href="https://github.com/QuEraComputing/GenericTensorNetworks.jl">GenericTensorNetworks.jl</a> for solving combinatorial optimization problems,</p>
</li>
<li><p><a href="https://github.com/QuantumBFS/YaoToEinsum.jl">YaoToEinsum.jl</a> for simulating large scale quantum circuit and</p>
</li>
<li><p><a href="https://github.com/TensorBFS/TensorInference.jl">TensorInference.jl</a> for Bayesian inference.</p>
</li>
</ul>
<p>Unlike other tensor contraction packages such as <code>ITensors.jl</code> and <code>TensorOperations.jl</code>, it is designed for large scale tensor networks with arbitrary topology. The key feature of <code>OMEinsum.jl</code> is that it can automatically optimize the contraction order of a tensor network. Related features are implemented in <a href="https://github.com/TensorBFS/OMEinsumContractionOrders.jl">OMEinsumContractionOrders.jl</a>.</p>
<p>We are looking for a student to work on the following tasks:</p>
<ul>
<li><p>Implement a better contraction order optimizer based on <a href="https://arxiv.org/abs/2202.07793">Tamaki&#39;s algorithm</a>.</p>
</li>
<li><p>Implement a hyper-graph visualization tool based on <a href="https://arxiv.org/abs/2308.05043">arXiv:2308.05043</a></p>
</li>
<li><p>Port the contraction order optimizers to <a href="https://github.com/Jutho/TensorOperations.jl">TensorOperations.jl</a></p>
</li>
</ul>
<p><strong>Recommended skills:</strong> familiarity with tensor networks, graph theory and high performance computing.</p>
<p><strong>Expected results:</strong></p>
<ul>
<li><p>new features added to the package <code>OMEinsumContractionOrders.jl</code> along with tests and relevant documentation.</p>
</li>
<li><p>a new package about hyper-graph visualization, and relevant feature added to <code>OMEinsum.jl</code>.</p>
</li>
<li><p>a pull request to <code>TensorOperations.jl</code> for better contraction order optimization.</p>
</li>
</ul>
<p><strong>Mentors:</strong> <a href="https://github.com/giggleliu">Jin-Guo Liu</a>, <a href="https://github.com/Jutho">Jutho Haegeman</a> and <a href="https://github.com/lkdvos">Lukas Devos</a></p>
<p><strong>Project difficulty:</strong> Medium to Hard</p>
<p><strong>Project length:</strong> 350 hrs</p>
<p><strong>Contact:</strong> feel free to ask questions via <a href="cacate0129@gmail.com">email</a> or the Julia slack &#40;user name: JinGuo Liu&#41;.</p>
<h1 id="documentation_tooling"><a href="#documentation_tooling" class="header-anchor">Documentation tooling</a></h1>
<h2 id="documenterjl"><a href="#documenterjl" class="header-anchor">Documenter.jl</a></h2>
<p>The Julia manual and the documentation for a large chunk of the ecosystem is generated using <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> – essentially a static site generator that integrates with Julia and its docsystem. There are tons of opportunities for improvements for anyone interested in working on the interface of Julia, documentation and various front-end technologies &#40;web, LaTeX&#41;.</p>
<h1 id="ferritejl_-_finite_element_toolbox_-_summer_of_code"><a href="#ferritejl_-_finite_element_toolbox_-_summer_of_code" class="header-anchor">Ferrite.jl - Finite Element Toolbox - Summer of Code</a></h1>
<p><a href="https://github.com/ferrite-fem/Ferrite.jl">Ferrite.jl</a> is a Julia package providing the basic building blocks to develop finite element simulations of partial differential equations. The package provides extensive examples to start from and is designed as a compromise between simplicity and generality, trying to map finite element concepts 1:1 with the code in a low-level . Ferrite is actively used in teaching finite element to students at several universities across different countries &#40;e.g. Ruhr-University Bochum and Chalmers University of Technology&#41;. Further infrastructure is provided in the form of different mesh parsers and a Julia based visualizer called <a href="https://github.com/Ferrite-FEM/FerriteViz.jl">FerriteViz.jl</a>.</p>
<p>Below we provide a four of potential project ideas in <a href="https://github.com/ferrite-fem/Ferrite.jl">Ferrite.jl</a>. However, interested students should feel free to explore ideas they are interested in. Please contact any of the mentors listed below, or join the <code>#ferrite-fem</code> channel on the Julia slack to discuss. Projects in finite element visualization are also possible with <a href="https://github.com/Ferrite-FEM/FerriteViz.jl">FerriteViz.jl</a>.</p>
<h2 id="fluid-structure_interaction_example"><a href="#fluid-structure_interaction_example" class="header-anchor">Fluid-Structure Interaction Example</a></h2>
<p><strong>Difficulty</strong>: Easy-Medium &#40;depending on your specific background&#41;</p>
<p><strong>Project size</strong>: 150-300 hours</p>
<p><strong>Problem</strong>: <a href="https://github.com/ferrite-fem/Ferrite.jl">Ferrite.jl</a> is designed with the possibility to define partial differential equations on subdomains. This makes it well-suited for interface-coupled multi-physics problems, as for example fluid-structure interaction problems. However, we currently do not have an example showing this capability in our documentation. We also do not provide all necessary utilities for interface-coupled problems.</p>
<p><strong>Minimum goal</strong>: The minimal goal of this project is to create a functional and documented linear fluid-structure interaction example coupling linear elasticity with a stokes flow in a simple setup. The code should come with proper test coverage.</p>
<p><strong>Extended goal</strong>: With this minimally functional example it is possible to extend the project into different directions, e.g. optimized solvers or nonlinear fluid-structure interaction.</p>
<p><strong>Recommended skills</strong>:</p>
<ul>
<li><p>Basic knowledge the finite element method</p>
</li>
<li><p>Basic knowledge about solids or fluids</p>
</li>
<li><p>The ability &#40;or eagerness to learn&#41; to write fast code</p>
</li>
</ul>
<p><strong>Mentors</strong>: <a href="https://github.com/termi-official">Dennis Ogiermann</a> and <a href="https://github.com/fredrikekre/">Fredrik Ekre</a></p>
<h2 id="investigation_of_performant_assembly_strategies"><a href="#investigation_of_performant_assembly_strategies" class="header-anchor">Investigation of Performant Assembly Strategies</a></h2>
<p><strong>Difficulty</strong>: Medium</p>
<p><strong>Project size</strong>: 250-350 hours</p>
<p><strong>Problem</strong>: <a href="https://github.com/ferrite-fem/Ferrite.jl">Ferrite.jl</a> has an outstanding performance in single-threaded finite element simulations due to elaborate elimination of redundant workloads. However, we recently identified that the way the single-threaded assembly works makes parallel assembly memory bound, rendering the implementation for &quot;cheap&quot; assembly loops not scalable on a wide range of systems. This problem will also translate to high-order schemes, where the single-threaded strategy as is prevents certain common optimization strategies &#40;e.g. sum factorization&#41;.</p>
<p><strong>Minimum goal</strong>: As a first step towards better parallel assembly performance it is the investion of different assembly strategies. Local and global matrix-free schemes are a possibility to explore here. The code has to be properly benchmarked and tested to identify different performance problems.</p>
<p><strong>Extended goal</strong>: With this minimally functional example it is possible to extend the project into different directions, e.g. optimized matrix-free solvers or GPU assembly.</p>
<p><strong>Recommended skills</strong>:</p>
<ul>
<li><p>Basic knowledge the finite element method</p>
</li>
<li><p>Basic knowledge about benchmarking</p>
</li>
<li><p>The ability &#40;or eagerness to learn&#41; to write fast code</p>
</li>
</ul>
<p><strong>Mentors</strong>: <a href="https://github.com/koehlerson">Maximilian Köhler</a> and <a href="https://github.com/termi-official">Dennis Ogiermann</a></p>
<h1 id="graph_neural_networks_-_summer_of_code"><a href="#graph_neural_networks_-_summer_of_code" class="header-anchor">Graph Neural Networks - Summer of Code</a></h1>
<p>Graph Neural Networks &#40;GNN&#41; are deep learning models well adapted to data that takes the form of graphs with feature vectors associated to nodes and edges. GNNs are a growing area of research and find many applications in complex networks analysis, relational reasoning, combinatorial optimization, molecule generation, and many other fields. </p>
<p><a href="https://github.com/CarloLucibello/GraphNeuralNetworks.jl">GraphNeuralNetworks.jl</a> is a pure Julia package for GNNs equipped with many features. It implements common graph convolutional layers, with CUDA support and graph batching for fast parallel operations. There are a number of ways by which the package could be improved.</p>
<h3 id="training_on_very_large_graphs"><a href="#training_on_very_large_graphs" class="header-anchor">Training on very large graphs  </a></h3>
<p>Graph containing several millions of nodes are too large for gpu memory. Mini-batch training is performed on subgraphs, as in the GraphSAGE algorithm.</p>
<p><strong>Duration</strong>: 350h.  </p>
<p><strong>Expected difficulty</strong>: hard.  </p>
<p><strong>Expected outcome</strong>: The necessary algorithmic components to scale GNN training to very large graphs.</p>
<h3 id="adding_graph_convolutional_layers"><a href="#adding_graph_convolutional_layers" class="header-anchor">Adding graph convolutional layers </a></h3>
<p>While we implement a good variety of graph convolutional layers, there is still a vast zoology to be implemented yet. Preprocessing tools, pooling operators, and other GNN-related functionalities can be considered as well.</p>
<p><strong>Duration</strong>: 175h.</p>
<p><strong>Expected difficulty</strong>: easy to medium.  </p>
<p><strong>Expected outcome</strong>: Enrich the package with a variety of new layers and operators.</p>
<h3 id="adding_models_and_examples"><a href="#adding_models_and_examples" class="header-anchor">Adding models and examples</a></h3>
<p>As part of the documentation and for bootstrapping new projects, we want to add fully worked out examples and applications of graph neural networks. We can start with entry-level tutorials and progressively introduce the reader to more advanced features. </p>
<p><strong>Duration</strong>: 175h.  </p>
<p><strong>Expected difficulty</strong>: medium.  </p>
<p><strong>Expected outcome</strong>: A few pedagogical and more advanced examples of graph neural networks applications.</p>
<h3 id="adding_graph_datasets"><a href="#adding_graph_datasets" class="header-anchor">Adding graph datasets</a></h3>
<p>Provide Julia friendly wrappers for common graph datasets in <a href="https://github.com/JuliaML/MLDatasets.jl"><code>MLDatasets.jl</code></a>. Create convenient interfaces for the Julia ML and data ecosystem. </p>
<p><strong>Duration</strong>: 175h.  </p>
<p><strong>Expected difficulty</strong>: easy.  </p>
<p><strong>Expected outcome</strong>: A large collection of graph datasets easily available to the Julia ecosystem.</p>
<h3 id="implement_layers_for_heterogeneous_graphs"><a href="#implement_layers_for_heterogeneous_graphs" class="header-anchor">Implement layers for heterogeneous graphs</a></h3>
<p>In some complex networks, the relations expressed by edges can be of different types. We currently support this with the <code>GNNHeteroGraph</code> type but none of the current graph convolutional layers support heterogeneous graphs  as inputs. With this project we will implement a few layers for heterographs.</p>
<p><strong>Duration</strong>: 175h.  </p>
<p><strong>Expected difficulty</strong>: medium.  </p>
<p><strong>Expected outcome</strong>: The implementation of a new graph type for heterogeneous networks and corresponding graph convolutional layers.</p>
<h3 id="improving_performance_using_sparse_linear_algebra"><a href="#improving_performance_using_sparse_linear_algebra" class="header-anchor">Improving performance using sparse linear algebra </a></h3>
<p>Many graph convolutional layers can be expressed as non-materializing algebraic operations involving the adjacency matrix instead of the slower and more memory consuming gather/scatter mechanism. We aim at extending as far as possible and in a gpu-friendly way these <em>fused</em> implementation.</p>
<p><strong>Duration</strong>: 350h.</p>
<p><strong>Expected difficulty</strong>: hard.</p>
<p><strong>Expected outcome</strong>: A noticeable performance increase for many graph convolutional operations.</p>
<h3 id="support_for_amgdpu_and_apple_silicon"><a href="#support_for_amgdpu_and_apple_silicon" class="header-anchor">Support for AMGDPU and Apple Silicon</a></h3>
<p>We currently support scatter/gather operation only on CPU and CUDA hardware. We aim at extending this to AMDGPU and Apple Silicon leveraging KernelAbstractions.jl, AMDGPU.jl and Metal.jl.</p>
<p><strong>Duration</strong>: 175h.</p>
<p><strong>Expected difficulty</strong>: medium.</p>
<p><strong>Expected outcome</strong>: Graph convolution speedup for AMD GPU and Apple hardware, performance roughly on par with CUDA.</p>
<h3 id="implement_layers_for_temporal_graphs"><a href="#implement_layers_for_temporal_graphs" class="header-anchor">Implement layers for Temporal Graphs</a></h3>
<p>A temporal graph is a graph whose topology changes over time. We currently support this with the <a href="https://github.com/CarloLucibello/GraphNeuralNetworks.jl/blob/master/src/GNNGraphs/temporalsnapshotsgnngraph.jl">TemporalSnapshotsGNNGraph</a> type, but none of the current graph convolution and pooling layers support temporal graphs as input. Currently, there are a few convolutional layers that take as input the special case of static graphs with temporal features. In this project, we will implement new layers that take temporal graphs as input, and we will create tutorials demonstrating how to use them.</p>
<p><strong>Duration</strong>: 350h. <strong>Expected difficulty</strong>: medium. <strong>Expected outcome</strong>: Implementation of new convolutional layers for temporal graphs and example tutorials.</p>
<h2 id="recommended_skills"><a href="#recommended_skills" class="header-anchor">Recommended skills</a></h2>
<p>Familiarity with graph neural networks and Flux.jl.</p>
<h2 id="mentors"><a href="#mentors" class="header-anchor">Mentors </a></h2>
<p><a href="https://github.com/CarloLucibello">Carlo Lucibello</a> &#40;author of <a href="https://github.com/CarloLucibello/GraphNeuralNetworks.jl">GraphNeuralNetworks.jl</a>&#41;. Feel free to contact us on the <a href="https://Julialang.slack.com/">Julia Slack Workspace</a> or by opening an issue in the GitHub repo.</p>
<h1 id="gui_projects_summer_of_code"><a href="#gui_projects_summer_of_code" class="header-anchor">GUI projects – Summer of Code</a></h1>
<h2 id="qml_and_makie_integration"><a href="#qml_and_makie_integration" class="header-anchor">QML and Makie integration</a></h2>
<p>The <a href="https://github.com/barche/QML.jl">QML.jl</a> package provides Julia bindings for <a href="https://doc.qt.io/qt-5/qtqml-index.html">Qt QML</a> on Windows, OS X and Linux. In the current state, basic GUI functionality exists, and rough integration with <a href="https://github.com/JuliaPlots/Makie.jl">Makie.jl</a> is available, allowing overlaying QML GUI elements over Makie visualizations.</p>
<h3 id="expected_results"><a href="#expected_results" class="header-anchor">Expected results</a></h3>
<ol>
<li><p><em>Split off the QML code for Makie into a separate package.</em> This will allow specifying proper package compatibility between QML and Makie, without making Makie a mandatory dependency for QML &#40;currently we use <a href="https://github.com/JuliaPackaging/Requires.jl">Requires.jl</a> for that&#41;</p>
</li>
<li><p><em>Improve the integration.</em> Currently, connections between Makie and QML need to be set up mostly manually. We need to implement some commonly used functionality, such as the registration of clicks in a viewport with proper coordinate conversion and navigation of 3D viewports.</p>
</li>
</ol>
<p><strong>Recommended Skills</strong>: Familiarity with both Julia and the Qt framework, some basic C&#43;&#43; skills, affinity with 3D graphics and OpenGL.</p>
<p><strong>Duration: 175h, expected difficulty: medium</strong></p>
<p><strong>Mentors</strong>: <a href="https://github.com/barche">Bart Janssens</a> and <a href="https://github.com/SimonDanisch">Simon Danish</a></p>
<h2 id="web_apps_in_makie_and_jsserve"><a href="#web_apps_in_makie_and_jsserve" class="header-anchor">Web apps in Makie and JSServe</a></h2>
<p><a href="https://github.com/JuliaPlots/Makie.jl">Makie.jl</a> is a visualization ecosystem for the Julia programming language, with a focus on interactivity and performance. <a href="https://github.com/SimonDanisch/JSServe.jl">JSServe.jl</a> is the core infrastructure library that makes Makie&#39;s web-based backend possible.</p>
<p>At the moment, all the necessary ingredients exist for designing web-based User Interfaces &#40;UI&#41; in Makie, but the process itself is quite low-level and time-consuming. The aim of this project is to streamline that process.</p>
<h3 id="expected_results__2"><a href="#expected_results__2" class="header-anchor">Expected results</a></h3>
<ul>
<li><p>Implement novel UI components and refine existing ones.</p>
</li>
<li><p>Introduce data structures suitable for representing complex UIs.</p>
</li>
<li><p>Add simpler syntaxes for common scenarios, akin to Interact&#39;s <a href="https://github.com/JuliaGizmos/Interact.jl#manipulate"><code>@manipulate</code></a> macro.</p>
</li>
<li><p>Improve documentation and tutorials.</p>
</li>
<li><p>Streamline the deployment process.</p>
</li>
</ul>
<p><strong>Bonus tasks.</strong> If time allows, one of the following directions could be pursued.</p>
<ol>
<li><p>Make Makie web-based plots more suitable for general web apps &#40;move more computation to the client side, improve interactivity and responsiveness&#41;.</p>
</li>
<li><p>Generalize the UI infrastructure to native widgets, which are already implemented in Makie but with a different interface.</p>
</li>
</ol>
<p><strong>Desired skills.</strong> Familiarity with HTML, JavaScript, and CSS, as well as reactive programming. Experience with the Julia visualization and UI ecosystem.</p>
<p><strong>Duration.</strong> 350h.</p>
<p><strong>Difficulty.</strong> Medium.</p>
<p><strong>Mentors.</strong> <a href="https://github.com/piever">Pietro Vertechi</a> and <a href="https://github.com/SimonDanisch">Simon Danisch</a>.</p>
<h1 id="high_performance_and_parallel_computing_projects_summer_of_code"><a href="#high_performance_and_parallel_computing_projects_summer_of_code" class="header-anchor">High Performance and Parallel Computing Projects – Summer of Code</a></h1>
<p>Julia is emerging as a serious tool for technical computing and is ideally suited for the ever-growing needs of big data analytics. This set of proposed projects addresses specific areas for improvement in analytics algorithms and distributed data management.</p>
<h2 id="scheduling_algorithms_for_dagger"><a href="#scheduling_algorithms_for_dagger" class="header-anchor">Scheduling Algorithms for Dagger</a></h2>
<p><strong>Difficulty:</strong> Medium &#40;175h&#41;</p>
<p>Dagger.jl is a native Julia framework and scheduler for distributed execution of Julia code and general purpose data parallelism, using dynamic, runtime-generated task graphs which are flexible enough to describe multiple classes of parallel algorithms. This project proposes to implement different scheduling algorithms for Dagger to optimize scheduling of certain classes of distributed algorithms, such as mapreduce and merge sort, and properly utilizing heterogeneous compute resources. Contributors will be expected to find published distributed scheduling algorithms and implement them on top of the Dagger framework, benchmarking scheduling performance on a variety of micro-benchmarks and real problems.</p>
<p>Mentors: <a href="https://github.com/jpsamaroo">Julian Samaroo</a>, <a href="https://github.com/krynju">Krystian Guliński</a></p>
<h2 id="distributed_training"><a href="#distributed_training" class="header-anchor">Distributed Training</a></h2>
<p><strong>Difficulty:</strong> Hard &#40;350h&#41;</p>
<p>Add a distributed training API for Flux models built on top of <a href="https://github.com/JuliaParallel/Dagger.jl">Dagger.jl</a>. More detailed milestones include building Dagger.jl abstractions for <a href="https://github.com/JuliaParallel/UCX.jl">UCX.jl</a>, then building tools to map Flux models into data parallel Dagger DAGs. The final result should demonstrate a Flux model training with multiple devices in parallel via the Dagger.jl APIs. A stretch goal will include mapping operations with a model to a DAG to facilitate model parallelism as well.</p>
<p>There are projects now that host the building blocks: <a href="https://github.com/FluxML/DaggerFlux.jl">DaggerFlux.jl</a> and <a href="https://github.com/DhairyaLGandhi/ResNetImageNet.jl">Distributed Data Parallel Training</a> which can serve as jumping off points.</p>
<p><strong>Skills:</strong> Familiarity with UCX, representing execution models as DAGs, Flux.jl, CUDA.jl and data/model parallelism in machine learning</p>
<p><strong>Mentors:</strong> <a href="https://github.com/jpsamaroo">Julian Samaroo</a>, and <a href="https://github.com/DhairyaLGandhi">Dhairya Gandhi</a></p>
<h2 id="distributed_arrays_over_dagger"><a href="#distributed_arrays_over_dagger" class="header-anchor">Distributed Arrays over Dagger</a></h2>
<p><strong>Difficulty:</strong> Medium &#40;175h&#41;</p>
<p>Array programming is possibly the most powerful abstraction in Julia, yet our distributed arrays support leaves much to be desired. This project&#39;s goal is to implement a new distributed array type on top of the Dagger.jl framework, which will allow this new array type to be easily distributed, multithreaded, and support GPU execution. Contributors will be expected to implement a variety of operations, such as mapreduce, sorting, slicing, and linear algebra, on top of their distributed array implementation. Final results will include extensive scaling benchmarks on a range of configurations, as well as an extensive test suite for supported operations.</p>
<p>Mentors: <a href="https://github.com/jpsamaroo">Julian Samaroo</a>, <a href="https://github.com/evelyne-ringoot">Evelyne Ringoot</a></p>
<h1 id="juliaimages_projects_summer_of_code"><a href="#juliaimages_projects_summer_of_code" class="header-anchor">JuliaImages Projects – Summer of Code</a></h1>
<div class="franklin-toc"><ol><li><a href="#view_all_gsocjsoc_projects">View all GSoC/JSoC Projects</a></li><li><a href="#projects">Projects</a><ol><li><a href="#list_of_projects">List of projects</a></li></ol></li><li><a href="#categorical_variable_encoding">Categorical variable encoding</a><ol><li><a href="#description">Description</a></li><li><a href="#prerequisites">Prerequisites</a></li><li><a href="#your_contribution">Your contribution</a></li><li><a href="#references">References</a></li></ol></li><li><a href="#machine_learning_in_predictive_survival_analysis">Machine Learning in Predictive Survival Analysis</a><ol><li><a href="#description__2">Description</a></li><li><a href="#prerequisites__2">Prerequisites</a></li><li><a href="#your_contribution__2">Your contribution</a></li><li><a href="#references__2">References</a></li></ol></li><li><a href="#deeper_bayesian_integration">Deeper Bayesian Integration</a><ol><li><a href="#description__3">Description</a></li><li><a href="#your_contributions">Your contributions</a></li><li><a href="#references__3">References</a></li><li><a href="#difficulty_medium_to_hard">Difficulty: Medium to Hard</a></li></ol></li><li><a href="#tracking_and_sharing_mlj_workflows_using_mlflow">Tracking and sharing MLJ workflows using MLflow</a><ol><li><a href="#description__4">Description</a></li><li><a href="#prerequisites__3">Prerequisites</a></li><li><a href="#your_contribution__3">Your contribution</a></li><li><a href="#references__4">References</a></li></ol></li><li><a href="#speed_demons_only_need_apply">Speed demons only need apply</a><ol><li><a href="#description__5">Description</a></li><li><a href="#prerequisites__4">Prerequisites</a></li><li><a href="#your_contribution__4">Your contribution</a></li><li><a href="#references__5">References</a></li></ol></li><li><a href="#improving_test_coverage_175_hours">Improving test coverage &#40;175 hours&#41;</a></li><li><a href="#multi-threading_improvement_projects_175_hours_each">Multi-threading Improvement Projects &#40;175 hours each&#41;</a></li><li><a href="#automation_of_testing_performance_benchmarking_350_hours">Automation of testing / performance benchmarking &#40;350 hours&#41;</a></li><li><a href="#documenterjl">Documenter.jl</a></li><li><a href="#fluid-structure_interaction_example">Fluid-Structure Interaction Example</a></li><li><a href="#investigation_of_performant_assembly_strategies">Investigation of Performant Assembly Strategies</a><ol><li><a href="#training_on_very_large_graphs">Training on very large graphs  </a></li><li><a href="#adding_graph_convolutional_layers">Adding graph convolutional layers </a></li><li><a href="#adding_models_and_examples">Adding models and examples</a></li><li><a href="#adding_graph_datasets">Adding graph datasets</a></li><li><a href="#implement_layers_for_heterogeneous_graphs">Implement layers for heterogeneous graphs</a></li><li><a href="#improving_performance_using_sparse_linear_algebra">Improving performance using sparse linear algebra </a></li><li><a href="#support_for_amgdpu_and_apple_silicon">Support for AMGDPU and Apple Silicon</a></li><li><a href="#implement_layers_for_temporal_graphs">Implement layers for Temporal Graphs</a></li></ol></li><li><a href="#recommended_skills">Recommended skills</a></li><li><a href="#mentors">Mentors </a></li><li><a href="#qml_and_makie_integration">QML and Makie integration</a><ol><li><a href="#expected_results">Expected results</a></li></ol></li><li><a href="#web_apps_in_makie_and_jsserve">Web apps in Makie and JSServe</a><ol><li><a href="#expected_results__2">Expected results</a></li></ol></li><li><a href="#scheduling_algorithms_for_dagger">Scheduling Algorithms for Dagger</a></li><li><a href="#distributed_training">Distributed Training</a></li><li><a href="#distributed_arrays_over_dagger">Distributed Arrays over Dagger</a></li><li><a href="#benchmarking_against_other_frameworks">Benchmarking against other frameworks</a></li><li><a href="#where_to_go_for_discussion_and_to_find_mentors">Where to go for discussion and to find mentors</a></li><li><a href="#c">C&#43;&#43;</a><ol><li><a href="#cxxwrap_stl">CxxWrap STL</a><ol><li><a href="#expected_outcome">Expected outcome</a></li></ol></li></ol></li><li><a href="#rust">Rust</a><ol><li><a href="#general_goal_of_juliaconstraints">General goal of JuliaConstraints</a></li></ol></li><li><a href="#constraint_programming-based_design_for_kumi_kumi_slope">Constraint Programming-Based Design for Kumi Kumi Slope</a><ol><li><a href="#core_objectives">Core Objectives</a></li></ol></li><li><a href="#agentsjl">Agents.jl</a></li><li><a href="#dynamicalsystemsjl">DynamicalSystems.jl</a></li><li><a href="#large_language_model_projects">Large Language Model Projects</a><ol><li><a href="#project_1_enhancing_llama2jl_with_gpu_support">Project 1: Enhancing llama2.jl with GPU Support</a></li><li><a href="#project_2_llamajl_-_low-level_c_interface">Project 2: Llama.jl - Low-level C interface</a></li><li><a href="#project_3_supercharging_the_knowledge_base_of_aihelpmejl">Project 3: Supercharging the Knowledge Base of AIHelpMe.jl</a></li><li><a href="#project_4_enhancing_julias_ai_ecosystem_with_colbert_v2_for_efficient_document_retrieval">Project 4: Enhancing Julia&#39;s AI Ecosystem with ColBERT v2 for Efficient Document Retrieval</a></li><li><a href="#project_5_enhancing_promptingtoolsjl_with_advanced_schema_support_and_functionality">Project 5: Enhancing PromptingTools.jl with Advanced Schema Support and Functionality</a></li><li><a href="#project_6_expanding_the_julia_large_language_model_leaderboard">Project 6: Expanding the Julia Large Language Model Leaderboard</a></li><li><a href="#project_7_counterfactuals_for_llms_model_explainability_and_generative_ai">Project 7: Counterfactuals for LLMs &#40;<em>Model Explainability</em> and <em>Generative AI</em>&#41;</a></li></ol></li><li><a href="#how_to_contact_us">How to Contact Us</a></li><li><a href="#observational_health_subecosystem_projects">Observational Health Subecosystem Projects</a><ol><li><a href="#project_1_developing_tooling_for_observational_health_research_in_julia">Project 1: Developing Tooling for Observational Health Research in Julia</a></li><li><a href="#project_2_developing_patient_level_prediction_tooling_within_julia">Project 2: Developing Patient Level Prediction Tooling within Julia</a></li></ol></li><li><a href="#medical_imaging_subecosystem_projects">Medical Imaging Subecosystem Projects</a><ol><li><a href="#project_3_adding_functionalities_to_medical_imaging_visualizations">Project 3: Adding functionalities to medical imaging visualizations</a></li><li><a href="#project_4_adding_dataset-wide_functions_and_integrations_of_augmentations">Project 4: Adding dataset-wide functions and integrations of augmentations</a></li><li><a href="#project_5_highly-efficient_mri_simulations_with_multi-vendor_gpu_support">Project 5: Highly-efficient MRI Simulations with Multi-Vendor GPU Support</a></li></ol></li><li><a href="#midification_of_music_from_wave_files">MIDIfication of music from wave files</a></li><li><a href="#efficient_symbolic-numeric_set_computations">Efficient symbolic-numeric set computations</a></li><li><a href="#reachability_with_sparse_polynomial_zonotopes">Reachability with sparse polynomial zonotopes</a></li><li><a href="#improving_the_hybrid_systems_reachability_api">Improving the hybrid systems reachability API</a></li><li><a href="#panel_data_analysis">Panel data analysis</a><ol><li><a href="#description__6">Description</a></li><li><a href="#prerequisites__5">Prerequisites</a></li><li><a href="#your_contribution__5">Your contribution</a></li><li><a href="#references__6">References</a></li></ol></li><li><a href="#distributionsjl_expansion">Distributions.jl Expansion</a><ol><li><a href="#prerequisites__6">Prerequisites</a></li><li><a href="#your_contribution__6">Your contribution</a></li></ol></li><li><a href="#hypothesistestingjl_expansion">HypothesisTesting.jl Expansion</a><ol><li><a href="#prerequisites__7">Prerequisites</a></li><li><a href="#your_contribution__7">Your contribution</a></li><li><a href="#references__7">References</a></li></ol></li><li><a href="#crraojl">CRRao.jl</a><ol><li><a href="#description__7">Description</a></li><li><a href="#prerequisites__8">Prerequisites</a></li><li><a href="#your_contribution__8">Your contribution</a></li></ol></li><li><a href="#juliastats_improvements">JuliaStats Improvements </a><ol><li><a href="#description__8">Description</a></li><li><a href="#prerequisites__9">Prerequisites</a></li><li><a href="#your_contribution__9">Your contribution</a></li></ol></li><li><a href="#surveyjl">Survey.jl</a><ol><li><a href="#prerequisites__10">Prerequisites</a></li><li><a href="#your_contribution__10">Your contribution</a></li><li><a href="#references__8">References</a></li></ol></li><li><a href="#smoothing_non-linear_continuous_time_systems">Smoothing non-linear continuous time systems</a><ol><li><a href="#reinforcement_learning_environments">Reinforcement Learning Environments</a><ol><li><a href="#expected_outcome__2">Expected outcome</a></li></ol></li><li><a href="#alphazerojl">AlphaZero.jl</a><ol><li><a href="#expected_outcomes">Expected Outcomes</a></li></ol></li></ol></li><li><a href="#numerical_linear_algebra">Numerical Linear Algebra</a><ol><li><a href="#matrix_functions">Matrix functions</a></li></ol></li><li><a href="#better_bignums_integration">Better Bignums Integration</a><ol><li><a href="#special_functions">Special functions</a></li><li><a href="#a_julia-native_ccsa_optimization_algorithm">A Julia-native CCSA optimization algorithm</a></li></ol></li><li><a href="#massive_parallel_factorized_bouncy_particle_sampler">Massive parallel factorized bouncy particle sampler</a></li><li><a href="#machine_learning_time_series_regression">Machine Learning Time Series Regression</a></li><li><a href="#machine_learning_for_nowcasting_and_forecasting">Machine learning for nowcasting and forecasting</a></li><li><a href="#time_series_forecasting_at_scales">Time series forecasting at scales</a></li><li><a href="#gpu_accelerated_simulator_of_clifford_circuits">GPU accelerated simulator of Clifford Circuits.</a></li><li><a href="#a_zoo_of_quantum_error_correcting_codes_andor_decoders">A Zoo of Quantum Error Correcting codes and/or decoders</a></li><li><a href="#leftright_multiplications_with_small_gates">Left/Right multiplications with small gates.</a></li><li><a href="#generation_of_fault_tolerant_ecc_circuits_flag_qubit_circuits_and_more">Generation of Fault Tolerant ECC Circuits, Flag Qubit Circuits and more</a></li><li><a href="#measurement-based_quantum_computing_mbqc_compiler">Measurement-Based Quantum Computing &#40;MBQC&#41; compiler</a></li><li><a href="#implementing_a_graph_state_simulator">Implementing a Graph State Simulator</a></li><li><a href="#simulation_of_slightly_non-clifford_circuits_and_states">Simulation of Slightly Non-Clifford Circuits and States</a></li><li><a href="#magic_state_modeling_-_distillation_injection_etc">Magic State Modeling - Distillation, Injection, Etc</a></li><li><a href="#gpu_accelerated_operators_and_ode_solvers">GPU accelerated operators and ODE solvers</a></li><li><a href="#autodifferentiation">Autodifferentiation</a></li><li><a href="#closer_integration_with_the_sciml_ecosystem">Closer Integration with the SciML Ecosystem</a></li><li><a href="#efficient_tensor_differentiation">Efficient Tensor Differentiation</a></li><li><a href="#symbolic_root_finding">Symbolic root finding</a></li><li><a href="#symbolic_integration_in_symbolicsjl">Symbolic Integration in Symbolics.jl</a></li><li><a href="#xla-style_optimization_from_symbolic_tracing">XLA-style optimization from symbolic tracing</a></li><li><a href="#automatically_improving_floating_point_accuracy_herbie">Automatically improving floating point accuracy &#40;Herbie&#41;</a></li><li><a href="#parquetjl_enhancements">Parquet.jl enhancements</a></li><li><a href="#dataframesjl_join_enhancements">DataFrames.jl join enhancements</a></li><li><a href="#project_1_conformal_prediction_meets_bayes_predictive_uncertainty">Project 1: Conformal Prediction meets Bayes &#40;<em>Predictive Uncertainty</em>&#41;</a></li><li><a href="#project_2_counterfactual_regression_model_explainability">Project 2: Counterfactual Regression &#40;<em>Model Explainability</em>&#41;</a></li><li><a href="#project_3_counterfactuals_for_llms_model_explainability_and_generative_ai">Project 3: Counterfactuals for LLMs &#40;<em>Model Explainability</em> and <em>Generative AI</em>&#41;</a></li><li><a href="#project_4_from_counterfactuals_to_interventions_recourse_through_minimal_causal_interventions">Project 4: From Counterfactuals to Interventions &#40;Recourse through Minimal Causal Interventions&#41;</a></li><li><a href="#about_us">About Us</a></li><li><a href="#how_to_contact_us__2">How to Contact Us</a></li><li><a href="#testing_and_benchmarking_of_topoptjl">Testing and benchmarking of TopOpt.jl</a></li><li><a href="#machine_learning_in_topology_optimization">Machine learning in topology optimization</a></li><li><a href="#optimization_on_a_uniform_rectilinear_grid">Optimization on a uniform rectilinear grid</a></li><li><a href="#adaptive_mesh_refinement_for_topology_optimization">Adaptive mesh refinement for topology optimization</a></li><li><a href="#heat_transfer_design_optimization">Heat transfer design optimization</a></li><li><a href="#compiler-based_automatic_differentiation_with_enzymejl">Compiler-based automatic differentiation with Enzyme.jl</a></li><li><a href="#advanced_visualization_and_in-situ_visualization_with_paraview">Advanced visualization and in-situ visualization with ParaView</a></li><li><a href="#implementing_models_from_posteriordb_in_turing_julia">Implementing models from PosteriorDB in Turing / Julia</a></li><li><a href="#improving_the_integration_between_turing_and_turings_mcmc_inference_packages">Improving the integration between Turing and Turing’s MCMC inference packages</a></li><li><a href="#gpu_support_for_normalizingflowsjl_and_bijectorsjl">GPU support for NormalizingFlows.jl and Bijectors.jl</a></li><li><a href="#batched_support_for_normalizingflowsjl_and_bijectorsjl">Batched support for NormalizingFlows.jl and Bijectors.jl</a></li><li><a href="#targets_for_benchmarking_samplers_with_vectorization_gpu_and_high-order_derivative_supports">Targets for Benchmarking Samplers with vectorization, GPU and high-order derivative supports</a></li><li><a href="#vs_code_extension">VS Code extension</a></li><li><a href="#package_installation_ui">Package installation UI</a></li><li><a href="#code_generation_improvements_and_async_abi">Code generation improvements and async ABI</a></li><li><a href="#wasm_threading">Wasm threading</a></li><li><a href="#high_performance_low-level_integration_of_js_objects">High performance, Low-level integration of js objects</a></li><li><a href="#dom_integration">DOM Integration</a></li><li><a href="#porting_existing_web-integration_packages_to_the_wasm_platform">Porting existing web-integration packages to the wasm platform</a></li><li><a href="#native_dependencies_for_the_web">Native dependencies for the web</a></li><li><a href="#distributed_computing_with_untrusted_parties">Distributed computing with untrusted parties</a></li><li><a href="#deployment">Deployment</a></li></ol></div>
<p><a href="https://github.com/JuliaImages">JuliaImages</a> &#40;see the <a href="https://juliaimages.github.io">documentation</a>&#41; is a framework in Julia for multidimensional arrays, image processing, and computer vision &#40;CV&#41;. It has an active development community and offers many features that unify CV and biomedical 3D/4D image processing, support big data, and promote interactive exploration.</p>
<p>Often the best ideas are the ones that candidate SoC contributors come up with on their own. We are happy to <a href="https://github.com/JuliaImages/Images.jl/discussions/new?category&#61;jsoc">discuss such ideas</a> and help you refine your proposal.  Below are some potential project ideas that might help spur some thoughts. In general, anything that is missing in JuliaImages, and worths three-months&#39; development can be considered as potential GSoC ideas. See the bottom of this page for information about mentors.</p>
<h2 id="benchmarking_against_other_frameworks"><a href="#benchmarking_against_other_frameworks" class="header-anchor">Benchmarking against other frameworks</a></h2>
<p><strong>Difficulty:</strong> Medium &#40;175h&#41; &#40;High priority&#41;</p>
<p>JuliaImages provides high-quality implementations of many algorithms; however, as yet there is no set of benchmarks that compare our code against that of other image-processing frameworks.  Developing such benchmarks would allow us to advertise our strengths and/or identify opportunities for further improvement.  See also the OpenCV project below.</p>
<p>Benchmarks for several performance-sensitive packages &#40;e.g., ImageFiltering, ImageTransformations, ImageMorphology, ImageContrastAdjustment, ImageEdgeDetection, ImageFeatures, and/or ImageSegmentation&#41; against frameworks like Scikit-image and OpenCV, and optionally others like ITK, ImageMagick, and Matlab/Octave. See also the <a href="https://github.com/JuliaImages/image_benchmarks">image benchmarks</a> repository.</p>
<p>This task splits into at least two pieces:</p>
<ul>
<li><p>developing frameworks for collecting the data, and</p>
</li>
<li><p>visualizing the results.</p>
</li>
</ul>
<p>One should also be aware of the fact that differences in implementation &#40;which may include <a href="https://github.com/JuliaImages/Images.jl/pull/855">differences in quality</a>&#41; may complicate the interpretation of some benchmarks.</p>
<p><strong>Skills:</strong> JuliaImages experiences is required. Some familiarities with other image processing frameworks is preferred.</p>
<p><strong>Mentors:</strong> <a href="https://github.com/timholy">Tim Holy</a></p>
<h2 id="where_to_go_for_discussion_and_to_find_mentors"><a href="#where_to_go_for_discussion_and_to_find_mentors" class="header-anchor">Where to go for discussion and to find mentors</a></h2>
<p>Interested contributors are encouraged to <a href="https://github.com/JuliaImages/Images.jl/discussions/new">open an discussion in Images.jl</a> to introduce themselves and discuss the detailed project ideas. To increase the chance of getting useful feedback, please provide detailed plans and ideas &#40;don&#39;t just copy the contents here&#41;.</p>
<h1 id="language_interoperability_summer_of_code"><a href="#language_interoperability_summer_of_code" class="header-anchor">Language interoperability – Summer of Code</a></h1>
<h2 id="c"><a href="#c" class="header-anchor">C&#43;&#43;</a></h2>
<h3 id="cxxwrap_stl"><a href="#cxxwrap_stl" class="header-anchor">CxxWrap STL</a></h3>
<p>The <a href="https://github.com/JuliaInterop/CxxWrap.jl">CxxWrap.jl</a> package provides a way to load compiled C&#43;&#43; code into Julia. It exposes a small fraction of the C&#43;&#43; standard library to Julia, but many more functions and containers &#40;e.g. <code>std::map</code>&#41; still need to be exposed. The objective of this project is to improve C&#43;&#43; standard library coverage.</p>
<h4 id="expected_outcome"><a href="#expected_outcome" class="header-anchor">Expected outcome</a></h4>
<ol>
<li><p>Add missing STL container types &#40;easy&#41;</p>
</li>
<li><p>Add support for STL algorithms &#40;intermediate&#41;</p>
</li>
<li><p>Investigate improvement of compile times and selection of included types &#40;advanced&#41;</p>
</li>
</ol>
<p><strong>Recommended Skills</strong>: Familiarity with both Julia and C&#43;&#43;</p>
<p><strong>Duration: 175h, expected difficulty: hard</strong></p>
<p><strong>Mentor</strong>: <a href="https://github.com/barche">Bart Janssens</a></p>
<h2 id="rust"><a href="#rust" class="header-anchor">Rust</a></h2>
<p>Take a look at the <a href="https://julialang.org/jsoc/gsoc/pluto/#wrapping_a_rust_http_server_in_julia">hyper.rs project, listed on the &quot;Pluto&quot; page</a>, about wrapping a Rust HTTP server in a Julia package.</p>
<h1 id="constraint_programming_in_julia"><a href="#constraint_programming_in_julia" class="header-anchor">Constraint Programming in Julia</a></h1>
<p><a href="https://juliaconstraints.github.io/">JuliaConstraints</a> is an organization supporting packages for Constraint Programming in Julia. Although it is independent of it, it aims for a tight integration with JuMP.jl over time. For a detailed overview of basic Constraint Programming in Julia, please have a look at our video from JuliaCon 2021 <a href="https://youtu.be/G4siuvNMj0c">Put some constraints into your life with JuliaCon&#40;straints&#41;</a>.</p>
<h3 id="general_goal_of_juliaconstraints"><a href="#general_goal_of_juliaconstraints" class="header-anchor">General goal of JuliaConstraints</a></h3>
<p>Often, problem-solving involves taking two actions: model and solve. Typically, there is a trade-off between ease of modeling and efficiency of solving. Therefore, one is often required to be a specialist to model and solve an optimization problem efficiently. We investigate the theoretical fundamentals and the implementation of tools to automize and make optimization frameworks. A general user should focus on the model of practical problems, regardless of the software or hardware available. Furthermore, we aim to encourage technical users to use our tools to improve their solving efficiency.</p>
<p><strong>Mentor:</strong> <a href="http://baffier.fr/">Jean-Francois Baffier</a> &#40;<a href="https://github.com/Azzaare">azzaare@github</a>&#41;</p>
<h2 id="constraint_programming-based_design_for_kumi_kumi_slope"><a href="#constraint_programming-based_design_for_kumi_kumi_slope" class="header-anchor">Constraint Programming-Based Design for Kumi Kumi Slope</a></h2>
<p>This project is at the forefront of developing a level-design tool for the Kumi Kumi Slope game, leveraging the Julia programming language&#39;s capabilities. It prioritizes creating an interactive graphical user interface &#40;GUI&#41; for users to actively engage in design optimization. While &#40;GL&#41;Makie.jl is a strong candidate for this GUI, the project remains open to other innovative solutions, such as a Genie.jl-based interface, to accommodate diverse development preferences. Key to this initiative is handling arbitrary domains, generating pools of solutions for multi-objective optimization, and providing visual outputs of game designs. This venture into Constraint Programming &#40;CP&#41; sets the groundwork for efficiency in design while embracing user-defined aesthetic goals and marks a pioneering step towards human-machine collaboration in architectural design.</p>
<h3 id="core_objectives"><a href="#core_objectives" class="header-anchor">Core Objectives</a></h3>
<ol>
<li><p><strong>Multi-Objective Optimization and Arbitrary Domains &#40;100-150 hours&#41;</strong></p>
<ul>
<li><p><em>Framework for Multi-Objective Optimization</em>: Establish a robust framework to tackle multiple objectives simultaneously, ranging from efficiency and compactness to playability and aesthetics.</p>
</li>
<li><p><em>Support for Arbitrary Domains</em>: Craft a method to define and manipulate arbitrary domains within the CP model, enabling a wide array of component types and design constraints.</p>
</li>
</ul>
</li>
<li><p><strong>Pools of Solutions and Interactive GUI Development &#40;150-200 hours&#41;</strong></p>
<ul>
<li><p><em>Generation of Solution Pools</em>: Design algorithms to create diverse pools of feasible solutions, catering to different optimization criteria and user preferences, fostering a nuanced approach to human-machine collaboration in design.</p>
</li>
<li><p><em>Interactive GUI Development</em>: Embark on the development of an interactive GUI, with &#40;GL&#41;Makie.jl or alternative tools, to facilitate the visualization and manipulation of Kumi Kumi Slope designs, empowering users to explore, select, and refine designs in a user-centric environment.</p>
</li>
</ul>
</li>
<li><p><strong>Visual Output and User Interaction &#40;100-150 hours&#41;</strong></p>
<ul>
<li><p><em>Visual Representation of Designs</em>: Guarantee that all potential solutions are visually represented within the GUI, enhancing user ability to evaluate and contrast different designs.</p>
</li>
<li><p><em>Feedback Mechanism for Design Refinement</em>: Integrate a feedback loop in the GUI, allowing user interactions to refine the solution pool, aligning it closer to user preferences and exemplifying the project&#39;s commitment to human-machine collaborative design.</p>
</li>
</ul>
</li>
</ol>
<p>This proposal aims to deliver a significant and impactful tool by the end of the GSoC period. It encourages candidates to dive deep into areas of particular interest, providing flexibility in project focus. By emphasizing realistic targets like the development of an interactive GUI and foundational work on handling arbitrary domains and multi-objective optimization, this project sets a precedent for future advancements in game design and opens the door to broader applications requiring sophisticated design and optimization tools.</p>
<h1 id="dynamical_systems_complex_systems_nonlinear_dynamics_summer_of_code"><a href="#dynamical_systems_complex_systems_nonlinear_dynamics_summer_of_code" class="header-anchor">Dynamical systems, complex systems &amp; nonlinear dynamics – Summer of Code</a></h1>
<h2 id="agentsjl"><a href="#agentsjl" class="header-anchor">Agents.jl</a></h2>
<p><strong>Difficulty</strong>: Medium to Hard.</p>
<p><strong>Length</strong>: 175 to 350 hours depending on the project.</p>
<p><a href="https://juliadynamics.github.io/Agents.jl/stable/">Agents.jl</a> is a pure Julia framework for agent-based modeling &#40;ABM&#41;.  It has an extensive list of features, excellent performance and is easy to learn, use, and extend. Comparisons with other popular frameworks written in Python or Java &#40;NetLOGO, MASON, Mesa&#41;, show that Agents.jl outperforms all of them in computational speed, list of features and usability.</p>
<p>In this project, contributors will be paired with lead developers of Agents.jl to improve Agents.jl with more features, better performance, and overall higher polish. We are open to discuss with potential candidate a project description and outline for it&#33;</p>
<p>Possible features to implement are:</p>
<div class="tight-list"><ul>
<li><p>GPU and/or HPC support in Agents.jl by integrating existing ABM packages &#40;Vanaha.jl or CellBasedModels.jl&#41; into Agents.jl API.</p>
</li>
<li><p>New type of space representing a planet, which can be used in climate policy or human evolution modelling, and new interface for an overarching ABM composed of several smaller ABMs</p>
</li>
</ul></div>
<p><strong>Pre-requisite</strong>: Having already contributed to a Julia package either in JuliaDynamics or of sufficient relevance to JuliaDynamics.</p>
<p><strong>Recommended Skills</strong>: Familiarity with agent based modelling, Agents.jl and Julia&#39;s Type System. Background in complex systems, sociology, or nonlinear dynamics is not required but would be advantageous.</p>
<p><strong>Expected Results</strong>: Well-documented, well-tested useful new features for Agents.jl.</p>
<p><strong>Mentors</strong>: <a href="https://github.com/Datseris">George Datseris</a>.</p>
<h2 id="dynamicalsystemsjl"><a href="#dynamicalsystemsjl" class="header-anchor">DynamicalSystems.jl</a></h2>
<p><strong>Difficulty:</strong> Easy to Medium to Hard, depending on the project.</p>
<p><strong>Length</strong>: 175 to 350 hours, depending on the project.</p>
<p><a href="https://juliadynamics.github.io/DynamicalSystems.jl/latest/">DynamicalSystems.jl</a> is an <a href="https://dsweb.siam.org/The-Magazine/Article/winners-of-the-dsweb-2018-software-contest">award-winning</a> Julia software library for dynamical systems, nonlinear dynamics, deterministic chaos, and nonlinear time series analysis. It has an impressive list of features, but one can never have enough. In this project, contributors will be able to enrich DynamicalSystems.jl with new algorithms and enrich their knowledge of nonlinear dynamics and computer-assisted exploration of complex systems.</p>
<p>We do not outline possible projects here, and instead we invite interested candidates to reach out to one  of the developers of DynamicalSystems.jl or its subpackages to devise a project outline. We strongly welcome candidates that already have potential project ideas in mind. To get ideas of possible projects we recommend having a look at the list of the open issues in the sub-packages of DynamicalSystems.jl.</p>
<p><strong>Pre-requisite</strong>: Having already contributed to a Julia package either in JuliaDynamics or of sufficient relevance to JuliaDynamics.</p>
<p><strong>Recommended Skills</strong>: Familiarity with nonlinear dynamics and/or differential equations and/or data analysis and the Julia language.</p>
<p><strong>Expected Results</strong>: Well-documented, well-tested new algorithms for DynamicalSystems.jl.</p>
<p><strong>Mentors</strong>: <a href="https://github.com/Datseris">George Datseris</a></p>
<h1 id="juliagenai_projects"><a href="#juliagenai_projects" class="header-anchor">JuliaGenAI Projects</a></h1>
<p><img src="https://github.com/JuliaGenAI/juliagenai.org/blob/main/assets/logos/logo-256.png?raw&#61;true" alt="JuliaGenAI Logo" /></p>
<p><a href="https://github.com/JuliaGenAI">JuliaGenAI</a> is an organization focused on advancing Generative AI research and looking for its applications within the Julia programming language ecosystem. Our community comprises AI researchers, developers, and enthusiasts passionate about pushing the boundaries of Generative AI using Julia&#39;s high-performance capabilities. We strive to create innovative tools and solutions that leverage the unique strengths of Julia in handling complex AI challenges.</p>
<p>There is a high overlap with organizations, you might be also interested in:</p>
<ul>
<li><p><a href="https://julialang.org/jsoc/gsoc/MLJ/">Projects with MLJ.jl</a> - For more traditional machine learning projects</p>
</li>
<li><p><a href="https://julialang.org/jsoc/gsoc/machine-learning/">Projects in Reinforcement Learning</a> - For projects around AlphaZero.jl</p>
</li>
<li><p><a href="https://fluxml.ai/gsoc/">Projects with FluxML</a> - For projects around Flux.jl, the backbone of Julia&#39;s deep learning ecosystem</p>
</li>
</ul>
<h2 id="large_language_model_projects"><a href="#large_language_model_projects" class="header-anchor">Large Language Model Projects</a></h2>
<h3 id="project_1_enhancing_llama2jl_with_gpu_support"><a href="#project_1_enhancing_llama2jl_with_gpu_support" class="header-anchor">Project 1: Enhancing llama2.jl with GPU Support</a></h3>
<p><strong>Project Overview:</strong> <a href="https://github.com/cafaxo/Llama2.jl">Llama2.jl</a> is a Julia native port for Llama architectures, originally based on <a href="https://github.com/karpathy/llama2.c">llama2.c</a>. This project aims to enhance Llama2.jl by implementing GPU support through KernelAbstraction.jl, significantly improving its performance.</p>
<p><strong>Mentor:</strong> <a href="https://github.com/cpfiffer">Cameron Pfiffer</a></p>
<p><strong>Project Difficulty</strong>: Hard</p>
<p><strong>Estimated Duration</strong>: 350 hours</p>
<p><strong>Ideal Candidate Profile:</strong></p>
<ul>
<li><p>Proficiency in Julia programming</p>
</li>
<li><p>Understanding of GPU computing</p>
</li>
<li><p>Experience with KernelAbstractions.jl</p>
</li>
</ul>
<p><strong>Project Goals and Deliverables:</strong></p>
<ul>
<li><p>Implementation of GPU support in llama2.jl</p>
</li>
<li><p>Comprehensive documentation and examples demonstrating the performance improvements</p>
</li>
<li><p>Contribution to llama2.jl&#39;s existing codebase and documentation</p>
</li>
</ul>
<h3 id="project_2_llamajl_-_low-level_c_interface"><a href="#project_2_llamajl_-_low-level_c_interface" class="header-anchor">Project 2: Llama.jl - Low-level C interface</a></h3>
<p><strong>Project Overview:</strong> <a href="https://github.com/marcom/Llama.jl">Llama.jl</a> is a Julia interface for <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> that powers many open-source tools today. It&#39;s currently leveraging only the high-level binaries. This project focuses on generating a low-level C interface to llama.cpp, enabling native access to internal model states, which would open incredible research opportunities and attractive applications &#40;eg, constraint generation, novel sampling algorithms, etc.&#41;</p>
<p><strong>Mentor:</strong> <a href="https://github.com/cpfiffer">Cameron Pfiffer</a></p>
<p><strong>Project Difficulty</strong>: Hard</p>
<p><strong>Estimated Duration</strong>: 175 hours</p>
<p><strong>Ideal Candidate Profile:</strong></p>
<ul>
<li><p>Proficiency in Julia and C programming</p>
</li>
</ul>
<p><strong>Project Goals and Deliverables:</strong></p>
<ul>
<li><p>Auto-generated C interface for tokenization and sampling functionality</p>
</li>
<li><p>Access to internal model states in llama.cpp during token generation</p>
</li>
<li><p>Ability to generate text from a given model state</p>
</li>
</ul>
<h3 id="project_3_supercharging_the_knowledge_base_of_aihelpmejl"><a href="#project_3_supercharging_the_knowledge_base_of_aihelpmejl" class="header-anchor">Project 3: Supercharging the Knowledge Base of AIHelpMe.jl</a></h3>
<p><strong>Project Overview:</strong></p>
<p>Julia stands out as a high-performance language that&#39;s essential yet underrepresented in GenAI training datasets. AIHelpMe.jl is our ambitious initiative to bridge this gap by enhancing Large Language Models&#39; &#40;LLMs&#41; understanding of Julia by providing this knowledge via In-Context Learning &#40;RAG, prompting&#41;. This project focuses on expanding the embedded knowledge base with up-to-date, context-rich Julia information and optimizing the Q&amp;A pipeline to deliver precise, relevant answers. By injecting targeted Julia code snippets and documentation into queries, AIHelpMe.jl aims to significantly improve the accuracy and utility of generative AI for Julia developers worldwide.</p>
<p><strong>Mentor:</strong> <a href="https://github.com/svilupp">Jan Siml</a> / <code>@svilup</code> on JuliaLang Slack / <code>Jan Siml</code> on Julia Zulip</p>
<p><strong>Project Difficulty:</strong> Medium</p>
<p><strong>Estimated Duration:</strong> 175 hours</p>
<p><strong>Who Should Apply:</strong></p>
<ul>
<li><p>Individuals with a solid grasp of the Julia programming language who are eager to deepen their involvement in the Julia and AI communities.</p>
</li>
<li><p>Applicants should have a foundational understanding of Retrieval-Augmented Generation &#40;RAG&#41; optimization techniques and a passion for improving AI technologies.</p>
</li>
</ul>
<p><strong>Project Goals and Deliverables:</strong></p>
<ol>
<li><p><strong>Knowledge Base Expansion:</strong> Grow the AIHelpMe.jl knowledge base to include comprehensive, up-to-date resources from critical Julia ecosystems such as the Julia documentation site, DataFrames, Makie, Plots/StatsPlots, the Tidier-verse, SciML, and more. See <a href="https://github.com/svilupp/AIHelpMe.jl/issues/3">Github Issue</a> for more details. This expansion is crucial for enriching the context and accuracy of AI-generated responses related to Julia programming.</p>
</li>
<li><p><strong>Performance Tuning:</strong> Achieve at least a 10&#37; improvement in accuracy and relevance on a golden Q&amp;A dataset, refining the AIHelpMe.jl Q&amp;A pipeline for enhanced performance.</p>
</li>
</ol>
<h3 id="project_4_enhancing_julias_ai_ecosystem_with_colbert_v2_for_efficient_document_retrieval"><a href="#project_4_enhancing_julias_ai_ecosystem_with_colbert_v2_for_efficient_document_retrieval" class="header-anchor">Project 4: Enhancing Julia&#39;s AI Ecosystem with ColBERT v2 for Efficient Document Retrieval</a></h3>
<p><strong>Project Overview:</strong></p>
<p>Dive into the forefront of generative AI and information retrieval by bringing ColBERT v2, a cutting-edge document retrieval and re-ranking framework, into the Julia programming world. This initiative aims not only to translate ColBERT v2 to operate natively in Julia but to seamlessly integrate it with AIHelpMe.jl &#40;and other downstream libraries&#41;. This integration promises to revolutionize the way users interact with AI by offering locally-hosted, more cost-efficient and highly performant document search capabilities. By enabling this sophisticated technology to run locally, we reduce dependency on large-scale commercial platforms, ensuring privacy and control over data, while maintaining minimal memory overheads.</p>
<p><strong>Mentor:</strong> <a href="https://github.com/svilupp">Jan Siml</a> <code>@svilup</code> on JuliaLang Slack / <code>Jan Siml</code> on Julia Zulip</p>
<p><strong>Project Difficulty:</strong> Hard</p>
<p><strong>Estimated Duration:</strong> 350 hours</p>
<p><strong>Ideal Candidate Profile:</strong></p>
<ul>
<li><p>Solid understanding of transformer architectures, with proficiency in Flux.jl or Transformers.jl.</p>
</li>
<li><p>Experience in semantic document retrieval &#40;or with Retrieval-Augmented Generation applications&#41;, and a keen interest in pushing the boundaries of AI technology.</p>
</li>
<li><p>A commitment to open-source development and a passion for contributing to an evolving ecosystem of Julia-based AI tools.</p>
</li>
</ul>
<p><strong>Project Goals and Expected Outcomes:</strong></p>
<ol>
<li><p><strong>Native Julia Translation of ColBERT v2:</strong> Successfully adapt ColBERT v2 to run within the Julia ecosystem. Focus is only the indexing and retrieval functionality of ColBERT v2, eg, the Retrieval and Indexing snippets you see in the <a href="https://github.com/stanford-futuredata/ColBERT">Example Usage Section</a>. For guidance, refer to the existing Indexing and Retrieval examples.</p>
</li>
<li><p><strong>Integration with AIHelpMe.jl:</strong> Seamlessly integrate as one of the embedding and retrieval backends AIHelpMe.jl &#40;defined in PromptingTools.jl&#41;.</p>
</li>
<li><p><strong>Package Registration and Documentation:</strong> Register the fully functional package within the Julia ecosystem, accompanied by comprehensive documentation and usage examples to foster adoption and contribution from the community.</p>
</li>
</ol>
<h3 id="project_5_enhancing_promptingtoolsjl_with_advanced_schema_support_and_functionality"><a href="#project_5_enhancing_promptingtoolsjl_with_advanced_schema_support_and_functionality" class="header-anchor">Project 5: Enhancing PromptingTools.jl with Advanced Schema Support and Functionality</a></h3>
<p><strong>Project Overview:</strong></p>
<p>PromptingTools.jl, a key tool in the Julia GenAI ecosystem. This project is a concerted effort to broaden the utility and applicability of PromptingTools.jl by incorporating a wider array of prompt templates and schemas, thereby catering to a diverse set of LLM backends. The initiative directly corresponds to <a href="https://github.com/svilupp/PromptingTools.jl/issues/67">Issue #67</a>, <a href="https://github.com/svilupp/PromptingTools.jl/issues/68">Issue #68</a> and <a href="https://github.com/svilupp/PromptingTools.jl/issues/69">Issue #69</a> on PromptingTools.jl&#39;s GitHub. By enhancing the library&#39;s functionality to support structured extraction with the Ollama backend and introducing more standardized prompt schemas, we aim to make PromptingTools.jl an even more powerful and indispensable resource for developers engaging with open-source Large Language Models &#40;LLMs&#41;.</p>
<p><strong>Mentor:</strong> <a href="https://github.com/svilupp">Jan Siml</a> / <code>@svilup</code> on JuliaLang Slack / <code>Jan Siml</code> on Julia Zulip</p>
<p><strong>Project Difficulty:</strong> Medium</p>
<p><strong>Estimated Duration:</strong> 175 hours</p>
<p><strong>Ideal Candidate Profile:</strong></p>
<ul>
<li><p>Proficient in Julia with a commitment to the advancement of the Julia AI ecosystem.</p>
</li>
<li><p>Experience with open-source LLMs, such as llama.cpp and vLLM, and familiarity with prompt engineering concepts.</p>
</li>
</ul>
<p><strong>Project Goals and Deliverables:</strong></p>
<ol>
<li><p><strong>Schema Integration and Functionality Enhancement:</strong> Implement and integrate a variety of common prompt schemas &#40;see details in <a href="https://github.com/svilupp/PromptingTools.jl/issues/67">Issue #67</a>&#41;. Develop methods for the <code>render</code> and <code>aigenerate</code> functions that enable easy use and rendering of these templates, complete with comprehensive documentation, examples, and tests.</p>
</li>
<li><p><strong>Structured Extraction Support:</strong> Add <code>aiextract</code> support for the Ollama backend, as currently, this functionality is not supported. This involves creating methods and templates that facilitate structured data extraction, thereby broadening the use cases and efficiency of interacting with AI models through PromptingTools.jl. See details in <a href="https://github.com/svilupp/PromptingTools.jl/issues/68">Issue #68</a>. Extending the functionality to other backends is a plus.</p>
</li>
<li><p><strong>Support for Common Backends:</strong> Extend the functionality of PromptingTools.jl to support common backends such as HuggingFace Transformers and vLLM, ensuring that the library is compatible with a wide range of LLMs. We need to create an example for each backend to demonstrate the functionality. See details in <a href="https://github.com/svilupp/PromptingTools.jl/issues/69">Issue #69</a></p>
</li>
</ol>
<h3 id="project_6_expanding_the_julia_large_language_model_leaderboard"><a href="#project_6_expanding_the_julia_large_language_model_leaderboard" class="header-anchor">Project 6: Expanding the Julia Large Language Model Leaderboard</a></h3>
<p><strong>Project Overview:</strong></p>
<p>As a pivotal resource for the Julia community, the <a href="https://github.com/svilupp/Julia-LLM-Leaderboard">Julia LLM Leaderboard</a> benchmarks open-source models for Julia code generation. This enhancement project seeks to extend the leaderboard by incorporating additional test cases and expanding benchmarks into Julia-specific applications beyond code generation, such as evaluating Retrieval-Augmented Generation &#40;RAG&#41; applications with a golden Q&amp;A dataset and many others. This initiative, addressing several GitHub issues, aims to improve the leaderboard&#39;s utility and accuracy, making it an even more indispensable tool for the community. Participants will have the chance to deepen their knowledge of Generative AI while contributing to a project that enhances how the Julia community selects the most effective AI models for their needs.</p>
<p><strong>Mentor:</strong> <a href="https://github.com/svilupp">Jan Siml</a> / <code>@svilup</code> on JuliaLang Slack / <code>Jan Siml</code> on Julia Zulip</p>
<p><strong>Project Difficulty:</strong> Easy/Medium</p>
<p><strong>Estimated Duration:</strong> 175 hours</p>
<p><strong>Ideal Candidate Profile:</strong></p>
<ul>
<li><p>Strong proficiency in Julia and an active participant in the Julia community.</p>
</li>
<li><p>Basic knowledge or interest in Generative AI, with a keenness to learn more through practical application.</p>
</li>
<li><p>A passion for contributing to open-source projects and a desire to help the Julia community identify the most effective AI models for their needs.</p>
</li>
</ul>
<p><strong>Project Goals and Deliverables:</strong></p>
<ol>
<li><p><strong>Test Case Expansion:</strong> Develop and integrate a diverse range of test cases to assess the capabilities of LLMs in Julia code generation more comprehensively, enhancing the leaderboard&#39;s robustness and reliability. See the details <a href="https://github.com/svilupp/Julia-LLM-Leaderboard/issues/5">here</a>.</p>
</li>
<li><p><strong>Benchmark Extension:</strong> Extend the leaderboard&#39;s benchmarking capabilities to include evaluations of RAG applications &#40;question-answering systems&#41;, focusing on their knowledge of the Julia programming language, and other Julia tasks like &quot;help me speed up this code&quot;, &quot;what is a more idiomatic way to write this&quot;, etc. There is a slight overlap with Project 3, however, the focus here is to pinpoint promising locally-hosted models with strong capabilities all around. See the details <a href="https://github.com/svilupp/Julia-LLM-Leaderboard/issues/6">here</a>.</p>
</li>
<li><p><strong>Documentation and Outreach:</strong> Document findings and best practices in a series of blog posts to share insights, highlight top-performing models, and guide the community on leveraging Generative AI effectively within the Julia ecosystem.</p>
</li>
</ol>
<h3 id="project_7_counterfactuals_for_llms_model_explainability_and_generative_ai"><a href="#project_7_counterfactuals_for_llms_model_explainability_and_generative_ai" class="header-anchor">Project 7: Counterfactuals for LLMs &#40;<em>Model Explainability</em> and <em>Generative AI</em>&#41;</a></h3>
<p><strong>Project Overview:</strong> This project aims to extend the functionality of <a href="https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl">CounterfactualExplanations.jl</a> to Large Language Models &#40;LLMs&#41;. As a backbone for this, support for computing feature attributions for LLMs will also need to be implemented. The project will contribute to both <a href="https://github.com/JuliaTrustworthyAI">Taija</a> and <a href="https://github.com/JuliaGenAI">JuliaGenAI</a>. </p>
<p><strong>Mentor:</strong> <a href="https://github.com/svilupp">Jan Siml</a> &#40;JuliaGenAI&#41; and <a href="https://github.com/pat-alt">Patrick Altmeyer</a> &#40;Taija&#41;</p>
<p><strong>Project Difficulty</strong>: Medium</p>
<p><strong>Estimated Duration</strong>: 175 hours</p>
<p><strong>Ideal Candidate Profile:</strong></p>
<ul>
<li><p>Experience with Julia and multiple dispatch of advantage, but not crucial</p>
</li>
<li><p>Good knowledge of machine learning and statistics</p>
</li>
<li><p>Good understanding of Large Language Models &#40;LLMs&#41;</p>
</li>
<li><p>Ideally previous experience with <a href="https://github.com/chengchingwen/Transformers.jl">Transformers.jl</a></p>
</li>
</ul>
<p><strong>Project Goals and Deliverables:</strong></p>
<ul>
<li><p>Carefully think about architecture choices: how can we fit support for LLMs into the existing code base of <a href="https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl">CounterfactualExplanations.jl</a>?</p>
</li>
<li><p>Implement current state-of-the-art approaches such as <a href="https://aclanthology.org/2021.findings-acl.336.pdf">MiCE</a> and <a href="https://aclanthology.org/2022.findings-emnlp.216.pdf">CORE</a></p>
</li>
<li><p>Comprehensively test and document your work</p>
</li>
</ul>
<h2 id="how_to_contact_us"><a href="#how_to_contact_us" class="header-anchor">How to Contact Us</a></h2>
<p>We&#39;d love to hear your ideas and discuss potential projects with you.</p>
<p>Probably the easiest way is to join our <a href="https://julialang.org/slack/">JuliaLang Slack</a> and join the <code>#generative-ai</code> channel.  You can also reach out to us on <a href="https://julialang.zulipchat.com/#narrow/stream/423470-generative-ai">Julia Zulip</a> or post a GitHub Issue on our website <a href="https://github.com/JuliaGenAI/juliagenai.org">JuliaGenAI</a>.</p>
<h1 id="juliahealth_projects"><a href="#juliahealth_projects" class="header-anchor">JuliaHealth Projects </a></h1>
<p>JuliaHealth is an organization dedicated to improving healthcare by promoting open-source technologies and data standards. Our community is made up of researchers, data scientists, software developers, and healthcare professionals who are passionate about using technology to improve patient outcomes and promote data-driven decision-making. We believe that by working together and sharing our knowledge and expertise, we can create powerful tools and solutions that have the potential to transform healthcare.</p>
<h2 id="observational_health_subecosystem_projects"><a href="#observational_health_subecosystem_projects" class="header-anchor">Observational Health Subecosystem Projects</a></h2>
<h3 id="project_1_developing_tooling_for_observational_health_research_in_julia"><a href="#project_1_developing_tooling_for_observational_health_research_in_julia" class="header-anchor">Project 1: Developing Tooling for Observational Health Research in Julia</a></h3>
<p><strong>Description:</strong> The OMOP Common Data Model &#40;OMOP CDM&#41; is a widely used data standard that allows researchers to analyze large, heterogeneous healthcare datasets in a consistent and efficient manner. JuliaHealth has several packages that can interact with databases that adhere to the OMOP CDM &#40;such as OMOPCDMCohortCreator.jl or OMOPCDMDatabaseConnector.jl&#41;. For this project, we are looking for students interested in further developing the tooling in Julia to interact with OMOP CDM databases. </p>
<ul>
<li><p><strong>Mentor:</strong> Jacob Zelko &#40;aka TheCedarPrince&#41; &#91;email: jacobszelko@gmail.com&#93;</p>
</li>
<li><p><strong>Difficulty</strong>: Medium</p>
</li>
<li><p><strong>Duration</strong>: 350 hours</p>
</li>
<li><p><strong>Suggested Skills and Background</strong>: </p>
<ul>
<li><p>Experience with Julia</p>
</li>
<li><p>Familiarity with some of the following Julia packages would be a strong asset: </p>
<ul>
<li><p>FunSQL.jl</p>
</li>
<li><p>DataFrames.jl </p>
</li>
<li><p>Distributed.jl </p>
</li>
<li><p>OMOPCDMCohortCreator.jl </p>
</li>
<li><p>OMOPCDMDatabaseConnector.jl </p>
</li>
<li><p>OMOPCommonDataModel.jl</p>
</li>
</ul>
</li>
<li><p>Comfort with the OMOP Common Data Model &#40;or a willingness to learn&#33;&#41;</p>
</li>
</ul>
</li>
<li><p><strong>Potential Outcomes:</strong> </p>
</li>
</ul>
<p>Some potential project outcomes could be:</p>
<ul>
<li><p>Expanding OMOPCDMCohortCreator.jl to enable users to add constraints to potential patient populations they want to create such as conditional date ranges for a given drug or disease diagnosis.</p>
</li>
<li><p>Support parallelization of OMOPCDMCohortCreator.jl based queries when developing a patient population. </p>
</li>
<li><p>Develop and explore novel ways for how population filters within OMOPCDMCohortCreator.jl can be composed together for rapid analysis.</p>
</li>
</ul>
<p>In whatever functionality that gets developed for tools within JuliaHealth, it will also be expected for students to contribute to the existing package documentation to highlight how new features can be used. Although not required, if students would like to submit a lightning talks, posters, etc. to JuliaCon in the future about their work, they will be supported in this endeavor&#33;</p>
<p>Please contact the mentor for this project if interested and want to discuss what else could be pursued in the course of this project.</p>
<h3 id="project_2_developing_patient_level_prediction_tooling_within_julia"><a href="#project_2_developing_patient_level_prediction_tooling_within_julia" class="header-anchor">Project 2: Developing Patient Level Prediction Tooling within Julia</a></h3>
<p><strong>Description:</strong> Patient level prediction &#40;PLP&#41; is an important area of research in healthcare that involves using patient data to predict outcomes such as disease progression, response to treatment, and hospital readmissions. JuliaHealth is interested in developing tooling for PLP that utilizes historical patient data, such as patient medical claims or electronic health records, that follow the OMOP Common Data Model &#40;OMOP CDM&#41;, a widely used data standard that allows researchers to analyze large, heterogeneous healthcare datasets in a consistent and efficient manner. For this project, we are looking for students interested in developing PLP tooling within Julia.</p>
<ul>
<li><p><strong>Mentor:</strong> Sebastian Vollmer &#91;email: sjvollmer@gmail.com&#93;, Jacob Zelko &#40;aka TheCedarPrince&#41; &#91;email: jacobszelko@gmail.com&#93;</p>
</li>
<li><p><strong>Difficulty</strong>: Hard</p>
</li>
<li><p><strong>Duration</strong>: 350 hours</p>
</li>
<li><p><strong>Suggested Skills and Background</strong>: </p>
<ul>
<li><p>Experience with Julia</p>
</li>
<li><p>Exposure to machine learning concepts and ideas</p>
</li>
<li><p>Familiarity with some of the following Julia packages would be a strong asset: </p>
<ul>
<li><p>DataFrames.jl </p>
</li>
<li><p>OMOPCDMCohortCreator.jl </p>
</li>
<li><p>MLJ.jl </p>
</li>
<li><p>ModelingToolkit.jl </p>
</li>
</ul>
</li>
<li><p>Comfort with the OMOP Common Data Model &#40;or a willingness to learn&#41;</p>
</li>
</ul>
</li>
<li><p><strong>Outcomes:</strong> </p>
</li>
</ul>
<p>This project will be very experimental and exploratory in nature. To constrain the expectations for this project, here is a possible approach students will follow while working on this project:</p>
<ul>
<li><p>Review existing literature on approaches to PLP</p>
</li>
<li><p>Familiarize oneself with tools for machine learning and prediction within the Julia ecosystem</p>
</li>
<li><p>Determine PLP research question to drive package development </p>
</li>
<li><p>Develop PLP package utilizing JuliaHealth tools to work with an OMOP CDM database </p>
</li>
<li><p>Test and validate PLP package for investigating the research question </p>
</li>
<li><p>Document findings and draft JuliaCon talk</p>
</li>
</ul>
<p>In whatever functionality that gets developed for tools within JuliaHealth, it will also be expected for students to contribute to the existing package documentation to highlight how new features can be used. For this project, it will be expected as part of the proposal to pursue drafting and giving a talk at JuliaCon.  Furthermore, although not required, publishing in the JuliaCon Proceedings will both be encouraged and supported by project mentors. </p>
<p>Additionally, depending on the success of the package, there is a potential to run experiments on actual patient data to generate actual patient population insights based on a chosen research question.  This could possibly turn into a separate research paper, conference submission, or poster submission.  Whatever may occur in this situation will be supported by project mentors. </p>
<h2 id="medical_imaging_subecosystem_projects"><a href="#medical_imaging_subecosystem_projects" class="header-anchor">Medical Imaging Subecosystem Projects</a></h2>
<p><a href="https://github.com/JuliaHealth/MedPipe3D.jl">MedPipe3D.jl</a> together with <a href="https://github.com/JuliaHealth/MedEye3d.jl">MedEye3D.jl</a> <a href="https://github.com/JuliaHealth/MedEval3D.jl">MedEval3D.jl</a> and currently in development <a href="https://github.com/JuliaHealth/MedImage.jl">MedImage.jl</a> is a set of libraries created to provide essential tools for 3D medical imaging to the Julia language ecosystem. </p>
<p>MedImage is a package for the standardization of loading medical imaging data, and for its basic processing that takes into consideration its spatial metadata. MedEye3D is a package that supports the display of medical imaging data.  MedEval3D has implemented some highly performant algorithms for calculating metrics needed to asses the performance of 3d segmentation models.  MedPipe3D was created as a package that improves integration between other parts of the small ecosystem &#40;MedEye3D, MedEval3D, and MedImage&#41;. </p>
<h3 id="project_3_adding_functionalities_to_medical_imaging_visualizations"><a href="#project_3_adding_functionalities_to_medical_imaging_visualizations" class="header-anchor">Project 3: Adding functionalities to medical imaging visualizations</a></h3>
<p><strong>Description:</strong>  MedEye3D is a package that supports the display of medical imaging data. It includes multiple functionalities specific to this use case like automatic windowing to display soft tissues, lungs, and other tissues. The display takes into account voxel spacing, support of overlaying display for multimodal imaging, and more. All with high performance powered by OpenGL and Rocket.jl. Still, a lot of further improvements are possible and are described in the Potential Outcomes section. </p>
<ul>
<li><p><strong>Mentor:</strong> Jakub Mitura &#91;email: jakub.mitura14@gmail.com&#93;</p>
</li>
<li><p><strong>Difficulty</strong>: Hard</p>
</li>
<li><p><strong>Duration</strong>: 350 hours</p>
</li>
<li><p><strong>Suggested Skills and Background</strong>: </p>
<ul>
<li><p>Experience with Julia</p>
</li>
<li><p>Basic familiarity with computer graphics preferably OpenGL example <a href="https://www.opengl-tutorial.org/beginners-tutorials/">link</a></p>
</li>
<li><p>Some experience with 3d volumetric data with spatial metadata &#40;or a willingness to learn&#33;&#41; look into for example <a href="https://simpleitk.readthedocs.io/en/master/fundamentalConcepts.html">link</a></p>
</li>
</ul>
</li>
<li><p><strong>Potential Outcomes:</strong> </p>
</li>
</ul>
<p>Although MedEye3D already supports displaying medical images, there are still some functionalities that will be useful for the implementation of some more advanced algorithms, like supervoxel segmentation or image registration &#40;and both of them are crucial for solving a lot of important problems in medical imaging&#41;. To achieve this this project&#39;s goal is to implement.</p>
<ol>
<li><p>Developing support for multiple image viewing with indicators for image registration like display of the borders, and display lines connecting points.</p>
</li>
<li><p>Automatic correct windowing for MRI and PET.</p>
</li>
<li><p>Support of display for supervoxels &#40;sv&#41;. Show borders of sv; indicate whether the gradient of the image is in agreement with sv borders.</p>
</li>
<li><p>Improve start time.</p>
</li>
<li><p>Simplify basic usage by providing high-level functions.</p>
</li>
</ol>
<ul>
<li><p><strong>Success criteria and time needed:</strong> How the success of functionality described above is defined and the approximate time required for each.</p>
</li>
</ul>
<ol>
<li><p>The user can load 2 different images, and they would display concurrently one next to the other. During scrolling the same area of the body should be displayed &#40;for well-registered sample images&#41; based on the supplied metadata. While moving the mouse cursor on one image the position of the cursor in the same physical spot on the other image should be displayed &#40;physical location calculated from spatial metadata&#41;.  120h</p>
</li>
<li><p>Given the most common PET and MRI modalities &#40;random FDG PET/CT, and T2, T1, FLAIR, ADC, DWI on MRI&#41; - the user will see the image similar to what is automatically displayed in 3DSlicer - 10h</p>
</li>
<li><p>Given an integer mask where a unique integer value will encode information about a single supervoxel and an underlying 3d medical image user will have the option to overlay the original image with the borders of the superpixels where adjacent borders will have different colors, or show those borders on the background of the image convolved with edge filter, for example, Sobel filter - 180h</p>
</li>
<li><p>Any measurable decrease in the start time of the viewer -   20h</p>
</li>
<li><p>The user will be able to display images just by supplying MedImage objects from the MedImage.jl library to a single display function -  20h</p>
</li>
</ol>
<h3 id="project_4_adding_dataset-wide_functions_and_integrations_of_augmentations"><a href="#project_4_adding_dataset-wide_functions_and_integrations_of_augmentations" class="header-anchor">Project 4: Adding dataset-wide functions and integrations of augmentations</a></h3>
<p><strong>Description:</strong>  MedPipe3D was created as a package that improves integration between other parts of the small ecosystem &#40;MedEye3D, MedEval3D, and MedImage&#41;. Currently, it needs to be expanded and adapted so it can be a basis for a fully functional medical imaging pipeline. It requires utilities for preprocessing specific to medical imaging - like uniformization of spacing, orientation, cropping, or padding. It needs to k fold cross validation and simple ensembling. Other necessary part of the segmentation pipeline are the augmentations that should be easier to use, and provide test time augmentation for uncertainty quantification. The last thing in the pipeline that is also important for practitioners is postprocessing - and the most popular postprocessing is finding and keeping only the largest connected component.</p>
<ul>
<li><p><strong>Mentor:</strong> Jakub Mitura &#91;email: jakub.mitura14@gmail.com&#93;</p>
</li>
<li><p><strong>Difficulty</strong>: Medium</p>
</li>
<li><p><strong>Duration</strong>: 350 hours</p>
</li>
<li><p><strong>Suggested Skills and Background</strong>: </p>
<ul>
<li><p>Experience with Julia</p>
</li>
<li><p>Familiarity with some of the following Julia packages would be a strong asset: </p>
<ul>
<li><p>MedEye3D.jl </p>
</li>
<li><p>MedEval3D.jl</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>Potential Outcomes:</strong> </p>
</li>
</ul>
<ol>
<li><p>Integrate augmentations like rotations recalling gamma etc.</p>
</li>
<li><p>Enable invertible augmentations and support test time augmentations.</p>
</li>
<li><p>Add patch-based data loading with probabilistic oversampling.</p>
</li>
<li><p>Calculate median and mean spacing and enable applying resampling to the median or mean spacing of the dataset.</p>
</li>
<li><p>Add basic post-processing like the largest connected component analysis.</p>
</li>
<li><p>Set all hyperparameters &#40;of augmentation; size of a patch, threshold for getting binary mask from probabilities&#41; in a struct or dictionary to enable hyperparameter tuning.</p>
</li>
<li><p>Enable automated display of the algorithm output in the validation epoch, including saving such outputs to persistent storage.</p>
</li>
<li><p>Support k-fold cross-validation.</p>
</li>
</ol>
<p>This set of changes although time-consuming to implement should not pose a significant issue to anybody with experience with the Julia programming language. However, implementing those will be a huge step in making Julia language a good alternative to Python in developing end-to-end medical imaging segmentation algorithms.</p>
<ul>
<li><p><strong>Success criteria and time needed:</strong> How the success of functionality described above is defined and the approximate time required for each.</p>
</li>
</ul>
<ol>
<li><p>Given the configuration struct supplied by the user the supplied augmentations will be executed with some defined probability after loading the image: Brightness transform, Contrast augmentation transform, Gamma Transform, Gaussian noise transform, Rician noise transform, Mirror transform, Scale transform, Gaussian blur transform, Simulate low-resolution transform, Elastic deformation transform -100h. </p>
</li>
<li><p>Enable some transformation to be executed on the model input, then inverse this transform on the model output; execute model inference n times when n is supplied by the user and return mean and standard deviation of segmentation masks produced by the model as the output -60h.</p>
</li>
<li><p>given the size of the 3D patch by the user algorithm after data loading will crop or pad the supplied image to meet the set size criterion. The part of the image where the label is present should be selected more frequently than the areas without during cropping, the probability that the area with some label indicated on segmentation mas will be chosen will equal p &#40;0-1&#41; where p is supplied by the user -40h. </p>
</li>
<li><p>given the list of paths to medical images it will load them calculate the mean or median spacing &#40;option selected by the user&#41;, and return it. Then during pipeline execution, all images should be resampled to a user-supplied spacing and user-supplied orientation - 40h.</p>
</li>
<li><p>Given a model output and a threshold that will be used for each channel of the output to binarize the output user will have an option to retrieve only n largest components from binarized algorithm output - 20h.</p>
</li>
<li><p>Probabilities and hyperparameters of all augmentations, thresholds for binarization of output channels chosen spacing for preprocessing, number and settings of test time augmentations should be available in a hyperparam struct that is the additional argument of the pipeline function and that can be used for hyperparameter tuning -30h.</p>
</li>
<li><p>During the validation epoch the images can be saved into persistent storage and a single random image loaded together with the output mask into MedEye3d for visualization during training -30h.</p>
</li>
<li><p>The user can set either val_percentage - which will lead to the division of the dataset to training and validation fold or supply k which will lead to k-fold cross-validation. In the latter option mean, threshold, and standard deviation of the ensemble will be returned as the final output of the model -30h.</p>
</li>
</ol>
<p>For each point mentor will also supply the person responsible for implementation with examples of required functionalities in Python or will point to the Julia libraries already implementing it &#40;that just need to be integrated&#41;.</p>
<h3 id="project_5_highly-efficient_mri_simulations_with_multi-vendor_gpu_support"><a href="#project_5_highly-efficient_mri_simulations_with_multi-vendor_gpu_support" class="header-anchor">Project 5: Highly-efficient MRI Simulations with Multi-Vendor GPU Support</a></h3>
<p><strong>Description:</strong>  KomaMRI.jl is a Julia package designed for highly-efficient Magnetic Resonance Imaging &#40;MRI&#41; simulations, serving both educational and research purposes. Simulations can help to grasp hard-to-understand MRI concepts, like pulse sequences, signal generation and acquisition. Moreover, they can guide the design of novel pulse sequences, and generate synthetic data for training machine learning models.</p>
<p>Currently, our simulator performs GPU-accelerated computations using CUDA arrays. We are now advancing to implement a new simulation method &#40;<code>BlochKernel&lt;:SimulationMethod</code>&#41; based on GPU kernel programming using KernelAbstractions.jl. This enhancement will not only boost computation speeds but also broaden accessibility with KernelAbstractions.jl&#39;s multi-vendor GPU support. This could enable the use of MRI simulations in iterative algorithms to solve inverse problems. We are seeking enthusiastic people interested in developing this functionality.</p>
<ul>
<li><p><strong>Mentors:</strong> Carlos Castillo &#91;email: cncastillo@uc.cl&#93;, Boris Oróstica &#91;email: beorostica@uc.cl&#93;, Pablo Irarrazaval &#91;email: pim@uc.cl&#93;</p>
</li>
<li><p><strong>Difficulty:</strong> Hard</p>
</li>
<li><p><strong>Duration:</strong> 350 hours &#40;2 months, 8 hours per day&#41;</p>
</li>
<li><p><strong>Suggested Skills and Background:</strong></p>
<ul>
<li><p>Experience with Julia</p>
</li>
<li><p>Exposure to MRI concepts and ideas</p>
</li>
<li><p>High-level knowledge of GPU programming</p>
</li>
<li><p>Familiarity with some of the following Julia packages would be desired:</p>
<ul>
<li><p>KernelAbstractions.jl</p>
</li>
<li><p>CUDA.jl</p>
</li>
<li><p>Adapt.jl</p>
</li>
<li><p>Functors.jl</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Outcomes</strong>:</p>
</li>
</ul>
<p>We expect the following outcomes by the end of this program:</p>
<ol>
<li><p>Extended and/or improved GPU-accelerated simulations, having generated a new simulation method <code>BlochKernel</code> with multi-vendor GPU support.</p>
</li>
<li><p>Developed documentation explaining the new simulation method, including showcasing some use-case examples.</p>
</li>
<li><p>Implemented automatic pipelines on Buildkite for testing the simulations across multiple GPU architectures.</p>
</li>
<li><p>Reported performance improvements between <code>BlochKernel</code> and <code>Bloch</code>.</p>
</li>
</ol>
<p>Please contact the mentors of this project if you are interested and want to discuss other aspects that could be pursued during the course of this project.</p>
<h1 id="music_data_analysis_-_summer_of_code"><a href="#music_data_analysis_-_summer_of_code" class="header-anchor">Music data analysis - Summer of Code</a></h1>
<p><a href="https://github.com/JuliaMusic">JuliaMusic</a> is an organization providing packages and functionalities that allow analyzing the properties of music performances. </p>
<h2 id="midification_of_music_from_wave_files"><a href="#midification_of_music_from_wave_files" class="header-anchor">MIDIfication of music from wave files</a></h2>
<p><strong>Difficulty</strong>: Medium.</p>
<p><strong>Length</strong>: 350 hours.</p>
<p>It is easy to analyze timing and intensity fluctuations in music that is the form of MIDI data.  This format is already digitalized, and packages such as MIDI.jl and MusicManipulations.jl allow for seamless data processing. But arguably the most interesting kind of music to analyze is the live one. Live music performances are recorded in wave formats.  Some algorithms exist that can detect the &quot;onsets&quot; of music hits,  but they are typically focused only on the timing information and hence forfeit detecting e.g., the intensity of the played note. Plus, there are very few code implementations online for this problem, almost all of which are old and unmaintained. We would like to implement an algorithm in MusicProcessing.jl that given a recording of a single instrument, it can &quot;MIDIfy&quot; it, which means to digitalize it into the MIDI format.</p>
<p><strong>Recommended Skills</strong>: Background in music, familiarity with digital signal processing.</p>
<p><strong>Expected results</strong>: A well-tested, well-documented function <code>midify</code> in MusicProcessing.jl.</p>
<p><strong>Mentors</strong>: <a href="https://github.com/Datseris/">George Datseris</a>.</p>
<h1 id="juliareach_-_summer_of_code"><a href="#juliareach_-_summer_of_code" class="header-anchor">JuliaReach - Summer of Code</a></h1>
<p><a href="https://github.com/JuliaReach">JuliaReach</a> is the Julia ecosystem for reachability computations of dynamical systems. Application domains of set-based reachability include formal verification, controller synthesis and estimation under uncertain model parameters or inputs. For further context reach us on the <a href="https://julialang.zulipchat.com/#narrow/stream/278609-juliareach">JuliaReach zulip</a> stream. You may also refer to the review article <a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-control-071420-081941">Set Propagation Techniques for Reachability Analysis</a>.</p>
<h2 id="efficient_symbolic-numeric_set_computations"><a href="#efficient_symbolic-numeric_set_computations" class="header-anchor">Efficient symbolic-numeric set computations</a></h2>
<p><strong>Difficulty</strong>: Medium.</p>
<p><strong>Description.</strong> <a href="https://github.com/JuliaReach/LazySets.jl">LazySets</a> is the core library of JuliaReach. It provides ways to symbolically compute with geometric sets, with a focus on lazy set representations and efficient high-dimensional processing. The library has been described in the article <a href="https://proceedings.juliacon.org/papers/10.21105/jcon.00097">LazySets.jl: Scalable Symbolic-Numeric Set Computations</a>.</p>
<p>The main interest in this project is to implement algorithms that leverage the structure of the sets. Typical examples include polytopes and zonotopes &#40;convex&#41;, polynomial zonotopes and Taylor models &#40;non-convex&#41; to name a few.</p>
<p><strong>Expected Results.</strong> The goal is to implement certain efficient state-of-the-art algorithms from the literature. The code is to be documented, tested, and evaluated in benchmarks. Specific tasks may include &#40;to be driven by the interets of the candidate&#41;: efficient vertex enumeration of <a href="https://juliareach.github.io/LazySets.jl/dev/lib/sets/Zonotope/#LazySets.Zonotope">zonotopes</a>; operations on polynomial zonotopes; operations on <a href="http://archive.www6.in.tum.de/www6/Main/Publications/Althoff2011f.pdf">zonotope bundles</a>; efficient disjointness checks between different set types; <a href="https://ieeexplore.ieee.org/document/7525593">complex zonotopes</a>.</p>
<p><strong>Expected Length.</strong> 175 hours.</p>
<p><strong>Recommended Skills.</strong> Familiarity with Julia and Git/GitHub is mandatory. Familiarity with <a href="https://github.com/JuliaReach/LazySets.jl">LazySets</a> is recommended. Basic knowledge of geometric terminology is appreciated but not required.</p>
<p><strong>Mentors</strong>: <a href="https://github.com/mforets">Marcelo Forets</a>, <a href="https://github.com/schillic">Christian Schilling</a>.</p>
<h2 id="reachability_with_sparse_polynomial_zonotopes"><a href="#reachability_with_sparse_polynomial_zonotopes" class="header-anchor">Reachability with sparse polynomial zonotopes</a></h2>
<p><strong>Difficulty</strong>: Medium.</p>
<p><strong>Description.</strong> Sparse polynomial zonotopes are a new non-convex set representation that are well-suited for reachability analysis of nonlinear dynamical systems. This project is a continuation of <a href="https://summerofcode.withgoogle.com/archive/2022/projects/feZrZfQX">GSoC&#39;2022 - Reachability with sparse polynomial zonotopes</a>, which implemented the basics in <a href="https://github.com/JuliaReach/LazySets.jl">LazySets</a>.</p>
<p><strong>Expected Results.</strong> It is expected to add efficient Julia implementations of a reachability algorithm for dynamical systems in <a href="https://github.com/JuliaReach/ReachabilityAnalysis.jl">ReachabilityAnalysis</a> which leverages polynomial zonotopes. A successful project should:</p>
<ul>
<li><p>Replicate the results from the article &#91;Reachability Analysis for Linear Systems with Uncertain Parameters using Polynomial Zonotopes</p>
</li>
</ul>
<p>&#93;&#40;https://dl.acm.org/doi/abs/10.1145/3575870.3587130&#41;.</p>
<ul>
<li><p>The code shall be documented, tested, and evaluated extensively in benchmarks.</p>
</li>
</ul>
<p>For ambitious candidates it is possible to draw connections with neural-network control systems as implemented in <a href="https://github.com/JuliaReach/ClosedLoopReachability.jl">ClosedLoopReachability.jl</a>.</p>
<p><strong>Expected Length.</strong> 175 hours.</p>
<p><strong>Recommended Skills.</strong> Familiarity with Julia and Git/GitHub is mandatory. Familiarity with the mentioned Julia packages is appreciated but not required. The project does not require theoretical contributions, but it requires reading a research literature, hence a certain level of academic experience is recommended.</p>
<p><strong>Literature and related packages.</strong> <a href="https://www.youtube.com/watch?v&#61;iMtq6YeIsjA">This video</a> explains the concept of polynomial zonotopes &#40;slides <a href="https://github.com/JuliaReach/juliareach-days-3-reachathon/blob/master/Challenge_5/Challenge5_PolynomialZonotopes.pdf">here</a>&#41;. The relevant theory is described in <a href="https://arxiv.org/pdf/1901.01780">this research article</a>. There exists a Matlab implementation in <a href="https://tumcps.github.io/CORA/">CORA</a> &#40;the implementation of polynomial zonotopes can be found in <a href="https://github.com/TUMcps/CORA/tree/master/contSet/&#37;40polyZonotope">this folder</a>&#41;.</p>
<p><strong>Mentors</strong>: <a href="https://github.com/mforets">Marcelo Forets</a>, <a href="https://github.com/schillic">Christian Schilling</a>.</p>
<h2 id="improving_the_hybrid_systems_reachability_api"><a href="#improving_the_hybrid_systems_reachability_api" class="header-anchor">Improving the hybrid systems reachability API</a></h2>
<p><strong>Difficulty</strong>: Medium.</p>
<p><strong>Description.</strong> <a href="https://github.com/JuliaReach/ReachabilityAnalysis.jl">ReachabilityAnalysis</a> is a Julia library for set propagation of dynamical systems. One of the main aims is to handle systems with mixed discrete-continuous behaviors &#40;known as hybrid systems in the literature&#41;. This project will focus on enhancing the capabilities of the library and overall improvement of the ecosystem for users.</p>
<p><strong>Expected Results.</strong>   Specific tasks may include: problem-specific heuristics for hybrid systems; API for time-varying input sets; flowpipe underapproximations. The code is to be documented, tested, and evaluated in benchmarks. Integration with <a href="https://github.com/SciML/ModelingToolkit.jl">ModelingToolkit.jl</a> can also be considered if there is interest.</p>
<p><strong>Expected Length.</strong> 175 hours.</p>
<p><strong>Recommended Skills.</strong> Familiarity with Julia and Git/GitHub is mandatory. Familiarity with <a href="https://github.com/JuliaReach/LazySets.jl">LazySets</a> and <a href="https://github.com/JuliaReach/ReachabilityAnalysis.jl">ReachabilityAnalysis</a> is welcome but not required.</p>
<p><strong>Mentors</strong>: <a href="https://github.com/mforets">Marcelo Forets</a>, <a href="https://github.com/schillic">Christian Schilling</a>.</p>
<h1 id="juliastats_projects_summer_of_code"><a href="#juliastats_projects_summer_of_code" class="header-anchor">JuliaStats Projects – Summer of Code</a></h1>
<p><a href="https://github.com/JuliaStats">JuliaStats</a> is an organization dedicated to providing high-quality packages for statistics in Julia.</p>
<h2 id="panel_data_analysis"><a href="#panel_data_analysis" class="header-anchor">Panel data analysis</a></h2>
<p>Implement panel analysis models and estimators in Julia.</p>
<p><strong>Difficulty.</strong> Moderate. <strong>Duration.</strong> 350 hours</p>
<h3 id="description__6"><a href="#description__6" class="header-anchor">Description</a></h3>
<p>Panel data is an important kind of statistical data that deals with observations of multiple units across time. Common examples of panel data include economic statistics &#40;where it is common to observe figures for several countries over time&#41;. This combination of longitudinal  and cross-sectional data can be powerful for extracting causal structure from data.</p>
<p><strong>Mentors.</strong> <a href="https://github.com/nilshg">Nils Gudat</a>, <a href="https://github.com/Nosferican">José Bayoán Santiago Calderón</a>, <a href="https://github.com/ParadaCarleton/">Carlos Parada</a></p>
<h3 id="prerequisites__5"><a href="#prerequisites__5" class="header-anchor">Prerequisites</a></h3>
<ul>
<li><p>Must be fluent in at least one language for statistical computing, and    willing to learn Julia before the start of projects.</p>
</li>
<li><p>Knowledge of basic statistical inference covering topics such as maximum   likelihood estimation, confidence intervals, and hypothesis testing. &#40;Must   know before applying.&#41;</p>
</li>
<li><p>Basic familiarity with time series statistics &#40;e.g. ARIMA models, autocorrelations&#41;    or panel data. &#40;Can be learned after applying.&#41;</p>
</li>
</ul>
<h3 id="your_contribution__5"><a href="#your_contribution__5" class="header-anchor">Your contribution</a></h3>
<p>Participants will:</p>
<ul>
<li><p>Learn and build on past approaches and packages for panel data analysis,   such as those in <a href="https://github.com/Nosferican/Econometrics.jl">Econometrics.jl</a>    and <a href="https://github.com/nilshg/SynthControl.jl">SynthControl.jl</a>.</p>
</li>
<li><p>Generalize <a href="https://github.com/nilshg/TreatmentPanels.jl">TreatmentPanels.jl</a> into an abstract interface   for dealing with and manipulating panel data.</p>
</li>
<li><p>Integrate existing estimators provided by packages such as Econometrics.jl    into a single package for panel data estimation.</p>
</li>
</ul>
<h3 id="references__6"><a href="#references__6" class="header-anchor">References</a></h3>
<ul>
<li><p><a href="http://web.pdx.edu/~crkl/ec510/pda_yaffee.pdf">A Primer for Panel Data Analysis</a></p>
</li>
<li><p><a href="https://mitpress.mit.edu/books/econometric-analysis-cross-section-and-panel-data-second-edition">Econometric Analysis of Cross Section and Panel Data</a> by Jeffrey Wooldridge</p>
</li>
</ul>
<h2 id="distributionsjl_expansion"><a href="#distributionsjl_expansion" class="header-anchor">Distributions.jl Expansion</a></h2>
<p>Distributions.jl is a package providing basic probability distributions and associated functions. </p>
<p><strong>Difficulty.</strong> Easy-Medium. <strong>Duration.</strong> 175-350 hours</p>
<h3 id="prerequisites__6"><a href="#prerequisites__6" class="header-anchor">Prerequisites</a></h3>
<ul>
<li><p>Must be fluent in Julia.</p>
</li>
<li><p>A college-level introduction to probability covering topics such as   probability density functions, moments and cumulants, and multivariate   distributions.</p>
</li>
</ul>
<h3 id="your_contribution__6"><a href="#your_contribution__6" class="header-anchor">Your contribution</a></h3>
<p>Possible improvements to Distributions.jl include:</p>
<ul>
<li><p>New distribution families, such as elliptical distributions or   distributions of order statistics.</p>
</li>
<li><p>Additional parametrizations and keyword constructors for current    distributions.</p>
</li>
<li><p>Extended support for distributions of transformed variables.</p>
</li>
<li><p><a href="https://github.com/JuliaStats/Distributions.jl/issues/294">Replace RMath RNGs.</a></p>
</li>
</ul>
<h2 id="hypothesistestingjl_expansion"><a href="#hypothesistestingjl_expansion" class="header-anchor">HypothesisTesting.jl Expansion</a></h2>
<p>HypothesisTesting.jl is a package that implements a range of hypothesis tests.</p>
<p><strong>Difficulty.</strong> Medium. <strong>Duration.</strong> 350 hours</p>
<p><strong>Mentors.</strong> <a href="https://www.cmi.ac.in/~sourish/">Sourish Das</a>, <a href="https://github.com/mousum-github">Mousum Dutta</a></p>
<h3 id="prerequisites__7"><a href="#prerequisites__7" class="header-anchor">Prerequisites</a></h3>
<ul>
<li><p>Must be fluent in Julia.</p>
</li>
<li><p>A college-level introduction to probability covering topics such as   probability density functions, moments and cumulants, and multivariate   distributions.</p>
</li>
</ul>
<h3 id="your_contribution__7"><a href="#your_contribution__7" class="header-anchor">Your contribution</a></h3>
<p>Improvements to Distributions.jl include:</p>
<ul>
<li><p>Develop Breusch-Pagan test against heteroskedasticity</p>
</li>
<li><p>Develop Harvey-Collier Test for linearity</p>
</li>
<li><p>Develop Bartlet Rank Test for randomness</p>
</li>
<li><p>Develop an exact dynamic programming solution to Wilcoxon–Mann–Whitney &#40;WMW&#41; test </p>
</li>
</ul>
<h3 id="references__7"><a href="#references__7" class="header-anchor">References</a></h3>
<ul>
<li><p><a href="https://search.r-project.org/CRAN/refmans/lmtest/html/bptest.html">bptest in R</a></p>
</li>
<li><p><a href="https://cran.r-project.org/web/packages/randtests/randtests.pdf">randtests in R</a></p>
</li>
<li><p>Alexander Marx, etal. &#40;2016&#41; “Exact Dynamic Programing Solution of the Wilcoxon–Mann–Whitney Test”    Genomics Proteomics Bioinformatics, 14, 55-61</p>
</li>
</ul>
<h2 id="crraojl"><a href="#crraojl" class="header-anchor">CRRao.jl</a></h2>
<p>Implement consistent APIs for statistical modeling in Julia. </p>
<p><strong>Difficulty.</strong> Medium. <strong>Duration.</strong> 350 hours</p>
<h3 id="description__7"><a href="#description__7" class="header-anchor">Description</a></h3>
<p>Currently, the Julia statistics ecosystem is quite fragmented. There is  value in having a consistent API for a wide variety of statistical models.  The <a href="https://github.com/xKDR/CRRao.jl">CRRao.jl</a> package offers this design.</p>
<p><strong>Mentors.</strong> <a href="https://www.cmi.ac.in/~sourish/">Sourish Das</a>, <a href="https://xkdr.org/author/ayush-patnaik">Ayush Patnaik</a></p>
<h3 id="prerequisites__8"><a href="#prerequisites__8" class="header-anchor">Prerequisites</a></h3>
<ul>
<li><p>Must be fluent in Julia.</p>
</li>
<li><p>Basic statistical inference covering topics such as maximum   likelihood estimation, confidence intervals, and hypothesis testing.</p>
</li>
</ul>
<h3 id="your_contribution__8"><a href="#your_contribution__8" class="header-anchor">Your contribution</a></h3>
<p>Participants will:</p>
<ul>
<li><p>Help create, test, and document standard statistical APIs for Julia.</p>
</li>
<li><p>Integrate MixedModels.jl</p>
</li>
</ul>
<h2 id="juliastats_improvements"><a href="#juliastats_improvements" class="header-anchor">JuliaStats Improvements </a></h2>
<p>General improvements to JuliaStats packages, depending on the interests  of participants.</p>
<p><strong>Difficulty.</strong> Easy-Hard. <strong>Duration.</strong> 175-350 hours.</p>
<h3 id="description__8"><a href="#description__8" class="header-anchor">Description</a></h3>
<p>JuliaStats provides many of the most popular packages in Julia, including:</p>
<ul>
<li><p>StatsBase.jl for basic statistics &#40;e.g. weights, sample statistics,   moments&#41;. </p>
</li>
<li><p>MixedModels.jl for random and mixed-effects linear models. </p>
</li>
<li><p>GLM.jl for generalized linear models. </p>
</li>
</ul>
<p>All of these packages are critically important to the Julia statistics community, and all could be improved.</p>
<p><strong>Mentors.</strong> <a href="https://github.com/mousum-github">Mousum Dutta</a>, <a href="https://xkdr.org/author/ayush-patnaik">Ayush Patnaik</a>, <a href="https://github.com/paradacarleton">Carlos Parada</a></p>
<h3 id="prerequisites__9"><a href="#prerequisites__9" class="header-anchor">Prerequisites</a></h3>
<ul>
<li><p>Must be fluent in at least one language for statistical computing, and    willing to learn Julia before the start of projects.</p>
</li>
<li><p>Knowledge of basic statistical inference covering topics such as maximum   likelihood estimation, confidence intervals, and hypothesis testing.</p>
</li>
</ul>
<h3 id="your_contribution__9"><a href="#your_contribution__9" class="header-anchor">Your contribution</a></h3>
<p>Participants will:</p>
<ul>
<li><p>Make JuliaStats better&#33; This can include additional estimators,   new features, performance improvements, or anything else you&#39;re   interested in.</p>
</li>
<li><p>StatsBase.jl improvements could include support for cumulants,   L-moments, or additional estimators.</p>
</li>
<li><p>Improved nonparametric density estimators, e.g. those in R&#39;s   <a href="https://cran.r-project.org/web/packages/locfit/index.html">Locfit</a>.</p>
</li>
</ul>
<h2 id="surveyjl"><a href="#surveyjl" class="header-anchor">Survey.jl</a></h2>
<p>This package is used to study complex survey data. Examples of real-world surveys include official government surveys in areas like economics, health and agriculture; financial and commercial surveys. Social and behavioural scientists like political scientists, sociologists, psychologists, biologists and macroeconomists also analyse surveys in academic and theoretical settings. The prevalence of &quot;big&quot; survey datasets has exploded with the ease of administering surveys online. The project aims to use performance enhancements of Julia to create a fast package for modern &quot;large&quot; surveys. </p>
<p><strong>Difficulty.</strong> Easy-Hard. <strong>Duration.</strong> 175-350 hours</p>
<p><strong>Mentors.</strong> <a href="https://xkdr.org/author/ayush-patnaik">Ayush Patnaik</a></p>
<h3 id="prerequisites__10"><a href="#prerequisites__10" class="header-anchor">Prerequisites</a></h3>
<ul>
<li><p>Experience with at least one language for statistical computing &#40;Julia, R, Python, SAS, Stata etc&#41;, and    willing to learn Julia before the start of projects.</p>
</li>
<li><p>Knowledge of basic statistical and probability concepts, preferably covered from academic course&#40;s&#41;.</p>
</li>
<li><p>&#40;Bonus&#41; Any prior experience or coursework with survey analysis, using any software or tool.</p>
</li>
</ul>
<h3 id="your_contribution__10"><a href="#your_contribution__10" class="header-anchor">Your contribution</a></h3>
<p>The project can be tailored around the background and interests of participants and depending on ability, several standalone mini-projects can be created. Participants can potentially work on:</p>
<ul>
<li><p>Generalised variance estimation methods using taylor linearisation</p>
</li>
<li><p>Post-stratification, raking or calibration, GREG estimation and related methods.</p>
</li>
<li><p>Connect Survey.jl with <a href="https://github.com/nalimilan/FreqTables.jl">FreqTable.jl</a> for contingency table analysis, or to survival analysis, or a machine learning library.</p>
</li>
<li><p>Improve support for multistage and Probability Proportional to Size &#40;PPS&#41; sampling with or without replacement.</p>
</li>
<li><p>Association tests &#40;with contingency tables&#41;, Rao-Scott, likelihood ratio tests for glms, Cox models, loglinear models.</p>
</li>
<li><p>Handling missing data, imputation like <a href="https://stat.ethz.ch/CRAN/web/packages/mitools/index.html">mitools</a>.</p>
</li>
</ul>
<h3 id="references__8"><a href="#references__8" class="header-anchor">References</a></h3>
<ul>
<li><p><a href="https://github.com/xKDR/Survey.jl">Survey.jl</a> - see some issues, past PR&#39;s and milestone ideas</p>
</li>
<li><p>Julia discourse post asking for community suggestions <a href="https://discourse.julialang.org/t/suggestions-for-the-design-of-survey-jl/86381/2">here</a></p>
</li>
<li><p>JuliaCon Statistics Symposium <a href="https://youtu.be/RY7SSfyNl9o">clip</a> for Survey</p>
</li>
<li><p><a href="https://d-nb.info/969712979/04">Model Assisted Survey Sampling</a> - Sarndal, Swensson, Wretman &#40;1992&#41;</p>
</li>
<li><p><a href="https://r-survey.r-forge.r-project.org/svybook/">Complex Surveys: a guide to analysis using R</a></p>
</li>
<li><p><a href="https://r-survey.r-forge.r-project.org/survey/">Survey analysis in R</a> for high level topics than can be implemented for Julia</p>
</li>
</ul>
<h1 id="stochastic_differential_equations_and_continuous_time_signal_processing_summer_of_code"><a href="#stochastic_differential_equations_and_continuous_time_signal_processing_summer_of_code" class="header-anchor">Stochastic differential equations and continuous time signal processing – Summer of Code</a></h1>
<h2 id="smoothing_non-linear_continuous_time_systems"><a href="#smoothing_non-linear_continuous_time_systems" class="header-anchor">Smoothing non-linear continuous time systems</a></h2>
<p>The contributor implements a state of the art smoother for continuous-time systems with additive Gaussian noise. The system&#39;s dynamics can be described as an ordinary differential equation with locally additive Gaussian random fluctuations, in other words a stochastic ordinary differential equation.</p>
<p>Given a series of measurements observed over time, containing statistical noise and other inaccuracies, the task is to produce an estimate of the unknown trajectory of the system that led to the observations.</p>
<p><em>Linear</em> continuous-time systems are smoothed with the fixed-lag Kalman-Bucy smoother &#40;related to the <a href="https://en.wikipedia.org/wiki/Kalman_filter#Kalman–Bucy_filter">Kalman–Bucy_filter</a>&#41;. It relies on coupled ODEs describing how mean and covariance of the conditional distribution of the latent system state evolve over time. A versatile implementation in Julia is missing.</p>
<p><strong>Expected Results</strong>: Build efficient implementation of non-linear smoothing of continuous stochastic dynamical systems.</p>
<p><strong>Recommended Skills</strong>: Gaussian random variables, Bayes&#39; formula, Stochastic Differential Equations</p>
<p><strong>Mentors</strong>: <a href="https://github.com/mschauer">Moritz Schauer</a></p>
<p><strong>Rating</strong>: Hard, 350 hours</p>
<h1 id="machine_learning_projects_-_summer_of_code"><a href="#machine_learning_projects_-_summer_of_code" class="header-anchor">Machine Learning Projects - Summer of Code</a></h1>
<p><strong>Note: FluxML participates as a NumFOCUS sub-organization. Head to <a href="http://fluxml.ai/gsoc/">the FluxML GSoC page</a> for their idea list.</strong></p>
<h3 id="reinforcement_learning_environments"><a href="#reinforcement_learning_environments" class="header-anchor">Reinforcement Learning Environments</a></h3>
<p>Time: 175h</p>
<p>Develop a series of reinforcement learning environments, in the spirit of the <a href="https://gym.openai.com">OpenAI Gym</a>. Although we have wrappers for the gym available, it is hard to install &#40;due to the Python dependency&#41; and, since it&#39;s written in Python and C code, we can&#39;t do more interesting things with it &#40;such as differentiate through the environments&#41;.</p>
<h4 id="expected_outcome__2"><a href="#expected_outcome__2" class="header-anchor">Expected outcome</a></h4>
<p>A pure-Julia version of selected environments that supports a similar API and visualisation options would be valuable to anyone doing RL with Flux.</p>
<p>Mentors: <a href="https://github.com/DhairyaLGandhi/">Dhairya Gandhi</a>.</p>
<h3 id="alphazerojl"><a href="#alphazerojl" class="header-anchor">AlphaZero.jl</a></h3>
<p>The philosophy of the <a href="https://github.com/jonathan-laurent/AlphaZero.jl">AlphaZero.jl</a> project is to provide an implementation of AlphaZero that is simple enough to be widely accessible for contributors and researchers, while also being sufficiently powerful and fast to enable meaningful experiments on limited computing resources &#40;our latest release is consistently between one and two orders of magnitude faster than competing Python implementations&#41;.</p>
<p>Here are a few project ideas that build on AlphaZero.jl. Please contact us for additional details and let us know about your experience and interests so that we can build a project that best suits your profile.</p>
<ul>
<li><p>&#91;Easy &#40;175h&#41;&#93; Integrate AlphaZero.jl with the <a href="https://github.com/JuliaReinforcementLearning/OpenSpiel.jl">OpenSpiel</a> game library and benchmark it on a series of simple board games.</p>
</li>
<li><p>&#91;Medium &#40;175h&#41;&#93; Use AlphaZero.jl to train a chess agent. In order to save computing resources and allow faster bootstrapping, you may train an initial policy using supervised learning.</p>
</li>
<li><p>&#91;Hard &#40;350h&#41;&#93; Build on AlphaZero.jl to implement the <a href="https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules">MuZero</a> algorithm.</p>
</li>
<li><p>&#91;Hard &#40;350h&#41;&#93; Explore applications of AlphaZero beyond board games &#40;e.g. theorem proving, chip design, chemical synthesis...&#41;.</p>
</li>
</ul>
<h4 id="expected_outcomes"><a href="#expected_outcomes" class="header-anchor">Expected Outcomes</a></h4>
<p>In all these projects, the goal is not only to showcase the current Julia ecosystem and test its limits, but also to push it forward through concrete contributions that other people can build on. Such contributions include:</p>
<ul>
<li><p>Improvements to existing Julia packages &#40;e.g. AlphaZero, ReinforcementLearning, CommonRLInterface, Dagger, Distributed, CUDA...&#41; through code, documentation or benchmarks.</p>
</li>
<li><p>A well-documented and replicable artifact to be added to <a href="https://github.com/jonathan-laurent/AlphaZero.jl/tree/master/games">AlphaZero.Examples</a>, <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningZoo.jl">ReinforcementLearningZoo</a> or released in its own package.</p>
</li>
<li><p>A blog post that details your experience, discusses the challenges you went through and identifies promising areas for future work.</p>
</li>
</ul>
<p><strong>Mentors</strong>: <a href="https://github.com/jonathan-laurent">Jonathan Laurent</a></p>
<h1 id="molecular_simulation_-_summer_of_code"><a href="#molecular_simulation_-_summer_of_code" class="header-anchor">Molecular Simulation - Summer of Code</a></h1>
<p>Much of science can be explained by the movement and interaction of molecules. Molecular dynamics &#40;MD&#41; is a computational technique used to explore these phenomena, from noble gases to biological macromolecules. <a href="https://github.com/JuliaMolSim/Molly.jl">Molly.jl</a> is a pure Julia package for MD, and for the simulation of physical systems more broadly. The package is currently under development with a focus on proteins and differentiable molecular simulation. There are a number of ways that the package could be improved:</p>
<ul>
<li><p><strong>Machine learning potentials &#40;duration: 175h, expected difficulty: easy to medium&#41;:</strong> in the last few years machine learning potentials have been improved significantly. Models such as ANI, ACE, NequIP and Allegro can be added to Molly.</p>
</li>
<li><p><strong>Better GPU performance &#40;duration: 175h, expected difficulty: medium&#41;:</strong> custom GPU kernels can be written to significantly speed up molecular simulation and make the performance of Molly comparable to mature software.</p>
</li>
<li><p><strong>Constraint algorithms &#40;duration: 175h, expected difficulty: medium&#41;:</strong> many simulations keep fast degrees of freedom such as bond lengths and bond angles fixed using approaches such as SHAKE, RATTLE and SETTLE. A fast implementation of these algorithms would be a valuable contribution.</p>
</li>
<li><p><strong>Electrostatic summation &#40;duration: 175h, expected difficulty: medium to hard&#41;:</strong> methods such as particle-mesh Ewald &#40;PME&#41; are in wide use for molecular simulation. Developing fast, flexible implementations and exploring compatibility with GPU acceleration and automatic differentiation would be an <a href="https://discourse.julialang.org/t/electrostatics-in-julia/41633">important contribution</a>.</p>
</li>
</ul>
<p><strong>Recommended skills:</strong> familiarity with computational chemistry, structural bioinformatics or simulating physical systems.</p>
<p><strong>Expected results:</strong> new features added to the package along with tests and relevant documentation.</p>
<p><strong>Mentor:</strong> <a href="https://github.com/jgreener64">Joe Greener</a></p>
<p><strong>Contact:</strong> feel free to ask questions via <a href="http://jgreener64.github.io">email</a> or #juliamolsim on the <a href="https://join.slack.com/t/julialang/shared_invite/zt-2a5wdtotu-H52pQQTMDOa4NwsTSgQ_lQ">Julia Slack</a>.</p>
<h1 id="numerical_projects_summer_of_code"><a href="#numerical_projects_summer_of_code" class="header-anchor">Numerical Projects – Summer of Code</a></h1>
<div class="franklin-toc"><ol><li><a href="#view_all_gsocjsoc_projects">View all GSoC/JSoC Projects</a></li><li><a href="#projects">Projects</a><ol><li><a href="#list_of_projects">List of projects</a></li></ol></li><li><a href="#categorical_variable_encoding">Categorical variable encoding</a><ol><li><a href="#description">Description</a></li><li><a href="#prerequisites">Prerequisites</a></li><li><a href="#your_contribution">Your contribution</a></li><li><a href="#references">References</a></li></ol></li><li><a href="#machine_learning_in_predictive_survival_analysis">Machine Learning in Predictive Survival Analysis</a><ol><li><a href="#description__2">Description</a></li><li><a href="#prerequisites__2">Prerequisites</a></li><li><a href="#your_contribution__2">Your contribution</a></li><li><a href="#references__2">References</a></li></ol></li><li><a href="#deeper_bayesian_integration">Deeper Bayesian Integration</a><ol><li><a href="#description__3">Description</a></li><li><a href="#your_contributions">Your contributions</a></li><li><a href="#references__3">References</a></li><li><a href="#difficulty_medium_to_hard">Difficulty: Medium to Hard</a></li></ol></li><li><a href="#tracking_and_sharing_mlj_workflows_using_mlflow">Tracking and sharing MLJ workflows using MLflow</a><ol><li><a href="#description__4">Description</a></li><li><a href="#prerequisites__3">Prerequisites</a></li><li><a href="#your_contribution__3">Your contribution</a></li><li><a href="#references__4">References</a></li></ol></li><li><a href="#speed_demons_only_need_apply">Speed demons only need apply</a><ol><li><a href="#description__5">Description</a></li><li><a href="#prerequisites__4">Prerequisites</a></li><li><a href="#your_contribution__4">Your contribution</a></li><li><a href="#references__5">References</a></li></ol></li><li><a href="#improving_test_coverage_175_hours">Improving test coverage &#40;175 hours&#41;</a></li><li><a href="#multi-threading_improvement_projects_175_hours_each">Multi-threading Improvement Projects &#40;175 hours each&#41;</a></li><li><a href="#automation_of_testing_performance_benchmarking_350_hours">Automation of testing / performance benchmarking &#40;350 hours&#41;</a></li><li><a href="#documenterjl">Documenter.jl</a></li><li><a href="#fluid-structure_interaction_example">Fluid-Structure Interaction Example</a></li><li><a href="#investigation_of_performant_assembly_strategies">Investigation of Performant Assembly Strategies</a><ol><li><a href="#training_on_very_large_graphs">Training on very large graphs  </a></li><li><a href="#adding_graph_convolutional_layers">Adding graph convolutional layers </a></li><li><a href="#adding_models_and_examples">Adding models and examples</a></li><li><a href="#adding_graph_datasets">Adding graph datasets</a></li><li><a href="#implement_layers_for_heterogeneous_graphs">Implement layers for heterogeneous graphs</a></li><li><a href="#improving_performance_using_sparse_linear_algebra">Improving performance using sparse linear algebra </a></li><li><a href="#support_for_amgdpu_and_apple_silicon">Support for AMGDPU and Apple Silicon</a></li><li><a href="#implement_layers_for_temporal_graphs">Implement layers for Temporal Graphs</a></li></ol></li><li><a href="#recommended_skills">Recommended skills</a></li><li><a href="#mentors">Mentors </a></li><li><a href="#qml_and_makie_integration">QML and Makie integration</a><ol><li><a href="#expected_results">Expected results</a></li></ol></li><li><a href="#web_apps_in_makie_and_jsserve">Web apps in Makie and JSServe</a><ol><li><a href="#expected_results__2">Expected results</a></li></ol></li><li><a href="#scheduling_algorithms_for_dagger">Scheduling Algorithms for Dagger</a></li><li><a href="#distributed_training">Distributed Training</a></li><li><a href="#distributed_arrays_over_dagger">Distributed Arrays over Dagger</a></li><li><a href="#benchmarking_against_other_frameworks">Benchmarking against other frameworks</a></li><li><a href="#where_to_go_for_discussion_and_to_find_mentors">Where to go for discussion and to find mentors</a></li><li><a href="#c">C&#43;&#43;</a><ol><li><a href="#cxxwrap_stl">CxxWrap STL</a><ol><li><a href="#expected_outcome">Expected outcome</a></li></ol></li></ol></li><li><a href="#rust">Rust</a><ol><li><a href="#general_goal_of_juliaconstraints">General goal of JuliaConstraints</a></li></ol></li><li><a href="#constraint_programming-based_design_for_kumi_kumi_slope">Constraint Programming-Based Design for Kumi Kumi Slope</a><ol><li><a href="#core_objectives">Core Objectives</a></li></ol></li><li><a href="#agentsjl">Agents.jl</a></li><li><a href="#dynamicalsystemsjl">DynamicalSystems.jl</a></li><li><a href="#large_language_model_projects">Large Language Model Projects</a><ol><li><a href="#project_1_enhancing_llama2jl_with_gpu_support">Project 1: Enhancing llama2.jl with GPU Support</a></li><li><a href="#project_2_llamajl_-_low-level_c_interface">Project 2: Llama.jl - Low-level C interface</a></li><li><a href="#project_3_supercharging_the_knowledge_base_of_aihelpmejl">Project 3: Supercharging the Knowledge Base of AIHelpMe.jl</a></li><li><a href="#project_4_enhancing_julias_ai_ecosystem_with_colbert_v2_for_efficient_document_retrieval">Project 4: Enhancing Julia&#39;s AI Ecosystem with ColBERT v2 for Efficient Document Retrieval</a></li><li><a href="#project_5_enhancing_promptingtoolsjl_with_advanced_schema_support_and_functionality">Project 5: Enhancing PromptingTools.jl with Advanced Schema Support and Functionality</a></li><li><a href="#project_6_expanding_the_julia_large_language_model_leaderboard">Project 6: Expanding the Julia Large Language Model Leaderboard</a></li><li><a href="#project_7_counterfactuals_for_llms_model_explainability_and_generative_ai">Project 7: Counterfactuals for LLMs &#40;<em>Model Explainability</em> and <em>Generative AI</em>&#41;</a></li></ol></li><li><a href="#how_to_contact_us">How to Contact Us</a></li><li><a href="#observational_health_subecosystem_projects">Observational Health Subecosystem Projects</a><ol><li><a href="#project_1_developing_tooling_for_observational_health_research_in_julia">Project 1: Developing Tooling for Observational Health Research in Julia</a></li><li><a href="#project_2_developing_patient_level_prediction_tooling_within_julia">Project 2: Developing Patient Level Prediction Tooling within Julia</a></li></ol></li><li><a href="#medical_imaging_subecosystem_projects">Medical Imaging Subecosystem Projects</a><ol><li><a href="#project_3_adding_functionalities_to_medical_imaging_visualizations">Project 3: Adding functionalities to medical imaging visualizations</a></li><li><a href="#project_4_adding_dataset-wide_functions_and_integrations_of_augmentations">Project 4: Adding dataset-wide functions and integrations of augmentations</a></li><li><a href="#project_5_highly-efficient_mri_simulations_with_multi-vendor_gpu_support">Project 5: Highly-efficient MRI Simulations with Multi-Vendor GPU Support</a></li></ol></li><li><a href="#midification_of_music_from_wave_files">MIDIfication of music from wave files</a></li><li><a href="#efficient_symbolic-numeric_set_computations">Efficient symbolic-numeric set computations</a></li><li><a href="#reachability_with_sparse_polynomial_zonotopes">Reachability with sparse polynomial zonotopes</a></li><li><a href="#improving_the_hybrid_systems_reachability_api">Improving the hybrid systems reachability API</a></li><li><a href="#panel_data_analysis">Panel data analysis</a><ol><li><a href="#description__6">Description</a></li><li><a href="#prerequisites__5">Prerequisites</a></li><li><a href="#your_contribution__5">Your contribution</a></li><li><a href="#references__6">References</a></li></ol></li><li><a href="#distributionsjl_expansion">Distributions.jl Expansion</a><ol><li><a href="#prerequisites__6">Prerequisites</a></li><li><a href="#your_contribution__6">Your contribution</a></li></ol></li><li><a href="#hypothesistestingjl_expansion">HypothesisTesting.jl Expansion</a><ol><li><a href="#prerequisites__7">Prerequisites</a></li><li><a href="#your_contribution__7">Your contribution</a></li><li><a href="#references__7">References</a></li></ol></li><li><a href="#crraojl">CRRao.jl</a><ol><li><a href="#description__7">Description</a></li><li><a href="#prerequisites__8">Prerequisites</a></li><li><a href="#your_contribution__8">Your contribution</a></li></ol></li><li><a href="#juliastats_improvements">JuliaStats Improvements </a><ol><li><a href="#description__8">Description</a></li><li><a href="#prerequisites__9">Prerequisites</a></li><li><a href="#your_contribution__9">Your contribution</a></li></ol></li><li><a href="#surveyjl">Survey.jl</a><ol><li><a href="#prerequisites__10">Prerequisites</a></li><li><a href="#your_contribution__10">Your contribution</a></li><li><a href="#references__8">References</a></li></ol></li><li><a href="#smoothing_non-linear_continuous_time_systems">Smoothing non-linear continuous time systems</a><ol><li><a href="#reinforcement_learning_environments">Reinforcement Learning Environments</a><ol><li><a href="#expected_outcome__2">Expected outcome</a></li></ol></li><li><a href="#alphazerojl">AlphaZero.jl</a><ol><li><a href="#expected_outcomes">Expected Outcomes</a></li></ol></li></ol></li><li><a href="#numerical_linear_algebra">Numerical Linear Algebra</a><ol><li><a href="#matrix_functions">Matrix functions</a></li></ol></li><li><a href="#better_bignums_integration">Better Bignums Integration</a><ol><li><a href="#special_functions">Special functions</a></li><li><a href="#a_julia-native_ccsa_optimization_algorithm">A Julia-native CCSA optimization algorithm</a></li></ol></li><li><a href="#massive_parallel_factorized_bouncy_particle_sampler">Massive parallel factorized bouncy particle sampler</a></li><li><a href="#machine_learning_time_series_regression">Machine Learning Time Series Regression</a></li><li><a href="#machine_learning_for_nowcasting_and_forecasting">Machine learning for nowcasting and forecasting</a></li><li><a href="#time_series_forecasting_at_scales">Time series forecasting at scales</a></li><li><a href="#gpu_accelerated_simulator_of_clifford_circuits">GPU accelerated simulator of Clifford Circuits.</a></li><li><a href="#a_zoo_of_quantum_error_correcting_codes_andor_decoders">A Zoo of Quantum Error Correcting codes and/or decoders</a></li><li><a href="#leftright_multiplications_with_small_gates">Left/Right multiplications with small gates.</a></li><li><a href="#generation_of_fault_tolerant_ecc_circuits_flag_qubit_circuits_and_more">Generation of Fault Tolerant ECC Circuits, Flag Qubit Circuits and more</a></li><li><a href="#measurement-based_quantum_computing_mbqc_compiler">Measurement-Based Quantum Computing &#40;MBQC&#41; compiler</a></li><li><a href="#implementing_a_graph_state_simulator">Implementing a Graph State Simulator</a></li><li><a href="#simulation_of_slightly_non-clifford_circuits_and_states">Simulation of Slightly Non-Clifford Circuits and States</a></li><li><a href="#magic_state_modeling_-_distillation_injection_etc">Magic State Modeling - Distillation, Injection, Etc</a></li><li><a href="#gpu_accelerated_operators_and_ode_solvers">GPU accelerated operators and ODE solvers</a></li><li><a href="#autodifferentiation">Autodifferentiation</a></li><li><a href="#closer_integration_with_the_sciml_ecosystem">Closer Integration with the SciML Ecosystem</a></li><li><a href="#efficient_tensor_differentiation">Efficient Tensor Differentiation</a></li><li><a href="#symbolic_root_finding">Symbolic root finding</a></li><li><a href="#symbolic_integration_in_symbolicsjl">Symbolic Integration in Symbolics.jl</a></li><li><a href="#xla-style_optimization_from_symbolic_tracing">XLA-style optimization from symbolic tracing</a></li><li><a href="#automatically_improving_floating_point_accuracy_herbie">Automatically improving floating point accuracy &#40;Herbie&#41;</a></li><li><a href="#parquetjl_enhancements">Parquet.jl enhancements</a></li><li><a href="#dataframesjl_join_enhancements">DataFrames.jl join enhancements</a></li><li><a href="#project_1_conformal_prediction_meets_bayes_predictive_uncertainty">Project 1: Conformal Prediction meets Bayes &#40;<em>Predictive Uncertainty</em>&#41;</a></li><li><a href="#project_2_counterfactual_regression_model_explainability">Project 2: Counterfactual Regression &#40;<em>Model Explainability</em>&#41;</a></li><li><a href="#project_3_counterfactuals_for_llms_model_explainability_and_generative_ai">Project 3: Counterfactuals for LLMs &#40;<em>Model Explainability</em> and <em>Generative AI</em>&#41;</a></li><li><a href="#project_4_from_counterfactuals_to_interventions_recourse_through_minimal_causal_interventions">Project 4: From Counterfactuals to Interventions &#40;Recourse through Minimal Causal Interventions&#41;</a></li><li><a href="#about_us">About Us</a></li><li><a href="#how_to_contact_us__2">How to Contact Us</a></li><li><a href="#testing_and_benchmarking_of_topoptjl">Testing and benchmarking of TopOpt.jl</a></li><li><a href="#machine_learning_in_topology_optimization">Machine learning in topology optimization</a></li><li><a href="#optimization_on_a_uniform_rectilinear_grid">Optimization on a uniform rectilinear grid</a></li><li><a href="#adaptive_mesh_refinement_for_topology_optimization">Adaptive mesh refinement for topology optimization</a></li><li><a href="#heat_transfer_design_optimization">Heat transfer design optimization</a></li><li><a href="#compiler-based_automatic_differentiation_with_enzymejl">Compiler-based automatic differentiation with Enzyme.jl</a></li><li><a href="#advanced_visualization_and_in-situ_visualization_with_paraview">Advanced visualization and in-situ visualization with ParaView</a></li><li><a href="#implementing_models_from_posteriordb_in_turing_julia">Implementing models from PosteriorDB in Turing / Julia</a></li><li><a href="#improving_the_integration_between_turing_and_turings_mcmc_inference_packages">Improving the integration between Turing and Turing’s MCMC inference packages</a></li><li><a href="#gpu_support_for_normalizingflowsjl_and_bijectorsjl">GPU support for NormalizingFlows.jl and Bijectors.jl</a></li><li><a href="#batched_support_for_normalizingflowsjl_and_bijectorsjl">Batched support for NormalizingFlows.jl and Bijectors.jl</a></li><li><a href="#targets_for_benchmarking_samplers_with_vectorization_gpu_and_high-order_derivative_supports">Targets for Benchmarking Samplers with vectorization, GPU and high-order derivative supports</a></li><li><a href="#vs_code_extension">VS Code extension</a></li><li><a href="#package_installation_ui">Package installation UI</a></li><li><a href="#code_generation_improvements_and_async_abi">Code generation improvements and async ABI</a></li><li><a href="#wasm_threading">Wasm threading</a></li><li><a href="#high_performance_low-level_integration_of_js_objects">High performance, Low-level integration of js objects</a></li><li><a href="#dom_integration">DOM Integration</a></li><li><a href="#porting_existing_web-integration_packages_to_the_wasm_platform">Porting existing web-integration packages to the wasm platform</a></li><li><a href="#native_dependencies_for_the_web">Native dependencies for the web</a></li><li><a href="#distributed_computing_with_untrusted_parties">Distributed computing with untrusted parties</a></li><li><a href="#deployment">Deployment</a></li></ol></div>
<h2 id="numerical_linear_algebra"><a href="#numerical_linear_algebra" class="header-anchor">Numerical Linear Algebra</a></h2>
<h3 id="matrix_functions"><a href="#matrix_functions" class="header-anchor">Matrix functions</a></h3>
<p>Matrix functions map matrices onto other matrices, and can often be interpreted as generalizations of ordinary functions like sine and exponential, which map numbers to numbers. Once considered a niche province of numerical algorithms, matrix functions now appear routinely in applications to cryptography, aircraft design, nonlinear dynamics, and finance.</p>
<p>This project proposes to implement state of the art algorithms that extend the currently available matrix functions in Julia, as outlined in issue <a href="https://github.com/JuliaLang/julia/issues/5840">#5840</a>. In addition to matrix generalizations of standard functions such as real matrix powers, surds and logarithms, contributors will be challenged to design generic interfaces for lifting general scalar-valued functions to their matrix analogues for the efficient computation of arbitrary &#40;well-behaved&#41; matrix functions and their derivatives.</p>
<p><strong>Recommended Skills</strong>: A strong understanding of calculus and numerical analysis.</p>
<p><strong>Expected Results</strong>: New and faster methods for evaluating matrix functions.</p>
<p><strong>Mentors:</strong> <a href="https://github.com/jiahao">Jiahao Chen</a>, <a href="https://github.com/stevengj">Steven Johnson</a>.</p>
<p><strong>Difficulty:</strong> Hard</p>
<h2 id="better_bignums_integration"><a href="#better_bignums_integration" class="header-anchor">Better Bignums Integration</a></h2>
<p>Julia currently supports big integers and rationals, making use of the GMP. However, GMP currently doesn&#39;t permit good integration with a garbage collector.</p>
<p>This project therefore involves exploring ways to improve BigInt, possibly including:</p>
<div class="tight-list"><ul>
<li><p>Modifying GMP to support high-performance garbage-collection</p>
</li>
<li><p>Reimplementation of aspects of BigInt in Julia</p>
</li>
<li><p>Lazy graph style APIs which can rewrite terms or apply optimisations</p>
</li>
</ul></div>
<p>This experimentation could be carried out as a package with a new implementation, or as patches over the existing implementation in Base.</p>
<p><strong>Expected Results</strong>: An implementation of BigInt in Julia with increased performance over the current one.</p>
<p><strong>Require Skills</strong>: Familiarity with extended precision numerics OR performance considerations. Familiarity either with Julia or GMP.</p>
<p><strong>Mentors</strong>: <a href="https://github.com/vtjnash">Jameson Nash</a></p>
<p><strong>Difficulty:</strong> Hard</p>
<h3 id="special_functions"><a href="#special_functions" class="header-anchor">Special functions</a></h3>
<p>As a technical computing language, Julia provides a huge number of <a href="https://en.wikipedia.org/wiki/Special_functions">special functions</a>, both in Base as well as packages such as <a href="https://github.com/JuliaStats/StatsFuns.jl">StatsFuns.jl</a>. At the moment, many of these are implemented in external libraries such as <a href="https://github.com/JuliaLang/Rmath-julia">Rmath</a> and <a href="https://github.com/JuliaLang/openspecfun">openspecfun</a>. This project would involve implementing these functions in native Julia &#40;possibly utilising the work in <a href="https://github.com/nolta/SpecialFunctions.jl">SpecialFunctions.jl</a>&#41;, seeking out opportunities for possible improvements along the way, such as supporting <code>Float32</code> and <code>BigFloat</code>, exploiting fused multiply-add operations, and improving errors and boundary cases.</p>
<p><strong>Recommended Skills</strong>: A strong understanding of calculus.</p>
<p><strong>Expected Results</strong>: New and faster methods for evaluating properties of special functions.</p>
<p><strong>Mentors:</strong> <a href="https://github.com/stevengj">Steven Johnson</a>, <a href="https://github.com/oscardssmith">Oscar Smith</a>. Ask on Discourse or on slack</p>
<h3 id="a_julia-native_ccsa_optimization_algorithm"><a href="#a_julia-native_ccsa_optimization_algorithm" class="header-anchor">A Julia-native CCSA optimization algorithm</a></h3>
<p>The CCSA algorithm by <a href="https://epubs.siam.org/doi/10.1137/S1052623499362822">Svanberg &#40;2001&#41;</a> is a <a href="https://en.wikipedia.org/wiki/Nonlinear_programming">nonlinear programming algorithm</a> widely used in <a href="https://en.wikipedia.org/wiki/Topology_optimization">topology optimization</a> and for other large-scale optimization problems: it is a robust algorithm that can handle arbitrary nonlinear inequality constraints and huge numbers of degrees of freedom.  Moreover, the relative simplicity of the algorithm makes it possible to easily incorporate sparsity in the Jacobian matrix &#40;for handling huge numbers of constraints&#41;, approximate-Hessian preconditioners, and as special-case optimizations for affine terms in the objective or constraints.  However, currently it is only available in Julia via the <a href="https://github.com/JuliaOpt/NLopt.jl">NLopt.jl</a> interface to an external C implementation, which greatly limits its flexibility.</p>
<p><strong>Recommended Skills</strong>: Experience with nonlinear optimization algorithms and understanding of <a href="https://en.wikipedia.org/wiki/Duality_&#40;optimization&#41;">Lagrange duality</a>, familiarity with sparse matrices and other Julia data structures.</p>
<p><strong>Expected Results</strong>: A package implementing a native-Julia CCSA algorithm.</p>
<p><strong>Mentors:</strong> <a href="https://github.com/stevengj">Steven Johnson</a>.</p>
<h1 id="event-chain_monte_carlo_methods_summer_of_code"><a href="#event-chain_monte_carlo_methods_summer_of_code" class="header-anchor">Event-chain Monte Carlo methods – Summer of Code</a></h1>
<h2 id="massive_parallel_factorized_bouncy_particle_sampler"><a href="#massive_parallel_factorized_bouncy_particle_sampler" class="header-anchor">Massive parallel factorized bouncy particle sampler</a></h2>
<p>At JuliaCon 2021 a new sampler Monte Carlo method &#40;for example as sampling algorithm for the posterior in Bayesian inference&#41; was  introduced &#91;1&#93;. The method exploits the factorization structure to sample <em>a single</em> continuous time Markov chain targeting a joint distribution in parallel. In contrast to parallel Gibbs sampling in the method at no time a subset of coordinates is kept fixed. In Gibbs sampling keeping  a subset fixed is the main device to achieve massive parallelism: given a separating set of coordinates, the conditional posterior factorizes into independent subproblems. In the presented method, a particle representing a parameter vector sampled from the posterior never  ceases to move, and it is only the decisions about changes of the direction of the movement which happen in parallel on subsets of coordinates.</p>
<p>There are already two implementations available which make use of Julias multithreading capabilities. Starting from that, the contributor  implements a version of the algorithm using GPU computing techniques as the methods is are suitable for these approaches.</p>
<p><strong>Expected Results</strong>: Implement massive parallel factorized bouncy particle sampler &#91;1,2&#93; using GPU computing.</p>
<p><strong>Recommended Skills</strong>: GPU computing, Markov processes, Bayesian inference.</p>
<p><strong>Mentors</strong>: <a href="https://github.com/mschauer">Moritz Schauer</a></p>
<p><strong>Rating</strong>: Hard, 350 hours</p>
<p>&#91;1&#93; Moritz Schauer: ZigZagBoomerang.jl - parallel inference and variable selection. JuliaCon 2021 contribution &#91;https://pretalx.com/juliacon2021/talk/LUVWJZ/&#93;, Youtube: &#91;https://www.youtube.com/watch?v&#61;wJAjP_I1BnQ&#93;, 2021.</p>
<p>&#91;2&#93; Joris Bierkens, Paul Fearnhead, Gareth Roberts: The Zig-Zag Process and Super-Efficient Sampling for Bayesian Analysis of Big Data. The Annals of Statistics, 2019, 47. Vol., Nr. 3, pp. 1288-1320. &#91;https://arxiv.org/abs/1607.03188&#93;.</p>
<h1 id="plutojl_projects"><a href="#plutojl_projects" class="header-anchor">Pluto.jl projects</a></h1>
<p>Unfortunately we won&#39;t have time to mentor this year.  Check back next year&#33;</p>
<h1 id="pythia_summer_of_code"><a href="#pythia_summer_of_code" class="header-anchor">Pythia – Summer of Code</a></h1>
<h2 id="machine_learning_time_series_regression"><a href="#machine_learning_time_series_regression" class="header-anchor">Machine Learning Time Series Regression</a></h2>
<p><a href="https://github.com/ababii/Pythia.jl">Pythia</a> is a package for scalable machine learning time series forecasting and nowcasting in Julia.</p>
<p>The project mentors are <a href="https://ababii.github.io/">Andrii Babii</a> and <a href="https://www.turing.ac.uk/people/researchers/sebastian-vollmer/">Sebastian Vollmer</a>.</p>
<h2 id="machine_learning_for_nowcasting_and_forecasting"><a href="#machine_learning_for_nowcasting_and_forecasting" class="header-anchor">Machine learning for nowcasting and forecasting</a></h2>
<p>This project involves developing scalable machine learning time series regressions for nowcasting and forecasting. Nowcasting in economics is the prediction of the present, the very near future, and the very recent past state of an economic indicator. The term is a contraction of &quot;now&quot; and &quot;forecasting&quot; and originates in meteorology. </p>
<p>The objective of this project is to introduce scalable regression-based nowcasting and forecasting methodologies that demonstrated the empirical success in data-rich environment recently. Examples of existing popular packages for regression-based nowcasting on other platforms include the &quot;MIDAS Matlab Toolbox&quot;, as well as the &#39;midasr&#39; and &#39;midasml&#39; packages in R. The starting point for this project is porting the &#39;midasml&#39; package from R to Julia. Currently Pythia has the sparse-group LASSO regression functionality for forecasting. </p>
<p>The following functions are of interest: in-sample and out-of sample forecasts/nowcasts, regularized MIDAS with Legendre polynomials, visualization of nowcasts, AIC/BIC and time series cross-validation tuning, forecast evaluation, pooled and fixed effects panel data regressions for forecasting and nowcasting, HAC-based inference for sparse-group LASSO, high-dimensional Granger causality tests. Other widely used existing functions from R/Python/Matlab are also of interest.</p>
<p><strong>Recommended skills:</strong> Graduate-level knowledge of time series analysis, machine learning, and optimization is helpful.</p>
<p><strong>Expected output:</strong> The contributor is expected to produce code, documentation, visualization, and real-data examples.</p>
<p><strong>References:</strong> Contact project mentors for references.</p>
<h2 id="time_series_forecasting_at_scales"><a href="#time_series_forecasting_at_scales" class="header-anchor">Time series forecasting at scales</a></h2>
<p>Modern business applications often involve forecasting hundreds of thousands of time series. Producing such a gigantic number of reliable and high-quality forecasts is computationally challenging, which limits the scope of potential methods that can be used in practice, see, e.g., the &#39;forecast&#39;, &#39;fable&#39;, or &#39;prophet&#39; packages in R. Currently, Julia lacks the scalable time series forecasting functionality and this project aims to develop the automated data-driven and scalable time series forecasting methods. </p>
<p>The following  functionality is of interest: forecasting intermittent demand &#40;Croston, adjusted Croston, INARMA&#41;, scalable seasonal ARIMA with covariates, loss-based forecasting &#40;gradient boosting&#41;, unsupervised time series clustering, forecast combinations, unit root tests &#40;ADF, KPSS&#41;. Other widely used existing functions from R/Python/Matlab are also of interest.</p>
<p><strong>Recommended skills:</strong> Graduate-level knowledge of time series analysis is helpful.</p>
<p><strong>Expected output:</strong> The contributor is expected to produce code, documentation, visualization, and real-data examples.</p>
<p><strong>References:</strong> Contact project mentors for references.</p>
<h1 id="tools_for_simulation_of_quantum_clifford_circuits"><a href="#tools_for_simulation_of_quantum_clifford_circuits" class="header-anchor">Tools for simulation of Quantum Clifford Circuits</a></h1>
<p>Clifford circuits are a class of quantum circuits that can be simulated efficiently on a classical computer. As such, they do not provide the computational advantage expected of universal quantum computers. Nonetheless, they are extremely important, as they underpin most techniques for quantum error correction and quantum networking. Software that efficiently simulates such circuits, at the scale of thousands or more qubits, is essential to the design of quantum hardware. The <a href="https://github.com/Krastanov/QuantumClifford.jl">QuantumClifford.jl</a> Julia project enables such simulations.</p>
<h2 id="gpu_accelerated_simulator_of_clifford_circuits"><a href="#gpu_accelerated_simulator_of_clifford_circuits" class="header-anchor">GPU accelerated simulator of Clifford Circuits.</a></h2>
<p>Simulation of Clifford circuits involves significant amounts of linear algebra with boolean matrices. This enables the use of many standard computation accelerators like GPUs, as long as these accelerators support bit-wise operations. The main complications is that the elements of the matrices under consideration are usually packed in order to increase performance and lower memory usage, i.e. a vector of 64 elements would be stored as a single 64 bit integer instead of as an array of 64 bools. A Summer of Code project could consist of implement the aforementioned linear algebra operations in GPU kernels, and then seamlessly integrating them in the rest of the QuantumClifford library. At a minimum that would include <a href="https://github.com/Krastanov/QuantumClifford.jl/blob/v0.4.0/src/QuantumClifford.jl#L725">Pauli-Pauli products</a> and certain <a href="https://github.com/Krastanov/QuantumClifford.jl/blob/v0.4.0/src/symbolic_cliffords.jl">small Clifford operators</a>, but could extend to general <a href="https://github.com/Krastanov/QuantumClifford.jl/blob/v0.4.0/src/QuantumClifford.jl#L1385">stabilizer tableau multiplication</a> and even <a href="https://github.com/Krastanov/QuantumClifford.jl/blob/v0.4.0/src/QuantumClifford.jl#L985">tableau diagonalization</a>. Some of these features are already implemented, but significant polish and further improvements and implementation of missing features is needed.</p>
<p><strong>Recommended skills:</strong> Basic knowledge of the <a href="https://krastanov.github.io/QuantumClifford.jl/dev/references/">stabilizer formalism</a> used for simulating Clifford circuits. Familiarity with performance profiling tools in Julia and Julia&#39;s GPU stack, including <a href="https://github.com/JuliaGPU/KernelAbstractions.jl">KernelAbstractions</a> and <a href="https://github.com/mcabbott/Tullio.jl">Tullio</a>.</p>
<p><strong>Mentors:</strong> <a href="https://github.com/Krastanov">Stefan Krastanov</a> <a href="mailto:stefan@krastanov.org">&lt;stefan@krastanov.org&gt;</a> and QuantumClifford.jl team members</p>
<p><strong>Expected duration:</strong> 175 hours &#40;but applicants can scope it to a longer project by including work on GPU-accelerated Gaussian elimination used in the canonicalization routines&#41;</p>
<p><strong>Difficulty:</strong> Medium if the applicant is familiar with Julia, even without understanding of Quantum Information Science &#40;but applicants can scope it to &quot;hard&quot; by including the aforementioned additional topics&#41;</p>
<h2 id="a_zoo_of_quantum_error_correcting_codes_andor_decoders"><a href="#a_zoo_of_quantum_error_correcting_codes_andor_decoders" class="header-anchor">A Zoo of Quantum Error Correcting codes and/or decoders</a></h2>
<p>Quantum Error Correcting codes are typically represented in a form similar to the parity check matrix of a classical code. This form is referred to as a Stabilizer tableaux. This project would involve creating a comprehensive library of frequently used quantum error correcting codes and/or implementing syndrome-decoding algorithms for such codes. The library already includes some simple codes and interfaces to a few decoders – adding another small code or providing a small documentation pull request could be a good way to prove competence when applying for this project. The project can be extended to a much longer one if work on decoders is included. A large part of this project would involve literature surveys. Some suggestions for codes to include: color codes, higher-dimensional topological codes, hyper graph product codes, twists in codes, newer LDPC codes, honeycomb codes, Floquet codes. Some suggestions for decoders to work on: iterative, small-set flip, ordered statistical decoding, belief propagation, neural belief propagation.</p>
<p><strong>Recommended skills:</strong> Knowledge of the <a href="https://krastanov.github.io/QuantumClifford.jl/dev/references/">stabilizer formalism</a> used for simulating Clifford circuits. Familiarity with tools like python&#39;s <code>ldpc</code>, <code>pymatching</code>, and <code>stim</code> can help. Consider checking out the <code>PyQDecoders.jl</code> julia wrapper package as well.</p>
<p><strong>Mentors:</strong> <a href="https://github.com/Krastanov">Stefan Krastanov</a> <a href="mailto:stefan@krastanov.org">&lt;stefan@krastanov.org&gt;</a> and QuantumClifford.jl team members</p>
<p><strong>Expected duration:</strong> 175 hours &#40;but applicants can scope it as longer, depending on the list of functionality they plan to implement&#41;</p>
<p><strong>Difficulty:</strong> Medium. Easy with some basic knowledge of quantum error correction</p>
<h2 id="leftright_multiplications_with_small_gates"><a href="#leftright_multiplications_with_small_gates" class="header-anchor">Left/Right multiplications with small gates.</a></h2>
<p>Applying an n-qubit Clifford gate to an n-qubit state &#40;tableaux&#41; is an operation similar to matrix multiplication, requiring O&#40;n^3&#41; steps. However, applying a single-qubit or two-qubit gate to an n-qubit tableaux is much faster as it needs to address only one or two columns of the tableaux. This project would focus on extending the left-multiplication special cases already started in <a href="https://github.com/Krastanov/QuantumClifford.jl/blob/master/src/symbolic_cliffords.jl">symbolic_cliffords.jl</a> and creating additional right-multiplication special cases &#40;for which <a href="https://github.com/Krastanov/QuantumClifford.jl/commit/d3e84c16b7b08ef6f1bc24e2bcf98641c2fff1ab#r67183201">the Stim library is a good reference</a>&#41;.</p>
<p><strong>Recommended skills:</strong> Knowledge of the <a href="https://krastanov.github.io/QuantumClifford.jl/dev/references/">stabilizer formalism</a> used for simulating Clifford circuits. Familiarity with performance profiling tools in Julia. Understanding of C/C&#43;&#43; if you plan to use the Stim library as a reference.</p>
<p><strong>Mentors:</strong> <a href="https://github.com/Krastanov">Stefan Krastanov</a> <a href="mailto:stefan@krastanov.org">&lt;stefan@krastanov.org&gt;</a> and QuantumClifford.jl team members</p>
<p><strong>Expected duration:</strong> 175 hours &#40;but applicants can scope it as longer if they plan for other significant optimization and API design work&#41;</p>
<p><strong>Difficulty:</strong> Easy</p>
<h2 id="generation_of_fault_tolerant_ecc_circuits_flag_qubit_circuits_and_more"><a href="#generation_of_fault_tolerant_ecc_circuits_flag_qubit_circuits_and_more" class="header-anchor">Generation of Fault Tolerant ECC Circuits, Flag Qubit Circuits and more</a></h2>
<p>The QuantumClifford library already has some <a href="https://github.com/QuantumSavory/QuantumClifford.jl/blob/v0.8.19/src/ecc/circuits.jl">support for generating different types of circuits related to error correction</a> &#40;mostly in terms of syndrome measurement circuits like Shor&#39;s&#41; and for evaluating the quality of error correcting codes and decoders. Significant improvement can be made by implementing more modern compilation schemes, especially ones relying on flag qubits.</p>
<p><strong>Recommended skills:</strong> Knowledge of the variety of flag qubit methods. Some useful references could be <a href="https://link.aps.org/accepted/10.1103/PhysRevLett.121.050502">a</a>, <a href="https://www.nature.com/articles/s41534-018-0085-z">b</a>, <a href="https://journals.aps.org/prxquantum/pdf/10.1103/PRXQuantum.1.010302">c</a>, and this <a href="https://www.youtube.com/watch?v&#61;etA9l2NUCXI">video lecture</a>.</p>
<p><strong>Mentors:</strong> <a href="https://github.com/Krastanov">Stefan Krastanov</a> <a href="mailto:stefan@krastanov.org">&lt;stefan@krastanov.org&gt;</a> and QuantumClifford.jl team members</p>
<p><strong>Expected duration:</strong> 175 hours &#40;but applicants can scope it as longer if they plan more extensive work&#41;</p>
<p><strong>Difficulty:</strong> Hard</p>
<h2 id="measurement-based_quantum_computing_mbqc_compiler"><a href="#measurement-based_quantum_computing_mbqc_compiler" class="header-anchor">Measurement-Based Quantum Computing &#40;MBQC&#41; compiler</a></h2>
<p>The MBQC model of quantum computation has a lot of overlap with the study of Stabilizer states. This project would be about the creation of an MBQC compiler and potentially simulator in Julia. E.g. if one is given an arbitrary graph state and a circuit, how is this circuit to be compiled in an MBQC model.</p>
<p><strong>Recommended skills:</strong> Knowledge of the MBQC model of quantum computation. This <a href="https://arxiv.org/pdf/2212.11975.pdf">paper and the related python library</a> can be a useful reference. Consider also <a href="https://quantum-journal.org/papers/q-2021-03-25-421/">this reference</a>.</p>
<p><strong>Mentors:</strong> <a href="https://github.com/Krastanov">Stefan Krastanov</a> <a href="mailto:stefan@krastanov.org">&lt;stefan@krastanov.org&gt;</a> and QuantumClifford.jl team members</p>
<p><strong>Expected duration:</strong> 175 hours &#40;but applicants can scope it as longer if they plan more extensive work&#41;</p>
<p><strong>Difficulty:</strong> Hard</p>
<h2 id="implementing_a_graph_state_simulator"><a href="#implementing_a_graph_state_simulator" class="header-anchor">Implementing a Graph State Simulator</a></h2>
<p>The graph states formalism is a way to work more efficiently with stabilizer states that have a sparse tableaux. This project would involve creation of the necessary gate simulation algorithms and conversions tools between graph formalism and stabilizer formalism &#40;some of which are <a href="https://github.com/QuantumSavory/QuantumClifford.jl/blob/master/src/graphs.jl">already available in the library</a>&#41;.</p>
<p><strong>Recommended skills:</strong> Understanding of the graph formalism. This <a href="https://arxiv.org/abs/quant-ph/0504117">paper can be a useful reference</a>.</p>
<p><strong>Mentors:</strong> <a href="https://github.com/Krastanov">Stefan Krastanov</a> <a href="mailto:stefan@krastanov.org">&lt;stefan@krastanov.org&gt;</a> and QuantumClifford.jl team members</p>
<p><strong>Expected duration:</strong> 175 hours &#40;but applicants can scope it as longer if they plan more extensive work&#41;</p>
<p><strong>Difficulty:</strong> Medium</p>
<h2 id="simulation_of_slightly_non-clifford_circuits_and_states"><a href="#simulation_of_slightly_non-clifford_circuits_and_states" class="header-anchor">Simulation of Slightly Non-Clifford Circuits and States</a></h2>
<p>There are various techniques used to augment Clifford circuit simulators to model circuits that are only &quot;mostly&quot; Clifford. Particularly famous are the Clifford&#43;T gate simulators. This project is about implementing such extensions.</p>
<p><strong>Recommended skills:</strong> In-depth understanding of the Stabilizer formalism, and understanding of some of the extensions to that method. We have some <a href="https://github.com/QuantumSavory/QuantumClifford.jl/blob/master/src/nonclifford.jl">initial implementations</a>. This <a href="https://arxiv.org/pdf/1808.00128.pdf">IBM paper</a> can also be a useful reference for other methods.</p>
<p><strong>Mentors:</strong> <a href="https://github.com/Krastanov">Stefan Krastanov</a> <a href="mailto:stefan@krastanov.org">&lt;stefan@krastanov.org&gt;</a> and QuantumClifford.jl team members</p>
<p><strong>Expected duration:</strong> 175 hours &#40;but applicants can scope it as longer if they plan more extensive work&#41;</p>
<p><strong>Difficulty:</strong> Hard</p>
<h2 id="magic_state_modeling_-_distillation_injection_etc"><a href="#magic_state_modeling_-_distillation_injection_etc" class="header-anchor">Magic State Modeling - Distillation, Injection, Etc</a></h2>
<p>Magic states are important non-stabilizer states that can be used for inducing non-Clifford gates in otherwise Clifford circuits. They are crucial for the creation of error-corrected universal circuits. This project would involve contributing tools for the analysis of such states and for the evaluation of distillation circuits and ECC circuits involving such states.</p>
<p><strong>Recommended skills:</strong> In-depth understanding of the theory of magic states and their use in fault tolerance.</p>
<p><strong>Mentors:</strong> <a href="https://github.com/Krastanov">Stefan Krastanov</a> <a href="mailto:stefan@krastanov.org">&lt;stefan@krastanov.org&gt;</a> and QuantumClifford.jl team members</p>
<p><strong>Expected duration:</strong> 175 hours &#40;but applicants can scope it as longer if they plan more extensive work&#41;</p>
<p><strong>Difficulty:</strong> Hard</p>
<h1 id="quantum_optics_and_state_vector_modeling_tools"><a href="#quantum_optics_and_state_vector_modeling_tools" class="header-anchor">Quantum Optics and State Vector Modeling Tools</a></h1>
<p>The most common way to represent and model quantum states is the state vector formalism &#40;underlying Schroedinger&#39;s and Heisenberg&#39;s equations as well as many other master equations&#41;. The <a href="https://github.com/qojulia/QuantumOptics.jl">QuantumOptics.jl</a> Julia project enables such simulations, utilizing much of the uniquely powerful DiffEq infrastructure in Julia.</p>
<h2 id="gpu_accelerated_operators_and_ode_solvers"><a href="#gpu_accelerated_operators_and_ode_solvers" class="header-anchor">GPU accelerated operators and ODE solvers</a></h2>
<p>Much of the internal representation of quantum states in QuantumOptics.jl relies on standard dense arrays. Thanks to the multiple-dispatch nature of Julia, much of these objects can already work well with GPU arrays. This project would involve a thorough investigation and validation of the current interfaces to make sure they work well with GPU arrays. In particular, attention will have to be paid to the &quot;lazy&quot; operators as special kernels might need to be implemented for them.</p>
<p><strong>Recommended skills:</strong> Familiarity with performance profiling tools in Julia and Julia&#39;s GPU stack, potentially including <a href="https://github.com/JuliaGPU/KernelAbstractions.jl">KernelAbstractions</a>.</p>
<p><strong>Mentors:</strong> <a href="https://github.com/Krastanov">Stefan Krastanov</a> <a href="mailto:stefan@krastanov.org">&lt;stefan@krastanov.org&gt;</a> and QuantumOptics.jl team members</p>
<p><strong>Expected duration:</strong> 175 hours &#40;but applicants can scope it as longer if they plan more extensive work&#41;</p>
<p><strong>Difficulty:</strong> Medium</p>
<h2 id="autodifferentiation"><a href="#autodifferentiation" class="header-anchor">Autodifferentiation</a></h2>
<p>Autodifferentiation is the capability of automatically generating efficient code to evaluate the numerical derivative of a given Julia function. Similarly to the GPU case above, much of this functionality already &quot;magically&quot; works, but there is no detailed test suite for it and no validation has been done. This project would involve implementing, validating, and testing the use of Julia autodiff tools in QuantumOptics.jl. ForwardDiff, Enzyme, Zygote, Diffractor, and AbstractDifferentiation are all tools that should have some level of validation and support, both in ODE solving and in simple operator applications.</p>
<p><strong>Recommended skills:</strong> Familiarity with the <a href="https://juliadiff.org/">Julia autodiff stack</a> and the SciML sensitivity analysis tooling. Familiarity with the difficulties to autodiff complex numbers &#40;in general and specifically in Julia&#41;. Understanding of the AbstractDifferentiation.jl package.</p>
<p><strong>Mentors:</strong> <a href="https://github.com/Krastanov">Stefan Krastanov</a> <a href="mailto:stefan@krastanov.org">&lt;stefan@krastanov.org&gt;</a> and QuantumOptics.jl team members</p>
<p><strong>Expected duration:</strong> 175 hours &#40;but applicants can scope it as longer if they plan more extensive work&#41;</p>
<p><strong>Difficulty:</strong> Easy-to-Medium</p>
<h2 id="closer_integration_with_the_sciml_ecosystem"><a href="#closer_integration_with_the_sciml_ecosystem" class="header-anchor">Closer Integration with the SciML Ecosystem</a></h2>
<p>SciML is the umbrella organization for much of the base numerical software development in the Julia ecosystem. We already use many of their capabilities, but it would be beneficial to more closely match the interfaces they expect. This project would be heavily on the software <strong>engineering</strong> side. Formal and informal interfaces we want to support include: better support for <a href="https://github.com/qojulia/QuantumOptics.jl/issues/298">DiffEq problem types</a> &#40;currently we wrap DiffEq problems in our own infrastructure and it is difficult to reuse them in SciML&#41;; better support for broadcast operations over state objects &#40;so that we can treat them closer to normal arrays and <a href="https://github.com/qojulia/QuantumOpticsBase.jl/pull/16">we can simply provide an initial state to a DiffEq solver without having to wrap/unwrap the data</a>&#41;; relying more heavily on <a href="https://docs.sciml.ai/SciMLOperators/stable/">SciMLOperators</a> which have significant <a href="https://github.com/qojulia/QuantumOpticsBase.jl/issues/99">overlap with our lazy operators</a>.</p>
<p><strong>Recommended skills:</strong> Familiarity with the SciML stack.</p>
<p><strong>Mentors:</strong> <a href="https://github.com/Krastanov">Stefan Krastanov</a> <a href="mailto:stefan@krastanov.org">&lt;stefan@krastanov.org&gt;</a> and QuantumOptics.jl team members</p>
<p><strong>Expected duration:</strong> 175 hours &#40;but applicants can scope it as longer if they plan more extensive work&#41;</p>
<p><strong>Difficulty:</strong> Easy</p>
<h1 id="symbolic_computation_project_ideas"><a href="#symbolic_computation_project_ideas" class="header-anchor">Symbolic computation project ideas</a></h1>
<h2 id="efficient_tensor_differentiation"><a href="#efficient_tensor_differentiation" class="header-anchor">Efficient Tensor Differentiation</a></h2>
<p>Implement the <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/main-65.pdf">D* algorithm</a> for tensor expressions.</p>
<p><strong>Recommended Skills</strong>: High school/freshman calculus and basic graph theory &#40;optional&#41;</p>
<p><strong>Expected Results</strong>: A working implementation of the D* algorithm that is capable of performing efficient differentiations on tensor expressions.</p>
<p><strong>Mentors</strong>: <a href="https://github.com/YingboMa">Yingbo Ma</a></p>
<p><strong>Duration</strong>: 350 hours</p>
<h2 id="symbolic_root_finding"><a href="#symbolic_root_finding" class="header-anchor">Symbolic root finding</a></h2>
<p>Symbolics.jl have robust ways to convert symbolic expressions into multi-variate polynomials. There is now a robust Groebner basis implementation in &#40;Groebner.jl&#41;. Finding roots and varieties of sets of polynomials would be extremely useful in many applications. This project would involve implementing various techniques for solving polynomial systems, and where possible other non-linear equation systems. A good proposal should try to enumerate a number of techniques that are worth implementing, for example:</p>
<ol>
<li><p>Analytical solutions for polynomial systems of degree &lt;&#61; 4</p>
</li>
<li><p>Use of HomotopyContinuations.jl for testing for solvability and finding numerical solutions</p>
</li>
<li><p>Newton-raphson methods</p>
</li>
<li><p>Using Groebner basis computations to find varieties</p>
</li>
</ol>
<p>The API for these features should be extremely user-friendly:</p>
<ol>
<li><p>A single <code>roots</code> function should take the sets of equations and result in the right type of roots as output &#40;either varieties or numerical answers&#41;</p>
</li>
<li><p>It should automatically find the fastest strategy to solve the set of equations and apply it.</p>
</li>
<li><p>It should fail with descriptive error messages when equations are not solvable, or degenerate in some way.</p>
</li>
<li><p>This should allow implementing symbolic eigenvalue computation when <code>eigs</code> is called.</p>
</li>
</ol>
<p><strong>Mentors</strong>: <a href="https://github.com/shashi">Shashi Gowda</a>, <a href="https://github.com/sumiya11">Alexander Demin</a> <strong>Duration</strong>: 350 hours</p>
<h2 id="symbolic_integration_in_symbolicsjl"><a href="#symbolic_integration_in_symbolicsjl" class="header-anchor">Symbolic Integration in Symbolics.jl</a></h2>
<p>Implement the <a href="https://dspace.mit.edu/handle/1721.1/11997">heuristic approach to symbolic integration</a>. Then hook into a repository of rules such as <a href="https://rulebasedintegration.org/">RUMI</a>. See also the potential of using symbolic-numeric integration techniques &#40;https://github.com/SciML/SymbolicNumericIntegration.jl&#41;</p>
<p><strong>Recommended Skills</strong>: High school/Freshman Calculus</p>
<p><strong>Expected Results</strong>: A working implementation of symbolic integration in the Symbolics.jl library, along with documentation and tutorials demonstrating its use in scientific disciplines.</p>
<p><strong>Mentors</strong>: <a href="https://github.com/shashi">Shashi Gowda</a>, <a href="https://github.com/YingboMa">Yingbo Ma</a></p>
<p><strong>Duration</strong>: 350 hours</p>
<h2 id="xla-style_optimization_from_symbolic_tracing"><a href="#xla-style_optimization_from_symbolic_tracing" class="header-anchor">XLA-style optimization from symbolic tracing</a></h2>
<p>Julia functions that take arrays and output arrays or scalars can be traced using Symbolics.jl variables to produce a trace of operations. This output can be optimized to use fused operations or call highly specific NNLib functions. In this project you will trace through Flux.jl neural-network functions and apply optimizations on the resultant symbolic expressions. This can be mostly implemented as rule-based rewriting rules &#40;see https://github.com/JuliaSymbolics/Symbolics.jl/pull/514&#41;.</p>
<p><strong>Recommended Skills</strong>: Knowledge of space and time complexities of array operations, experience in optimizing array code.</p>
<p><strong>Mentors</strong>: <a href="https://github.com/shashi">Shashi Gowda</a></p>
<p><strong>Duration</strong>: 175 hours</p>
<h2 id="automatically_improving_floating_point_accuracy_herbie"><a href="#automatically_improving_floating_point_accuracy_herbie" class="header-anchor">Automatically improving floating point accuracy &#40;Herbie&#41;</a></h2>
<p><a href="https://herbie.uwplse.org/">Herbie</a> documents a way to optimize floating point functions so as to reduce instruction count while reorganizing operations such that floating point inaccuracies do not get magnified. It would be a great addition to have this written in Julia and have it work on Symbolics.jl expressions. An ideal implementation would use the e-graph facilities of Metatheory.jl to implement this.</p>
<p><strong>Mentors</strong>: <a href="https://github.com/shashi">Shashi Gowda</a>, <a href="https://github.com/0x0f0f0f">Alessandro Cheli</a></p>
<p><strong>Duration</strong>: 350 hours</p>
<h1 id="tabular_data_summer_of_code"><a href="#tabular_data_summer_of_code" class="header-anchor">Tabular Data – Summer of Code</a></h1>
<h2 id="parquetjl_enhancements"><a href="#parquetjl_enhancements" class="header-anchor">Parquet.jl enhancements</a></h2>
<p><strong>Difficulty</strong>: Medium</p>
<p><strong>Duration</strong>: 175 hours</p>
<p><a href="https://parquet.apache.org/">Apache Parquet</a> is a binary data format for tabular data. It has features for compression and memory-mapping of datasets on disk. A decent implementation of Parquet in Julia is likely to be highly performant. It will be useful as a standard format for distributing tabular data in a binary format. There exists a Parquet.jl package that has a Parquet reader and a writer. It currently conforms to the Julia Tabular file IO interface at a very basic level. It needs more work to add support for critical elements that would make Parquet.jl usable for fast large scale parallel data processing. Each of these goals can be targeted as a single, short duration &#40;175 hrs&#41; project.</p>
<div class="tight-list"><ul>
<li><p>Lazy loading and support for out-of-core processing, with Arrow.jl and Tables.jl integration. Improved usability and performance of Parquet reader and writer for large files.</p>
</li>
<li><p>Reading from and writing data on to cloud data stores, including support for partitioned data.</p>
</li>
<li><p>Support for missing data types and encodings making the Julia implementation fully featured.</p>
</li>
</ul></div>
<p><strong>Resources:</strong></p>
<div class="tight-list"><ul>
<li><p>The <a href="https://parquet.apache.org/documentation/latest/">Parquet</a> file format &#40;also are many articles and talks on the Parquet storage format on the internet&#41;</p>
</li>
<li><p><a href="https://quinnj.home.blog/2019/07/21/a-tour-of-the-data-ecosystem-in-julia/">A tour of the data ecosystem in Julia</a></p>
</li>
<li><p><a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a></p>
</li>
<li><p><a href="https://github.com/JuliaData/Arrow.jl">Arrow.jl</a></p>
</li>
</ul></div>
<p><strong>Recommended skills:</strong> Good knowledge of Julia language, Julia data stack and writing performant Julia code.</p>
<p><strong>Expected Results:</strong> Depends on the specific projects we would agree on.</p>
<p><strong>Mentors:</strong> <a href="https://github.com/tanmaykm">Tanmay Mohapatra</a></p>
<h2 id="dataframesjl_join_enhancements"><a href="#dataframesjl_join_enhancements" class="header-anchor">DataFrames.jl join enhancements</a></h2>
<p><strong>Difficulty</strong>: Hard</p>
<p><strong>Duration</strong>: 175 hours</p>
<p><a href="https://github.com/JuliaData/DataFrames.jl">DataFrames.jl</a> is one of the more popular implementations of tabular data type for Julia. One of the features it supports is data frame joining. However, more work is needed to improve this functionality. The specific targets for this project are &#40;a final list of targets included in the scope of the project can be decided later&#41;.</p>
<div class="tight-list"><ul>
<li><p>fully implement multi-threading support by joins, reduce memory requirements of used join algorithms &#40;which should additionally improve their performance&#41;, verify efficiency of alternative joining strategies in comparison to those currently used and implement them along with adaptive algorithm choosing the right joining strategy depending on the passed data;</p>
</li>
<li><p>implement join allowing for efficient matching on non-equal keys; special attention should be made to matching on keys that are date/time and spatial objects;</p>
</li>
<li><p>implement join allowing for an in-place update of columns of one data frame by values stored in another data frame based on matching key and condition specifying when an update should be performed;</p>
</li>
<li><p>implement an more flexible mechanizm than currently available allowing to define output data frame column names when performing a join.</p>
</li>
</ul></div>
<p><strong>Resources:</strong></p>
<div class="tight-list"><ul>
<li><p><a href="https://github.com/JuliaData/DataFrames.jl">DataFrames.jl</a></p>
</li>
<li><p><a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a></p>
</li>
<li><p><a href="https://github.com/JuliaData/DataAPI.jl">DataAPI.jl</a></p>
</li>
</ul></div>
<p><strong>Recommended skills:</strong> Good knowledge of Julia language, Julia data stack and writing performant multi-threaded Julia code. Experience with benchmarking code and writing tests. Knowledge of join algorithms &#40;as e.g. used in databases like <a href="https://duckdb.org/">DuckDB</a> or other tabular data manipulation ecosystems e.g. <a href="https://www.pola.rs/">Polars</a> or <a href="https://github.com/Rdatatable/data.table">data.table</a>&#41;.</p>
<p><strong>Expected Results:</strong> Depends on the specific projects we would agree on.</p>
<p><strong>Mentors:</strong> <a href="https://github.com/bkamins">Bogumił Kamiński</a></p>
<h1 id="taija_projects"><a href="#taija_projects" class="header-anchor">Taija Projects</a></h1>
<p><a href="https://github.com/JuliaTrustworthyAI">Taija</a> is an organization that hosts software geared towards Trustworthy Artificial Intelligence in Julia. Taija currently covers a range of approaches towards making AI systems more trustworthy:</p>
<ul>
<li><p>Model Explainability &#40;<a href="https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl">CounterfactualExplanations.jl</a>&#41;</p>
</li>
<li><p>Algorithmic Recourse &#40;<a href="https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl">CounterfactualExplanations.jl</a>, <a href="https://github.com/JuliaTrustworthyAI/AlgorithmicRecourseDynamics.jl">AlgorithmicRecourseDynamics.jl</a>&#41;</p>
</li>
<li><p>Predictive Uncertainty Quantification &#40;<a href="https://github.com/JuliaTrustworthyAI/ConformalPrediction.jl">ConformalPrediction.jl</a>, <a href="https://github.com/JuliaTrustworthyAI/LaplaceRedux.jl">LaplaceRedux.jl</a>&#41;</p>
</li>
<li><p>Effortless Bayesian Deep Learning &#40;<a href="https://github.com/JuliaTrustworthyAI/LaplaceRedux.jl">LaplaceRedux.jl</a>&#41;</p>
</li>
<li><p>Hybrid Learning &#40;<a href="https://github.com/JuliaTrustworthyAI/JointEnergyModels.jl">JointEnergyModels.jl</a>&#41;</p>
</li>
</ul>
<p>Various meta packages can be used to extend the core functionality:</p>
<ul>
<li><p>Plotting &#40;<a href="https://github.com/JuliaTrustworthyAI/TaijaPlotting.jl">TaijaPlotting.jl</a>&#41;</p>
</li>
<li><p>Datasets for testing and benchmarking &#40;<a href="https://github.com/JuliaTrustworthyAI/TaijaData.jl">TaijaData.jl</a>&#41;</p>
</li>
<li><p>Interoperability with other programming languages &#40;<a href="https://github.com/JuliaTrustworthyAI/TaijaInteroperability.jl">TaijaInteroperability.jl</a>&#41;</p>
</li>
</ul>
<p>There is a high overlap with organizations, you might be also interested in:</p>
<ul>
<li><p><a href="https://julialang.org/jsoc/gsoc/MLJ/">Projects with MLJ.jl</a> - For more traditional machine learning projects</p>
</li>
<li><p><a href="https://fluxml.ai/gsoc/">Projects with FluxML</a> - For projects around Flux.jl, the backbone of Julia&#39;s deep learning ecosystem</p>
</li>
</ul>
<h2 id="project_1_conformal_prediction_meets_bayes_predictive_uncertainty"><a href="#project_1_conformal_prediction_meets_bayes_predictive_uncertainty" class="header-anchor">Project 1: Conformal Prediction meets Bayes &#40;<em>Predictive Uncertainty</em>&#41;</a></h2>
<p><strong>Project Overview:</strong> <a href="https://github.com/JuliaTrustworthyAI/ConformalPrediction.jl">ConformalPrediction.jl</a> is a package for Predictive Uncertainty Quantification through Conformal Prediction for Machine Learning models trained in MLJ. This project aims to enhance ConformalPrediction.jl by adding support for <a href="https://github.com/JuliaTrustworthyAI/ConformalPrediction.jl/issues/64">Conformal&#40;ized&#41; Bayes</a>. </p>
<p><strong>Mentor:</strong> <a href="https://github.com/pat-alt">Patrick Altmeyer</a> and/or <a href="https://nl.linkedin.com/in/mfarmanbar">Mojtaba Farmanbar</a></p>
<p><strong>Project Difficulty</strong>: Medium</p>
<p><strong>Estimated Duration</strong>: 175 hours</p>
<p><strong>Ideal Candidate Profile:</strong></p>
<ul>
<li><p>Basic knowledge of Julia or strong knowledge of similar programming languages &#40;R, Python, MATLAB, ...&#41;</p>
</li>
<li><p>Good understanding of Bayesian methods</p>
</li>
<li><p>Basic knowledge of Conformal Prediction</p>
</li>
</ul>
<p><strong>Project Goals and Deliverables:</strong></p>
<ul>
<li><p>Implement support for conformalizing predictive distributions &#40;<a href="https://github.com/JuliaTrustworthyAI/ConformalPrediction.jl/issues/109">#109</a>&#41;</p>
</li>
<li><p>Implement support for Conformal Bayes through Add-One-In Importance Sampling &#40;<a href="https://github.com/JuliaTrustworthyAI/ConformalPrediction.jl/issues/110">#110</a>&#41;</p>
</li>
<li><p>Implement other recent approaches combining Bayes with Conformal Prediction that you find interesting</p>
</li>
<li><p>Comprehensively test and document your work</p>
</li>
</ul>
<h2 id="project_2_counterfactual_regression_model_explainability"><a href="#project_2_counterfactual_regression_model_explainability" class="header-anchor">Project 2: Counterfactual Regression &#40;<em>Model Explainability</em>&#41;</a></h2>
<p><strong>Project Overview:</strong> <a href="https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl">CounterfactualExplanations.jl</a> is a package for Counterfactual Explanations and Algorithmic Recourse in Julia. This project aims to extend the package functionality to <a href="https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl/issues/388">regression models</a>. </p>
<p><strong>Mentor:</strong> <a href="https://github.com/pat-alt">Patrick Altmeyer</a></p>
<p><strong>Project Difficulty</strong>: Hard</p>
<p><strong>Estimated Duration</strong>: 350 hours</p>
<p><strong>Ideal Candidate Profile:</strong></p>
<ul>
<li><p>Experience with Julia and multiple dispatch of advantage, but not crucial</p>
</li>
<li><p>Good knowledge of machine learning and statistics</p>
</li>
<li><p>Solid understanding of supervised models &#40;classification and regression&#41;</p>
</li>
</ul>
<p><strong>Project Goals and Deliverables:</strong></p>
<ul>
<li><p>Carefully think about architecture choices: how can we fit support for regression models into the existing code base?</p>
</li>
<li><p>Add support for the following approaches: <a href="https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl/issues/391">ad-hoc thresholding</a>, <a href="https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl/issues/392">Bayesian optimisation</a>, <a href="https://openreview.net/forum?id&#61;IrEYkhuxup&amp;noteId&#61;IrEYkhuxup">information-theoretic saliency</a>.</p>
</li>
<li><p>Comprehensively test and document your work</p>
</li>
</ul>
<h2 id="project_3_counterfactuals_for_llms_model_explainability_and_generative_ai"><a href="#project_3_counterfactuals_for_llms_model_explainability_and_generative_ai" class="header-anchor">Project 3: Counterfactuals for LLMs &#40;<em>Model Explainability</em> and <em>Generative AI</em>&#41;</a></h2>
<p><strong>Project Overview:</strong> This project aims to extend the functionality of <a href="https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl">CounterfactualExplanations.jl</a> to Large Language Models &#40;LLMs&#41;. As a backbone for this, support for computing feature attributions for LLMs will also need to be implemented. The project will contribute to both <a href="https://github.com/JuliaTrustworthyAI">Taija</a> and <a href="https://github.com/JuliaGenAI">JuliaGenAI</a>. </p>
<p><strong>Mentor:</strong> <a href="https://github.com/pat-alt">Patrick Altmeyer</a> &#40;Taija&#41; and <a href="https://github.com/svilupp">Jan Siml</a>  &#40;JuliaGenAI&#41;</p>
<p><strong>Project Difficulty</strong>: Medium</p>
<p><strong>Estimated Duration</strong>: 175 hours</p>
<p><strong>Ideal Candidate Profile:</strong></p>
<ul>
<li><p>Experience with Julia and multiple dispatch of advantage, but not crucial</p>
</li>
<li><p>Good knowledge of machine learning and statistics</p>
</li>
<li><p>Good understanding of Large Language Models &#40;LLMs&#41;</p>
</li>
<li><p>Ideally previous experience with <a href="https://github.com/chengchingwen/Transformers.jl">Transformers.jl</a></p>
</li>
</ul>
<p><strong>Project Goals and Deliverables:</strong></p>
<ul>
<li><p>Carefully think about architecture choices: how can we fit support for LLMs into the existing code base of <a href="https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl">CounterfactualExplanations.jl</a>?</p>
</li>
<li><p>Implement current state-of-the-art approaches such as <a href="https://aclanthology.org/2021.findings-acl.336.pdf">MiCE</a> and <a href="https://aclanthology.org/2022.findings-emnlp.216.pdf">CORE</a></p>
</li>
<li><p>Comprehensively test and document your work</p>
</li>
</ul>
<h2 id="project_4_from_counterfactuals_to_interventions_recourse_through_minimal_causal_interventions"><a href="#project_4_from_counterfactuals_to_interventions_recourse_through_minimal_causal_interventions" class="header-anchor">Project 4: From Counterfactuals to Interventions &#40;Recourse through Minimal Causal Interventions&#41;</a></h2>
<p><strong>Project Overview:</strong> This extension aims to enhance the CounterfactualExplanations.jl package by incorporating a module for generating actionable recourse through minimal causal interventions.</p>
<p><strong>Mentor:</strong> <a href="https://github.com/pat-alt">Patrick Altmeyer</a> &#40;Taija&#41; and <a href="https://github.com/mschauer">Moritz Schauer</a> &#40;CausalInference.jl&#41;</p>
<p><strong>Project Difficulty:</strong> Hard</p>
<p><strong>Estimated Duration:</strong> 350 hours</p>
<p><strong>Ideal Candidate Profile:</strong></p>
<ul>
<li><p>Experience with Julia</p>
</li>
<li><p>Background in causality and familiarity with counterfactual reasoning.</p>
</li>
<li><p>Basic knowledge of minimal interventions and causal graph building.</p>
</li>
</ul>
<p><strong>Project Goals and Deliverables:</strong></p>
<ul>
<li><p>Carefully think about architecture choices: how can we fit support for causal interventions into the existing code base?</p>
</li>
<li><p>Develop code that could integrate causal graph building with other Julia libs such as <a href="https://github.com/JuliaGraphs/Graphs.jl">Graphs.jl</a>, <a href="https://juliagraphs.org/GraphPlot.jl/">GraphPlot.jl</a> and <a href="https://github.com/mschauer/CausalInference.jl">CausalInference.jl</a>.</p>
</li>
<li><p>Implement current state-of-the-art approaches for minimal interventions using structured causal models &#40;SCMs&#41;.</p>
</li>
<li><p>Comprehensively test and document your work.</p>
</li>
</ul>
<h2 id="about_us"><a href="#about_us" class="header-anchor">About Us</a></h2>
<p><a href="https://www.paltmeyer.com/">Patrick Altmeyer</a> is a PhD Candidate in Trustworthy Artificial Intelligence at Delft University of Technology working on the intersection of Computer Science and Finance. He has presented work related to Taija at JuliaCon 2022 and 2023. In the past year, Patrick has mentored multiple groups of students at Delft University of Technology who have made major contributions to Taija. </p>
<h2 id="how_to_contact_us__2"><a href="#how_to_contact_us__2" class="header-anchor">How to Contact Us</a></h2>
<p>We&#39;d love to hear your ideas and discuss potential projects with you.</p>
<p>Probably the easiest way is to join our <a href="https://julialang.org/slack/">JuliaLang Slack</a> and join the <code>#taija</code> channel. You can also post a GitHub Issue on our organization <a href="https://github.com/JuliaTrustworthyAI/.github/issues">repo</a>.</p>
<p><a href="https://github.com/JuliaTrustworthyAI"><img src="https://raw.githubusercontent.com/TrustworthyAIJulia/.github/main/profile/www/wide_logo.png" alt="Taija Logo" /></a></p>
<h1 id="topopt_projects_summer_of_code"><a href="#topopt_projects_summer_of_code" class="header-anchor">TopOpt Projects – Summer of Code</a></h1>
<p><a href="https://github.com/JuliaTopOpt/TopOpt.jl">TopOpt.jl</a> is a <a href="https://en.wikipedia.org/wiki/Topology_optimization">topology optimization</a> package written in pure Julia. Topology optimization is an exciting field at the intersection of shape representation, physics simulations and mathematical optimization, and the Julia language is a great fit for this field. To learn more about <code>TopOpt.jl</code>, check the following <a href="https://www.youtube.com/watch?v&#61;sBqdkxPXluU">JuliaCon talk</a>.</p>
<p>The following is a tentative list of projects in topology optimization that you could be working on in the coming Julia Season of Contributions or Google Summer of Code. If you are interested in exploring any of these topics or if you have other interests related to topology optimization, please reach out to the main mentor <a href="https://github.com/mohamed82008">Mohamed Tarek</a> via email.</p>
<h2 id="testing_and_benchmarking_of_topoptjl"><a href="#testing_and_benchmarking_of_topoptjl" class="header-anchor">Testing and benchmarking of TopOpt.jl</a></h2>
<p><strong>Project difficulty</strong>: Medium</p>
<p><strong>Work load</strong>: 350 hours</p>
<p><strong>Description</strong>: The goal of this project is to improve the unit test coverage and reliability of TopOpt.jl by testing its implementations against other software&#39;s outputs. Testing and benchmarking stress and buckling constraints and their derivatives will be the main focus of this project. Matlab scripts from papers may have to be translated to Julia for correctness and performance comparison.</p>
<p><strong>Knowledge prerequisites</strong>: structural mechanics, optimization, Julia programming</p>
<h2 id="machine_learning_in_topology_optimization"><a href="#machine_learning_in_topology_optimization" class="header-anchor">Machine learning in topology optimization</a></h2>
<p><strong>Project difficulty</strong>: Medium</p>
<p><strong>Work load</strong>: 350 hours</p>
<p><strong>Description</strong>: There are numerous ways to use machine learning for design optimization in topology optimization. The following are all recent papers with applications of neural networks and machine learning in topology optimization. There are also exciting research opportunities in this direction.</p>
<ul>
<li><p><a href="https://openreview.net/pdf?id&#61;DUy-qLzqvlU">DNN-based Topology optimization: Spatial Invariance and Neural Tangent Kernel</a></p>
</li>
<li><p><a href="https://openreview.net/pdf?id&#61;bBHHU4dW88g">NTopo: Mesh-free Topology Optimization using Implicit Neural Representations</a></p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S004578252100414X?via&#37;3Dihub">TONR: An exploration for a novel way combining neural network with topology optimization</a></p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s00158-020-02748-4">TOuNN: Topology Optimization using Neural Networks</a></p>
</li>
</ul>
<p>In this project you will implement one of the algorithms discussed in any of these papers.</p>
<p><strong>Knowledge prerequisites</strong>: neural networks, optimization, Julia programming</p>
<h2 id="optimization_on_a_uniform_rectilinear_grid"><a href="#optimization_on_a_uniform_rectilinear_grid" class="header-anchor">Optimization on a uniform rectilinear grid</a></h2>
<p><strong>Project difficulty</strong>: Medium</p>
<p><strong>Work load</strong>: 350 hours</p>
<p><strong>Description</strong>: Currently in TopOpt.jl, there are only unstructured meshes supported. This is a very flexible type of mesh but it&#39;s not as memory efficient as uniform rectilinear grids where all the elements are assumed to have the same shape. This is the most common grid used in topology optimization in practice. Currently in TopOpt.jl, the uniform rectilinear grid will be stored as an unstructured mesh which is unnecessarily inefficient. In this project, you will optimize the finite element analysis and topology optimization codes in TopOpt.jl for uniform rectilinear grids.</p>
<p><strong>Knowledge prerequisites</strong>: knowledge of mesh types, Julia programming</p>
<h2 id="adaptive_mesh_refinement_for_topology_optimization"><a href="#adaptive_mesh_refinement_for_topology_optimization" class="header-anchor">Adaptive mesh refinement for topology optimization</a></h2>
<p><strong>Project difficulty</strong>: Medium</p>
<p><strong>Work load</strong>: 350 hours</p>
<p><strong>Description</strong>: Topology optimization problems with more mesh elements take longer to simulate and to optimize. In this project, you will explore the use of adaptive mesh refinement starting from a coarse mesh, optimizing and only refining the elements that need further optimization. This is an effective way to accelerate topology optimization algorithms.</p>
<p><strong>Knowledge prerequisites</strong>: adaptive mesh refinement, Julia programming</p>
<h2 id="heat_transfer_design_optimization"><a href="#heat_transfer_design_optimization" class="header-anchor">Heat transfer design optimization</a></h2>
<p><strong>Project difficulty</strong>: Medium</p>
<p><strong>Work load</strong>: 175 or 350 hours</p>
<p><strong>Description</strong>: All of the examples in TopOpt.jl and problem types are currently of the linear elasticity, quasi-static class of problems. The goal of this project is to implement more problem types and examples from the field of heat transfer. Both steady-state heat transfer problems and linear elasticity problems make use of elliptic partial differential equations so the code from linear elasticity problems should be largely reusable for heat transfer problems with minimum changes.</p>
<p><strong>Knowledge prerequisites</strong>: finite element analysis, heat equation, Julia programming</p>
<h1 id="modern_computational_fluid_dynamics_with_trixijl"><a href="#modern_computational_fluid_dynamics_with_trixijl" class="header-anchor">Modern computational fluid dynamics with Trixi.jl</a></h1>
<p><a href="https://github.com/trixi-framework/Trixi.jl/">Trixi.jl</a> is a Julia package for adaptive  high-order numerical simulations of conservation laws. It is designed to be simple to use for students and researchers, extensible for research and teaching, as well as efficient  and suitable for high-performance computing.</p>
<h2 id="compiler-based_automatic_differentiation_with_enzymejl"><a href="#compiler-based_automatic_differentiation_with_enzymejl" class="header-anchor">Compiler-based automatic differentiation with Enzyme.jl</a></h2>
<p><strong>Difficulty</strong>: Medium &#40;up to hard, depending on the chosen subtasks&#41;</p>
<p><strong>Project size</strong>: 175 hours or 350 hours, depending on the chosen subtasks</p>
<p><a href="https://github.com/EnzymeAD/Enzyme.jl">Enzyme.jl</a> is the Julia frontend of Enzyme,  a modern automatic differentiation &#40;AD&#41; framework working at the level of LLVM code.  It can provide fast forward and reverse mode AD and - unlike some other AD packages -  supports mutating operations. Since <a href="https://github.com/trixi-framework/Trixi.jl/">Trixi.jl</a>  relies on mutating operations and caches for performance, this feature is crucial to obtain an implementation that works efficiently for both simulation runs and AD.</p>
<p>The overall goal of this project is to create a working prototype of  <a href="https://github.com/trixi-framework/Trixi.jl/">Trixi.jl</a> &#40;or a subset thereof&#41;  using <a href="https://github.com/EnzymeAD/Enzyme.jl">Enzyme.jl</a> for AD, and to support as  many of Trixi&#39;s advanced features as possible, such as adaptive mesh refinement, shock capturing etc.</p>
<p>Possible subtasks in this project include</p>
<ul>
<li><p>Explore and implement forward/backward mode AD via <a href="https://github.com/EnzymeAD/Enzyme.jl">Enzyme.jl</a>  for a simplified simulation for the 1D advection equation or the 1D compressible Euler equations &#40;e.g., compute the Jacobian of the right-hand side evaluation <code>Trixi.rhs&#33;</code> on a simple mesh in serial execution&#41;</p>
</li>
<li><p>Explore and implement forward mode AD via <a href="https://github.com/EnzymeAD/Enzyme.jl">Enzyme.jl</a> of semidiscretizations provided by <a href="https://github.com/trixi-framework/Trixi.jl/">Trixi.jl</a>, mimicking the functionality that is already <a href="https://trixi-framework.github.io/Trixi.jl/stable/tutorials/differentiable_programming/">available via ForwardDiff.jl</a></p>
</li>
<li><p>Explore and implement reverse mode AD via <a href="https://github.com/EnzymeAD/Enzyme.jl">Enzyme.jl</a> of semidiscretizations provided by <a href="https://github.com/trixi-framework/Trixi.jl/">Trixi.jl</a> as required for modern machine learning tasks</p>
</li>
<li><p>Explore and implement AD via <a href="https://github.com/EnzymeAD/Enzyme.jl">Enzyme.jl</a> of full simulations combining semidiscretizations of <a href="https://github.com/trixi-framework/Trixi.jl/">Trixi.jl</a> with time integration methods of <a href="https://github.com/SciML/OrdinaryDiffEq.jl">OrdinaryDiffEq.jl</a></p>
</li>
</ul>
<p>Related subtasks in this project not related directly to <a href="https://github.com/EnzymeAD/Enzyme.jl">Enzyme.jl</a> but using other packages include</p>
<ul>
<li><p>Explore and implement means to improve the current handling of caches to simplify AD and differentiable programming with semidiscretizations of <a href="https://github.com/trixi-framework/Trixi.jl/">Trixi.jl</a> in general, e.g., via <a href="https://github.com/SciML/PreallocationTools.jl">PreallocationTools.jl</a>.</p>
</li>
<li><p>Extend the current AD support based on <a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff.jl</a>  to other functionality of <a href="https://github.com/trixi-framework/Trixi.jl/">Trixi.jl</a>, e.g., <a href="https://github.com/trixi-framework/Trixi.jl/issues/1252">shock capturing discretizations</a>, <a href="https://github.com/trixi-framework/Trixi.jl/issues/910">MPI parallel simulations</a>, and other <a href="https://github.com/trixi-framework/Trixi.jl/issues/462">features currently not supported</a></p>
</li>
</ul>
<p>This project is good for both software engineers interested in the fields of numerical analysis and scientific machine learning as well as those students who are interested in pursuing graduate research in the field.</p>
<p><strong>Recommended skills</strong>: Good knowledge of at least one numerical discretization scheme  &#40;e.g., finite volume, discontinuous Galerkin, finite differences&#41;; initial knowledge  in automatic differentiation; preferably the ability &#40;or eagerness to learn&#41; to write fast code</p>
<p><strong>Expected results</strong>: Contributions to state of the art and production-quality automatic differentiation tools for Trixi.jl</p>
<p><strong>Mentors</strong>: <a href="https://github.com/ranocha">Hendrik Ranocha</a>, <a href="https://github.com/sloede">Michael Schlottke-Lakemper</a></p>
<h2 id="advanced_visualization_and_in-situ_visualization_with_paraview"><a href="#advanced_visualization_and_in-situ_visualization_with_paraview" class="header-anchor">Advanced visualization and in-situ visualization with ParaView</a></h2>
<p><strong>Difficulty</strong>: Medium</p>
<p><strong>Project size</strong>: 175 hours or 350 hours, depending on the chosen subtasks</p>
<p>Visualizing and documenting results is a crucial part of the scientific process. In <a href="https://github.com/trixi-framework/Trixi.jl/">Trixi.jl</a>, we rely for visualization on a combination of pure Julia packages &#40;such as <a href="https://github.com/JuliaPlots/Plots.jl">Plots.jl</a> and <a href="https://github.com/MakieOrg/Makie.jl">Makie.jl</a>&#41; and the open-source scientific visualization suite <a href="https://www.paraview.org">ParaView</a>. While the Julia solutions are excellent for visualizing 1D and 2D data, ParaView is the first choice for creating publication-quality figures from 3D data.</p>
<p>Currently, visualization with ParaView is only possible after a simulation is finished and requires an additional postprocessing step, where the native output files of Trixi.jl are converted to <a href="https://vtk.org">VTK</a> files using <a href="https://github.com/trixi-framework/Trixi2Vtk.jl">Trixi2Vtk.jl</a>. This extra step makes it somewhat inconvenient to use, especially when the current state of a numerical solution is to be checked during a long, multi-hour simulation run.</p>
<p>The goal of this project is therefore to make such visualizations easier by introducing two significant improvements:</p>
<ul>
<li><p>Add the capability to write out native <a href="https://docs.vtk.org/en/latest/design_documents/VTKFileFormats.html#vtkhdf-file-format">VTKHDF</a> files directly during a simulation, in serial and parallel.</p>
</li>
<li><p>Enable parallel in-situ visualization of the results, i.e., to visualize results by connecting ParaView to a currently running, parallel Trixi.jl simulation using the <a href="https://catalyst-in-situ.readthedocs.io/en/latest/index.html">Catalyst API</a>.</p>
</li>
</ul>
<p>Both tasks are related in that they require the student to familiarize themselves with both the data formats internally used in Trixi.jl as well as the visualization pipelines of VTK/ParaView. However, they can be performed independently and thus this project is suitable for either a 175 hour or a 350 hour commitment, depending on whether one or both tasks are to be tackled.</p>
<p>This project is good for both software engineers interested in the fields of visualization and scientific data analysis as well as those students who are interested in pursuing graduate research in the field of numerical analysis and high-performance computing.</p>
<p><strong>Recommended skills</strong>: Some knowledge of at least one numerical discretization scheme &#40;e.g., finite volume, discontinuous Galerkin, finite differences&#41; is helpful; initial knowledge about visualization or parallel processing; preferably the ability &#40;or eagerness to learn&#41; to write fast code.</p>
<p><strong>Expected results</strong>: Scalable, production quality visualization of scientific results for Trixi.jl.</p>
<p><strong>Mentors</strong>: <a href="https://github.com/sloede">Michael Schlottke-Lakemper</a>, <a href="https://www.mi.uni-koeln.de/NumSim/dr-benedict-geihe/">Benedict Geihe</a>, <a href="https://github.com/jmark">Johannes Markert</a></p>
<h1 id="turing_projects_-_summer_of_code"><a href="#turing_projects_-_summer_of_code" class="header-anchor">Turing Projects - Summer of Code</a></h1>
<p><a href="https://turinglang.org/">Turing</a> is a universal probabilistic programming language embedded in Julia. Turing allows the user to write models in standard Julia syntax, and provide a wide range of sampling-based inference methods for solving problems across probabilistic machine learning, Bayesian statistics and data science etc. Since Turing is implemented in pure Julia code, its compiler and inference methods are amenable to hacking: new model families and inference methods can be easily added.</p>
<p>Below is a list of ideas for potential projects, though you are welcome to propose your own to the Turing team. If you are interested in exploring any of these projects, please reach out to the listed project mentors or Xianda Sun &#40;at xs307&#91;at&#93;cam.ac.uk&#41;. You can find their contact information <a href="https://turinglang.org/stable/team">here</a>.</p>
<h2 id="implementing_models_from_posteriordb_in_turing_julia"><a href="#implementing_models_from_posteriordb_in_turing_julia" class="header-anchor">Implementing models from PosteriorDB in Turing / Julia</a></h2>
<p><strong>Mentors:</strong> Seth Axen, Tor Fjelde, Kai Xu, Hong Ge</p>
<p><strong>Project difficulty:</strong> Medium</p>
<p><strong>Project length:</strong> 175 hrs or 350 hrs</p>
<p><strong>Description:</strong> <a href="https://github.com/stan-dev/posteriordb">posteriordb</a> is a database of 120 diverse Bayesian models implemented in Stan &#40;with 1 example model in PyMC&#41; with reference posterior draws, data, and metadata. For performance comparison and for showcasing best practices in Turing, it is useful to have Turing implementations of these models. The goal of this project is to implement a large subset of these models in Turing/Julia.</p>
<p>For each model, we consider the following tasks: Correctness test: when reference posterior draws and sampler configuration are available in posteriordb, correctness of the implementation and consistency can be tested by sampling the model with the same configuration and comparing the samples to the reference draws. Best practices: all models must be checked to be differentiable with all Turing-supported AD frameworks.</p>
<h2 id="improving_the_integration_between_turing_and_turings_mcmc_inference_packages"><a href="#improving_the_integration_between_turing_and_turings_mcmc_inference_packages" class="header-anchor">Improving the integration between Turing and Turing’s MCMC inference packages</a></h2>
<p><strong>Mentors:</strong> Tor Fjelde, Jaime Ruiz Zapatero, Cameron Pfiffer, David Widmann</p>
<p><strong>Project difficulty:</strong> Easy</p>
<p><strong>Project length:</strong> 175 hrs</p>
<p><strong>Description:</strong> Most samplers in Turing.jl implements the AbstractMCMC.jl interface, allowing a unified way for the user to interact with the samplers. The interface of AbstractMCMC.jl is currently very bare-bones and does not lend itself nicely to interoperability between samplers.</p>
<p>For example, it’s completely valid to compose to MCMC kernels, e.g. taking one step using the RWMH from AdvancedMH.jl, followed by taking one step using NUTS from AdvancedHMC.jl. Unfortunately, implementing such a composition requires explicitly defining conversions between the state returned from RWMH and the state returned from NUTS, and conversion of state from NUTS to state of RWMH. Doing this for one such sampler-pair is generally very easy to do, but once you have to do this for N samplers, suddenly the amount of work needed to be done becomes insurmountable.</p>
<p>One way to deal alleviate this issue would be to add a simple interface for interacting with the states of the samplers, e.g. a method for getting the current values in the state, a method for setting the current values in the state, in addition to a set of glue-methods which can be overridden in the specific case where more information can be shared between the states.</p>
<p>As an example of some ongoing work that attempts to take a step in this direction is: <a href="https://github.com/TuringLang/AbstractMCMC.jl/pull/86">https://github.com/TuringLang/AbstractMCMC.jl/pull/86</a></p>
<h2 id="gpu_support_for_normalizingflowsjl_and_bijectorsjl"><a href="#gpu_support_for_normalizingflowsjl_and_bijectorsjl" class="header-anchor">GPU support for NormalizingFlows.jl and Bijectors.jl</a></h2>
<p><strong>Mentors:</strong> Tor Fjelde, Tim Hargreaves, Xianda Sun, Kai Xu, Hong Ge</p>
<p><strong>Project difficulty:</strong> Hard</p>
<p><strong>Project length:</strong> 175 hrs or 350 hrs</p>
<p><strong>Description:</strong> Bijectors.jl, a package that facilitates transformations of distributions within Turing.jl, currently lacks full GPU compatibility. This limitation stems partly from the implementation details of certain bijectors and also from how some distributions are implemented in the Distributions.jl package. NormalizingFlows.jl, a newer addition to the Turing.jl ecosystem built atop Bijectors.jl, offers a user-friendly interface and utility functions for training normalizing flows but shares the same GPU compatibility issues.</p>
<p>The aim of this project is to enhance GPU support for both Bijectors.jl and NormalizingFlows.jl.</p>
<h2 id="batched_support_for_normalizingflowsjl_and_bijectorsjl"><a href="#batched_support_for_normalizingflowsjl_and_bijectorsjl" class="header-anchor">Batched support for NormalizingFlows.jl and Bijectors.jl</a></h2>
<p><strong>Mentors:</strong> Tor Fjelde, Xianda Sun, David Widmann, Hong Ge</p>
<p><strong>Project difficulty:</strong> Medium</p>
<p><strong>Project length:</strong> 350 hrs</p>
<p><strong>Description:</strong> This project aims to introduce a <code>batched mode</code> to Bijectors.jl and NormalizingFlows.jl, which are built on top of Bijectors.jl.</p>
<p>Put simply, we want to enable users to provide multiple inputs to the model simultaneously by “stacking” the parameters into a higher-dimensional array.</p>
<p>The implementation can take various forms, as a team of developers who care about both performance and user experience, we are open to different approaches and discussions. One possible approach is to develop a mechanism that signals the code to process the given input as a batch rather than as individual entries.  A preliminary implementation can be found <a href="https://github.com/torfjelde/Batching.jl">here</a>.</p>
<h2 id="targets_for_benchmarking_samplers_with_vectorization_gpu_and_high-order_derivative_supports"><a href="#targets_for_benchmarking_samplers_with_vectorization_gpu_and_high-order_derivative_supports" class="header-anchor">Targets for Benchmarking Samplers with vectorization, GPU and high-order derivative supports</a></h2>
<p><strong>Mentors:</strong> Kai Xu, Hong Ge</p>
<p><strong>Project difficulty:</strong> Medium</p>
<p><strong>Project length:</strong> 175 hrs</p>
<p><strong>Description:</strong> The project aims to develop a comprehensive collection of target distributions designed to study and benchmark Markov Chain Monte Carlo &#40;MCMC&#41; samplers in various computational environments. This collection will be an extension and enhancement of the existing Julia package, <a href="https://github.com/xukai92/VecTargets.jl">VecTargets.jl</a>, which currently offers limited support for vectorization, GPU acceleration, and high-order derivatives. The main objectives of this project include:</p>
<ul>
<li><p>Ensuring that the target distributions fully support vectorization and GPU acceleration</p>
</li>
<li><p>Making high-order derivatives &#40;up to 3rd order&#41; seamlessly integrable with the target distributions</p>
</li>
<li><p>Creating a clear and comprehensive documentation that outlines the capabilities and limitations of the project, including explicit details on cases where vectorization, GPU acceleration, or high-order derivatives are not supported.</p>
</li>
<li><p>Investigating and documenting how different Automatic Differentiation &#40;AD&#41; packages available in Julia can be combined or utilized to achieve efficient and accurate computation of high-order derivatives.</p>
</li>
</ul>
<p>By achieving these goals, the project aims to offer a robust framework that can significantly contribute to the research and development of more efficient and powerful MCMC samplers, thereby advancing the field of computational statistics and machine learning.</p>
<h1 id="vs_code_projects"><a href="#vs_code_projects" class="header-anchor">VS Code projects</a></h1>
<h2 id="vs_code_extension"><a href="#vs_code_extension" class="header-anchor">VS Code extension</a></h2>
<p>We are generally looking for folks that want to help with the <a href="https://www.julia-vscode.org/">Julia VS Code extension</a>. We have a long list of open issues, and some of them amount to significant projects.</p>
<p><strong>Required Skills</strong>: TypeScript, Julia, web development.</p>
<p><strong>Expected Results</strong>: Depends on the specific projects we would agree on.</p>
<p><strong>Mentors</strong>: <a href="https://github.com/davidanthoff">David Anthoff</a></p>
<h2 id="package_installation_ui"><a href="#package_installation_ui" class="header-anchor">Package installation UI</a></h2>
<p>The VSCode extension for Julia could provide a simple way to browse available packages and view what&#39;s installed on a users system. To start with, this project could simply provide a GUI that reads in package data from a <code>Project.toml</code>/<code>Manifest.toml</code> and show some UI elements to add/remove/manage those packages.</p>
<p>This could also be extended by having metadata about the package, such as a readme, github stars, activity and so on &#40;somewhat similar to the VSCode-native extension explorer&#41;.</p>
<p><strong>Expected Results</strong>: A UI in VSCode for package operations.</p>
<p><strong>Recommended Skills</strong>: Familiarity with TypeScript and Julia development.</p>
<p><strong>Mentors</strong>: <a href="https://github.com/pfitzseb">Sebastian Pfitzner</a></p>
<p><em>Also take a look at <a href="https://julialang.org/jsoc/gsoc/pluto/">Pluto - VS Code integration</a>&#33;</em></p>
<h1 id="web_platform_projects_summer_of_code"><a href="#web_platform_projects_summer_of_code" class="header-anchor">Web Platform Projects – Summer of Code</a></h1>
<p>Julia has early support for targeting WebAssembly and running in the web browser. Please note that this is a rapidly moving area &#40;see the <a href="https://github.com/Keno/julia-wasm">project repository</a> for a more detailed overview&#41;, so if you are interested in this work, please make sure to inform yourself of the current state and talk to us to scope out an appropriate project. The below is intended as a set of possible starting points.</p>
<p>Mentor for these projects is <a href="https://github.com/Keno">Keno Fischer</a> unless otherwise stated.</p>
<h2 id="code_generation_improvements_and_async_abi"><a href="#code_generation_improvements_and_async_abi" class="header-anchor">Code generation improvements and async ABI</a></h2>
<p>Because Julia relies on an asynchronous task runtime and WebAssembly currently lacks native support for stack management, Julia needs to explicitly manage task stacks in the wasm heap and perform a compiler transformation to use this stack instead of the native WebAssembly stack. The overhead of this transformation directly impacts the performance of Julia on the wasm platform. Additionally, since all code Julia uses &#40;including arbitrary C/C&#43;&#43; libraries&#41; must be compiled using this transformation, it needs to cover a wide variety of inputs and be coordinated with other users having similar needs &#40;e.g. the Pyodide project to run python on the web&#41;. The project would aim to improve the quality, robustness and flexibility of this transformation.</p>
<p><strong>Recommended Skills</strong>: Experience with LLVM.</p>
<h2 id="wasm_threading"><a href="#wasm_threading" class="header-anchor">Wasm threading</a></h2>
<p>WebAssembly is in the process of standardizing <a href="https://github.com/WebAssembly/threads">threads</a>. Simultaneously, work is ongoing to introduce a new threading runtime in Julia &#40;see <a href="https://github.com/JuliaLang/julia/pull/22631">#22631</a> and replated PRs&#41;. This project would investigate enabling threading support for Julia on the WebAssembly platform, implementing runtime parallel primitives on the web assembly platform and ensuring that high level threading constructs are correctly mapped to the underlying platform. Please note that both the WebAssembly and Julia threading infrastructure is still in active development and may continue to change over the duration of the project. An informed understanding of the state of these projects is a definite prerequisite for this project.</p>
<p><strong>Recommended Skills</strong>: Experience with C and multi-threaded programming.</p>
<h2 id="high_performance_low-level_integration_of_js_objects"><a href="#high_performance_low-level_integration_of_js_objects" class="header-anchor">High performance, Low-level integration of js objects</a></h2>
<p>WebAssembly is in the process of adding <a href="https://github.com/WebAssembly/reference-types">first class references to native objects</a> to their specification. This capability should allow very high performance integration between julia and javascript objects. Since it is not possible to store references to javascript objects in regular memory, adding this capability will require several changes to the runtime system and code generation &#40;possibly including at the LLVM level&#41; in order to properly track these references and emit them either as direct references to as indirect references to the reference table.</p>
<p><strong>Recommended Skills</strong>: Experience with C.</p>
<h2 id="dom_integration"><a href="#dom_integration" class="header-anchor">DOM Integration</a></h2>
<p>While Julia now runs on the web platform, it is not yet a language that&#39;s suitable for first-class development of web applications. One of the biggest missing features is integration with and abstraction over more complicated javascript objects and APIs, in particular the DOM. Inspiration may be drawn from similar projects in <a href="https://github.com/koute/stdweb">Rust</a> or other languages.</p>
<p><strong>Recommended Skills</strong>: Experience with writing libraries in Julia, experience with JavaScript Web APIs.</p>
<h2 id="porting_existing_web-integration_packages_to_the_wasm_platform"><a href="#porting_existing_web-integration_packages_to_the_wasm_platform" class="header-anchor">Porting existing web-integration packages to the wasm platform</a></h2>
<p>Several Julia libraries &#40;e.g. WebIO.jl, Escher.jl&#41; provide input and output capabilities for the web platform. Porting these libraries to run directly on the wasm platform would enable a number of existing UIs to automatically work on the web.</p>
<p><strong>Recommended Skills</strong>: Experience with writing libraries in Julia.</p>
<h2 id="native_dependencies_for_the_web"><a href="#native_dependencies_for_the_web" class="header-anchor">Native dependencies for the web</a></h2>
<p>The Julia project uses <a href="https://github.com/JuliaPackaging/BinaryBuilder.jl">BinaryBuilder</a> to provide binaries of native dependencies of julia packages. Experimental support exists to extend this support to the wasm platform, but few packages have been ported. This project would consist of attempting to port a significant fraction of the binary dependencies of the julia ecosystem to the web platform by improving the toolchain support in BinaryBuilder or &#40;if necessary&#41;, porting upstream packages to fix assumptions not applicable on the wasm platform.</p>
<p><strong>Recommended Skills</strong>: Experience with building native libraries in Unix environments.</p>
<h2 id="distributed_computing_with_untrusted_parties"><a href="#distributed_computing_with_untrusted_parties" class="header-anchor">Distributed computing with untrusted parties</a></h2>
<p>The Distributed computing abstractions in Julia provide convenient abstraction for implementing programs that span many communicating Julia processes on different machines. However, the existing abstractions generally assume that all communicating processes are part of the same trust domain &#40;e.g. they allow messages to execute arbitrary code on the remote&#41;. With some of the nodes potentially running in the web browser &#40;or multiple browser nodes being part of the same distributed computing cluster via WebRPC&#41;, this assumption no longer holds true and new interfaces need to be designed to support multiple trust domains without overly restricting usability.</p>
<p><strong>Recommended Skills</strong>: Experience with distributed computing and writing libraries in Julia.</p>
<h2 id="deployment"><a href="#deployment" class="header-anchor">Deployment</a></h2>
<p>Currently supported use cases for Julia on the web platform are primarily geared towards providing interactive environments to support exploration of the full language. Of course, this leads to significantly larger binaries than would be required for using Julia as part of a production deployment. By disabling dynamic language features &#40;e.g. eval&#41; one could generate small binaries suitable for deployment. Some progress towards this exists in packages like <a href="https://github.com/JuliaLang/PackageCompiler.jl">PackageCompiler.jl</a>, though significant work remains to be done.</p>
<p><strong>Recommended Skills</strong>: Interest in or experience with Julia internals.</p>

</div><br><br>

<!-- CONTENT ENDS HERE -->
    
    

    <!-- http://tutsplus.github.io/clipboard/ -->

<script>
(function(){

	// Get the elements.
	// - the 'pre' element.
	// - the 'div' with the 'paste-content' id.

	var pre = document.getElementsByTagName('pre');

	// Add a copy button in the 'pre' element.
	// which only has the className of 'language-' or ' hljs'(if enable highlight.js pre-render).

	for (var i = 0; i < pre.length; i++) {
		var tag_name = pre[i].children[0].className
            	var isLanguage = tag_name.startsWith('language-') || tag_name.endsWith(' hljs');
		if ( isLanguage ) {
			var button           = document.createElement('button');
					button.className = 'copy-button';
					button.textContent = 'Copy';

					pre[i].appendChild(button);
		}
	};

	// Run Clipboard

	var copyCode = new Clipboard('.copy-button', {
		target: function(trigger) {
			return trigger.previousElementSibling;
    }
	});

	// On success:
	// - Change the "Copy" text to "Copied".
	// - Swap it to "Copy" in 2s.
	// - Lead user to the "contenteditable" area with Velocity scroll.

	copyCode.on('success', function(event) {
		event.clearSelection();
		event.trigger.textContent = 'Copied';
		window.setTimeout(function() {
			event.trigger.textContent = 'Copy';
		}, 2000);

	});

	// On error (Safari):
	// - Change the  "Press Ctrl+C to copy"
	// - Swap it to "Copy" in 2s.

	copyCode.on('error', function(event) {
		event.trigger.textContent = 'Press "Ctrl + C" to copy';
		window.setTimeout(function() {
			event.trigger.textContent = 'Copy';
		}, 5000);
	});

})();
</script>


    <footer class="container-fluid footer-copy">
  <div class="container">
    <div class="row footrow">
      <ul>
        <li><a href="/project">About</a></li>
        <li><a href="/about/help">Get Help</a></li>
        <li><a href="/governance/">Governance</a></li>
        <li><a href="/research/#publications">Publications</a></li>
        <li><a href="/research/#sponsors">Sponsors</a></li>
      </ul>
      <ul>
        <li><a href="/downloads/">Downloads</a></li>
        <li><a href="/downloads/">All Releases</a></li>
        <li><a href="https://github.com/JuliaLang/julia">Source Code</a></li>
        <li><a href="/downloads/#current_stable_release">Current Stable Release</a></li>
        <li><a href="/downloads/#long_term_support_release">Longterm Support Release</a></li>
      </ul>
      <ul>
        <li><a href="https://docs.julialang.org/en/v1/">Documentation</a></li>
        <li><a href="https://juliaacademy.com">JuliaAcademy</a></li>
        <li><a href="https://www.youtube.com/user/JuliaLanguage">YouTube</a></li>
        <li><a href="/learning/getting-started/">Getting Started</a></li>
        <li><a href="https://docs.julialang.org/en/v1/manual/faq/">FAQ</a></li>
        <li><a href="/learning/books">Books</a></li>
      </ul>
      <ul>
        <li><a href="/community/">Community</a></li>
        <li><a href="/community/standards/">Code of Conduct</a></li>
        <li><a href="/community/stewards/">Stewards</a></li>
        <li><a href="/diversity/">Diversity</a></li>
        <li><a href="https://juliagenderinclusive.github.io">Julia Gender Inclusive</a></li>
        <li><a href="https://juliacon.org">JuliaCon</a></li>
        <li><a href="/community/#julia_user_and_developer_survey">User/Developer Survey</a></li>
        <li><a href="/shop/">Shop Merchandise</a></li>
      </ul>
      <ul>
        <li><a href="https://github.com/JuliaLang/julia/blob/master/CONTRIBUTING.md">Contributing</a></li>
        <li><a href="/contribute">Contributor's Guide</a></li>
        <li><a href="https://github.com/JuliaLang/julia/issues">Issue Tracker</a></li>
        <li><a href="https://github.com/JuliaLang/julia/security/policy">Report a Security Issue</a></li>
        <li><a href="https://github.com/search?q=is%3Aopen+is%3Aissue+language%3AJulia+label%3A%22help+wanted%22">Help Wanted Issues</a></li>
        <li><a href="https://github.com/search?q=is%3Aopen+is%3Aissue+language%3AJulia+label%3A%22good+first+issue%22">Good First Issue</a></li>
        <li><a href="https://docs.julialang.org/en/v1/devdocs/init/">Dev Docs</a></li>
      </ul>
    </div>
    <div id="footer-bottom" class="row">
      <div class="col-md-10 py-2">
        <p>Last modified: October 30, 2024. This site is powered by <a href="https://www.netlify.com">Netlify</a>, <a href="https://franklinjl.org">Franklin.jl</a>, and the <a href="https://julialang.org">Julia Programming Language</a>.</p>
        <p>We thank <a href="https://www.fastly.com">Fastly</a> for their generous infrastructure support.</p>
        <p>©2024 JuliaLang.org <a href="https://github.com/JuliaLang/www.julialang.org/graphs/contributors">contributors</a>. The content on this website is made available under the <a href="https://github.com/JuliaLang/www.julialang.org/blob/master/LICENSE.md">MIT license</a>.</p>
      </div>
      <div class="col-md-2 py-2">
        <span class="float-sm-right">
          <a class="github-button" href="https://github.com/sponsors/julialang" data-icon="octicon-heart" data-size="large" aria-label="Sponsor @julialang on GitHub">Sponsor</a>
        </span>
      </div>
    </div>
  </div>
</footer>

<script src="/libs/jquery/jquery.min.js"></script>
<script src="/libs/bootstrap/bootstrap.min.js"></script>
<!-- <script src="/libs/highlight/highlight.min.js"></script> -->
<!-- <script>hljs.initHighlightingOnLoad();</script> -->

    <script src="/libs/groups.js"></script>
    <script src="/libs/map.js"></script>
  </body>
</html>
