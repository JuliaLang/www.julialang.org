<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv=x-ua-compatible  content="ie=edge"> <meta http-equiv=content-type  content="text/html; charset=utf-8" /> <meta name=author  content="Jeff Bezanson, Stefan Karpinski, Viral Shah, Alan Edelman, et al." /> <meta name=description  content="The official website for the Julia Language. Julia is a language that is fast, dynamic, easy to use, and open source. Click here to learn more."/> <meta property="og:title" content="The Julia Language"/> <meta property="og:image" content="/assets/images/julia-open-graph.png"/> <meta property="og:description" content="Official website for the Julia programming language"/> <link href="https://fonts.googleapis.com/css?family=Roboto:400,400i,500,500i,700,700i" rel=stylesheet > <link rel=stylesheet  href="/libs/bootstrap/bootstrap.min.css" /> <link rel=stylesheet  href="/css/app.css" /> <link rel=stylesheet  href="/css/fonts.css" /> <link rel=stylesheet  href="/css/franklin.css" /> <script async defer src="/libs/buttons.js"></script> <!-- --> <script type="application/javascript"> var doNotTrack = false; if (!doNotTrack) { window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date; ga('create', 'UA-28835595-1', 'auto'); ga('send', 'pageview'); } </script> <script async src='https://www.google-analytics.com/analytics.js'></script> <link rel=icon  href="/assets/infra/julia.ico"> <title>High Performance and Parallel Computing Projects – Summer of Code</title> <style> .container ul li p {margin-bottom: 0;} </style> <div class="container py-3 py-lg-0"> <nav class="navbar navbar-expand-lg navbar-light bg-light" id=main-menu > <a class=navbar-brand  href="/" id=logo > <img src="/assets/infra/logo.svg" height=55  width=85  alt="JuliaLang Logo"/> </a> <button class="navbar-toggler ml-auto hidden-sm-up float-xs-left" type=button  data-toggle=collapse  data-target="#navbarSupportedContent" aria-controls=navbarSupportedContent  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarSupportedContent > <ul class="navbar-nav mr-auto"> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="/downloads/">Download</a> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="https://docs.julialang.org">Documentation</a> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="/blog/">Blog</a> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="/community/">Community</a> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="/learning/">Learning</a> <li class="nav-item flex-md-fill text-md-center"> <a class=nav-link  href="/research/">Research</a> <li class="nav-item active flex-md-fill text-md-center"> <a class=nav-link  href="/jsoc/">JSoC</a> <li class="nav-item donate flex-md-fill text-md-center"> <a class=github-button  href="https://github.com/sponsors/julialang" data-icon=octicon-heart  data-size=large  aria-label="Sponsor @julialang on GitHub">Sponsor</a> </ul> </div> </nav> </div> <br><br> <div class=container > <h1 id=high_performance_and_parallel_computing_projects_summer_of_code ><a href="#high_performance_and_parallel_computing_projects_summer_of_code">High Performance and Parallel Computing Projects – Summer of Code</a></h1> <p>Julia is emerging as a serious tool for technical computing and is ideally suited for the ever-growing needs of big data analytics. This set of proposed projects addresses specific areas for improvement in analytics algorithms and distributed data management.</p> <h2 id=general_high-performance_networking_using_ucx ><a href="#general_high-performance_networking_using_ucx">General High-performance networking using UCX</a></h2> <p>An important missing piece in Julia&#39;s parallelism story is the ability to use fast interconnects without sacrificing programming model. Right now, you can either use <code>Distributed.remotecall</code> for a flexible programming model, but <code>MPI</code> is necessary if you are serious about performance on cutting-edge clusters and supercomputers. Can this situation be assuaged? Write a fast communication library that can make use of any interconnect by way of wrapping the <a href="https://www.openucx.org/">UCX networking library</a>. Show improvements on a sizable benchmark.</p> <p>Mentors: <a href="https://github.com/shashi">Shashi Gowda</a>, <a href="https://github.com/vchuravy">Valentin Churavy</a></p> <h2 id=simple_persistent_distributed_storage ><a href="#simple_persistent_distributed_storage">Simple persistent distributed storage</a></h2> <p>This project proposes to implement a very simple persistent storage mechanism for Julia variables so that data can be saved to and loaded from disk with a consistent interface that is agnostic of the underlying storage layer. Data will be tagged with a minimal amount of metadata by default to support type annotations, time-stamped versioning and other user-specifiable tags, not unlike the <code>git stash</code> mechanism for storing blobs. The underlying engine for persistent storage should be generic and interoperable with any reasonable choice of binary blob storage mechanism, e.g. MongoDB, ODBC, or HDFS. Of particular interest will be persistent storage for distributed objects such as <code>DArray</code>s, and making use of the underlying storage engine&#39;s mechanisms for data movement and redundant storage for such data.</p> <h2 id=dynamic_distributed_execution_for_data_parallel_tasks_in_julia ><a href="#dynamic_distributed_execution_for_data_parallel_tasks_in_julia">Dynamic distributed execution for data parallel tasks in Julia</a></h2> <p>Distributed computation frameworks like Hadoop/MapReduce have demonstrated the usefulness of an abstraction layer that takes care of low level concurrency concerns such as atomicity and fine-grained synchronization, thus allowing users to concentrate on task-level decomposition of extremely large problems such as massively distributed text processing. However, the tree-based scatter/gather design of MapReduce limits its usefulness for general purpose data parallelism, and in particular poses significant restrictions on the implementation of iterative algorithms.</p> <p>This project proposal is to implement a native Julia framework for distributed execution for general purpose data parallelism, using dynamic, runtime-generated general task graphs which are flexible enough to describe multiple classes of parallel algorithms. Students will be expected to weave together native Julia parallelism constructs such as the <code>ClusterManager</code> for massively parallel execution, and automate the handling of data dependencies using native Julia <code>RemoteRefs</code> as remote data futures and handles. Students will also be encouraged to experiment with novel scheduling algorithms.</p> <h2 id=model_zoo_on_tpu ><a href="#model_zoo_on_tpu">Model Zoo on TPU</a></h2> <p>Julia has experimental support for executing code on TPUs &#40;https://github.com/JuliaTPU/XLA.jl&#41; TPUs enable training cutting edge machine learning models written using Flux. However, TPUs are not able to execute arbitrary code and thus often require individual attention to fix new patterns in XLA.jl or other packages. Additionally, the performance characteristics of the TPU hardware are quite unlike that of CPU or even GPU and models may thus require TPU-specific adjustments to achieve peak performance. Lastly, the speed of TPUs presents significant challenges to data input pipelines even at single-TPU levels of performance. Having a wide set of models available that are tuned for TPU will aid in finding common abstractions for models independent of hardware.</p> <p>Mentors: <a href="https://github.com/Keno">Keno Fischer</a></p> <h2 id=scientific_integration_benchmarks ><a href="#scientific_integration_benchmarks">Scientific Integration Benchmarks</a></h2> <p>A benchmark suite would help us to keep Julia&#39;s performance for ML models in shape, as well as revealing opportunities for improvement. Like the model-zoo project, this would involve contributing standard models that exercise common ML use cases &#40;images, text etc&#41; and profiling them. The project could extend to include improving performance where possible, or creating a &quot;benchmarking CI&quot; like Julia&#39;s own <a href="https://github.com/JuliaCI/Nanosoldier.jl">nanosoldier</a>.</p> <p>Mentors: <a href="https://github.com/dhairyagandhi96/">Dhairya Gandhi</a>, <a href="https://github.com/staticfloat">Elliot Saba</a>.</p> <h2 id=multi-gpu_training ><a href="#multi-gpu_training">Multi-GPU training</a></h2> <p>Implement and demonstrate multi-GPU parallelism. One route is to expose communication primitives from NVIDIA&#39;s <a href="https://developer.nvidia.com/nccl">NCCL</a> library and use these to build tooling for model parallelism and distributed training. The project should demonstrate parallel training of a Flux model with benchmarks.</p> <p>Mentors: <a href="https://github.com/vchuravy">Valentin Churavy</a>, <a href="https://github.com/maleadt">Tim Besard</a></p> <h2 id=distributed_training ><a href="#distributed_training">Distributed Training</a></h2> <p>Add a distributed training API to Flux, possibly inspired by PyTorch&#39;s equivalent. Any distributed training algorithm could be used &#40;ideally the foundations make it easy to experiment with different setups&#41;, though the easiest is likely to implement an MXNet-like parameter server. It should demonstrate training a Flux model with data distributed over multiple nodes, with benchmarks.</p> <p>Mentors: <a href="https://github.com/vchuravy">Valentin Churavy</a>, <a href="https://github.com/maleadt">Tim Besard</a></p> <h2 id=sparse_gpu_and_ml_support ><a href="#sparse_gpu_and_ml_support">Sparse GPU and ML support</a></h2> <p>While Julia supports dense GPU arrays well via <a href="https://github.com/JuliaGPU/CUSPARSE.jl">CuArrays</a>, we lack up-to-date wrappers for sparse operations. This project would involve wrapping CUDA&#39;s sparse support, with <a href="https://github.com/JuliaGPU/CUSPARSE.jl">CUSPARSE.jl</a> as a starting point, adding them to CuArrays.jl, and perhaps demonstrating their use via a sparse machine learning model. </div> <br><br> <footer class="container-fluid footer-copy"> <div class=container > <div class=row > <div class="col-md-10 py-2"> <p>This website is built with <a style="color: #7a95dd" href="https://franklinjl.org">Franklin.jl</a> - a Julia native package for building websites. We thank <a style="color: #7a95dd" href="https://www.fastly.com">Fastly</a> for their generous infrastructure support. <p>©2020 JuliaLang.org <a style="color: #7a95dd" href="https://github.com/JuliaLang/www.julialang.org/graphs/contributors">contributors</a>. The content on this website is made available under the <a style="color: #7a95dd" href="https://github.com/JuliaLang/www.julialang.org/blob/master/LICENSE.md">MIT license</a>. </div> <div class="col-md-2 py-2"> <a class=github-button  href="https://github.com/sponsors/julialang" data-icon=octicon-heart  data-size=large  aria-label="Sponsor @julialang on GitHub">Sponsor</a> </div> </div> </div> </footer> <script src="/libs/jquery/jquery.min.js"></script> <script src="/libs/bootstrap/bootstrap.min.js"></script> <script src="/libs/platform.js"></script>