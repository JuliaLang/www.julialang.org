<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title>GSoC 2018: Adding Newer Features and Speeding up Convolutions in Flux</title>
<meta name="author" content="Jeff Bezanson, Stefan Karpinski, Viral Shah, Alan Edelman, et al." />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Official website for the Julia programming language. Join the Julia community today.">

<meta property="og:title" content="The Julia Language"/>
<meta property="og:image" content="http://www.julialang.org/images/julia-open-graph.png"/>
<meta property="og:description" content="Official website for the Julia programming language"/>

<link href="https://fonts.googleapis.com/css?family=Roboto:400,400i,500,500i,700,700i" rel="stylesheet">
<link rel="stylesheet" href="julialang.org/v2/css/bootstrap.min.css" />
<link rel="stylesheet" href="julialang.org/v2/css/app.css" />
<link rel="stylesheet" href="julialang.org/v2/css/fonts.css" />

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
             extensions: ["AMSmath.js", "AMSsymbols.js"] }
    }
  });
</script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-28835595-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


  

  
</head>

<body>
  
  

<div class="container py-3 py-lg-0">
  <nav class="navbar navbar-expand-lg navbar-light bg-light" id="main-menu">

    <a class="navbar-brand" href="../../../../" id="logo">
      <img src="../../../../v2/img/logo.svg" height="55" width="85" alt="JuliaLang Logo"/>
    </a>

    <button class="navbar-toggler ml-auto hidden-sm-up float-xs-left" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="../../../../downloads/">Download</a>
        </li>
        <li class="nav-item flex-md-fill text-md-center">
          <a class="nav-link" href="https://docs.julialang.org">Documentation</a>
        </li>
        <li class="nav-item  active  flex-md-fill text-md-center">
          <a class="nav-link" href="../../../../blog/">Blog</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="../../../../community/">Community</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="../../../../learning/">Learning</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="../../../../research/">Research</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="../../../../jsoc/">JSoC</a>
        </li>
        <li class="nav-item donate flex-md-fill text-md-center">
          <a class="btn btn-success" href="https://numfocus.org/donate-to-julia">Donate</a>
        </li>
      </ul>
    </div>

  </nav>
</div>



  <br /><br />

  <div class="container">

  <div id="blogpost">
    <h1>GSoC 2018: Adding Newer Features and Speeding up Convolutions in Flux</h1>

    <p class="metadata">
      <span class="timestamp">13 August, 2018</span>
      
      &nbsp;|&nbsp;
      <span class="author"><a href="http://github.com/avik-pal/">Avik Pal</a></span>
      
    </p>

    

<p>Over the summer I have been working at improving the Computer Vision capabilities of Flux. My specific line of work was to <strong>add newer models to the Flux model-zoo</strong>, <strong>implement some new features</strong> and also <strong>improve the speed of the previous layers</strong>. Specifically, I achieved a <strong>18-fold</strong> speed up for the <strong>Convolutions</strong> and around <strong>3-fold</strong> for <strong>BatchNorm</strong>.</p>

<h1 id="a-short-summary-of-my-work-during-gsoc-2018">A Short Summary of my work during GSoC 2018</h1>

<p>I am listing all the essential PRs I had made during this project. Some of them are merged, some are unmerged, and some are even a work in progress. We discuss only major PRs, leaving out bug fixes and small patches. So here they are</p>

<ol>
<li><a href="https://github.com/FluxML/Flux.jl">Flux.jl</a>

<ul>
<li><a href="https://github.com/FluxML/Flux.jl/pull/335">Adds support for a more efficient CUDNN Binding for Convolutions</a></li>
<li><a href="https://github.com/FluxML/Flux.jl/pull/294">Implements a wrapper for CUDNN BatchNorm and hooks it up with Flux</a></li>
<li><a href="https://github.com/FluxML/Flux.jl/pull/279">Allows Flux to support Depthwise Convolutions in CPU</a></li>
</ul></li>
<li><a href="https://github.com/JuliaGPU/CuArrays.jl">CuArrays.jl</a>

<ul>
<li><a href="https://github.com/JuliaGPU/CuArrays.jl/pull/96">Provide support for allocating workspace in CUDNN Convolutions</a></li>
<li><a href="https://github.com/JuliaGPU/CuArrays.jl/pull/100">Adds wrappers for CUDNN Activation Functions and some functions for efficient Convolutions</a></li>
</ul></li>
<li><a href="https://github.com/FluxML/Metalhead.jl">Metalhead.jl</a>

<ul>
<li><a href="https://github.com/FluxML/Metalhead.jl/pull/10">Fix the API of the newly added models</a></li>
<li><a href="https://github.com/FluxML/Metalhead.jl/pull/14">Improve the accuracy of the models in Metalhead</a></li>
<li><a href="https://github.com/FluxML/Metalhead.jl/pull/17">Add new models to Metalhead</a></li>
</ul></li>
<li><a href="https://github.com/FluxML/NNlib.jl">NNlib.jl</a>

<ul>
<li><a href="https://github.com/FluxML/NNlib.jl/pull/42">Add the support of Pure Julia Depthwise Convolutions and their Gradients</a></li>
</ul></li>
<li><a href="https://github.com/FluxML/model-zoo">model-zoo</a>

<ul>
<li><a href="https://github.com/FluxML/model-zoo/pull/33">Add the VGG models to model-zoo</a></li>
<li><a href="https://github.com/FluxML/model-zoo/pull/44">Showcase the use of Residual Networks as a Scene Classifier</a></li>
<li><a href="https://github.com/FluxML/model-zoo/pull/45">Put popular Imagenet Winning models like Inception Nets into model-zoo</a></li>
</ul></li>
</ol>

<p>The following new packages were developed during the course of this project.</p>

<ol>
<li><a href="https://github.com/avik-pal/FastStyleTransfer.jl">FastStyleTransfer.jl</a></li>
<li><a href="https://github.com/avik-pal/MURA.jl">MURA.jl</a></li>
<li><a href="https://github.com/avik-pal/DeepLearningBenchmarks">DeepLearningBenchmarks</a></li>
<li><a href="https://github.com/avik-pal/CNNVisualize.jl">CNNVisualize.jl</a></li>
<li><a href="https://github.com/avik-pal/DeepDream.jl">DeepDream.jl</a></li>
</ol>

<h1 id="a-walkthrough-of-the-pull-requests">A Walkthrough of the Pull Requests</h1>

<p>Let&rsquo;s go through these changes one by one.</p>

<h3 id="add-a-wrapper-for-cudnn-batchnorm">Add a wrapper for CUDNN BatchNorm</h3>

<p>Flux currently lacks a dedicated GPU Kernel for BatchNorm. BatchNorm is one of the most important layers of Neural Networks, and they speed up training by dealing with the internal mean covariance shift. Until now we were using the Flux CPU code for BatchNorm (which obviously will be slow). So this PR aims to solve this problem by wrapping the CUDNN Batchnorm Layer and integrating it with the Flux AD. Some highlights of the speed (and memory consumption) improvements are <code>1.860 s (1367 allocations: 50.92 KiB)</code> &mdash;&gt; <code>2.782 ms (276 allocations: 10.38 KiB)</code>. I am benchmarking the <strong>total time (forward + backward)</strong> of <strong>BatchNorm(100)</strong> for a <strong>224 * 224 * 100 * 10</strong> sized array. This PR is yet to be merged. It needs to be updated to Julia 1.0 (which is supported by the Flux master) before merging.</p>

<h3 id="speed-up-the-cuda-convolutions-in-flux">Speed up the CUDA Convolutions in Flux</h3>

<p>I performed benchmarks between Flux and Pytorch (read on to know more about that). We went on profile the neural networks and found some issues in Flux Conv Layer. The major bottleneck was in the <code>broadcasted bias addition</code> that we were performing. So instead of using the <code>broadcasted bias addition</code> we use <code>cudnnAddTensor</code> for CUDNN Version prior to 7.1. For anything above 7.1, we shift to using <code>cudnnConvolutionBiasActivationForward</code> with the activation always being <code>identity</code> and finally dispatch over the other activations. The major improvements to speed using this update reflects in the <code>DeepLearningBenchmarks</code> repo. Also, this PR depends on a CuArrays PR, so it cannot be merged until the CuArrays has been merged. Also, it requires updates to be able to adapt to Julia 1.0.</p>

<h3 id="native-julia-depthwise-convolutions-in-flux-and-nnlib">Native Julia Depthwise Convolutions in Flux and NNlib</h3>

<p>Depthwise Separable Convolutions are vital for Mobile Applications of Deep Neural Networks. MobileNets and Xception Net make direct use of this form of Convolution. So it is quite essential for a deep learning library to support such convolutions out of the box. Firstly this involved implementing the CPU version of the code in NNlib. Then we just need to the hook up the depthwise convolution into the Flux AD. Out of box support also allow some of the <em>to be</em> added models in <strong>Metalhead.jl</strong> and <strong>model-zoo</strong> to be easily defined. As a part of some future work on this topic, there needs to be a CUDNN binding for this algorithm.</p>

<h3 id="adding-support-for-more-cudnn-convolution-algorithms">Adding support for more CUDNN Convolution Algorithms</h3>

<p>There are a variety of Convolution Algorithms around. All these use the properties of the <code>input tensor</code>, and the <code>filter tensor</code> and have very specialized routines developed for efficient convolutions. Thankfully CUDNN has these specially developed convolution routines built into it. So we need to integrate it directly into <code>CuArrays</code> and expose its API for use from other packages like <code>Flux</code>. The wrappers for a simple convolution operation was pre-written in CuArrays. So we only need to create the wrappers for <code>workspace allocation</code>. This PR adds the necessary wrappers and changes the convolution function definitions to expose the API for <code>algorithm change</code>. So for the end user, the only change would be to change the keyword argument <code>algo</code>.</p>

<h3 id="add-wrappers-for-more-convolution-and-activation-functions">Add wrappers for more Convolution and Activation Functions</h3>

<p>When benchmarking the Flux Convolution Code, we figured out some of the major bottlenecks that were coming out of the <code>Backward Pass for Convolution Bias</code>. Hence the natural choice was to wrap the CUDNN Function which efficiently calculates the Gradient for Bias. Also, we were able to wrap a function for applying <code>activation</code> and <code>adding bias</code> at the same time. To use this function, <code>cudnnConvolutionBiasActivationFunction</code> we needed to wrap the Activation Forward and Backward Pass functions. Now lets see what kind of speed improvements we achieved with this update.</p>

<p align = "center">
    <img src = "../../../../images/blog/2018-08-13-GSoC-Flux-Computer-Vision/flux_internal_benchmarks.png" width="750" height="500">
</p>

<h3 id="fixing-the-api-of-new-metalhead-models">Fixing the API of new Metalhead models</h3>

<p>Some models like GoogleNet and Resnet were added to Metalhead recently (special thanks to <strong>Ayush Shridhar [@ayush1999]</strong> for his work on <strong>ONNX.jl</strong>). However, this code is <strong>generated automatically</strong> and not necessarily human readable. Moreover, the only thing we could do we these models was to perform predictions. We can&rsquo;t use it for something like feature extraction. So we ported some of my models from the model-zoo and manually load the weights into it.</p>

<h3 id="improve-the-accuracy-of-metalhead-models">Improve the accuracy of Metalhead models</h3>

<p>The accuracy of the existing loaded models into Flux was pretty bad. We had already tried out a variety of preprocessing steps but mostly of no use. After some trial and errors, we were able to figure out the primary reason. We were using the weights for <strong>Cross Correlation Operation</strong> in place of a <strong>Convulution Operation</strong>. For now, this is fixed by manually flipping the weights before loading them. As a long-term solution, we are exposing the parameter to choose between <strong>convolution</strong> and <strong>cross-correlation</strong> in <code>NNlib</code> and eventually in <code>Flux</code>.</p>

<h3 id="add-bleeding-edge-computer-vision-models-in-metalhead">Add bleeding edge Computer Vision models in Metalhead</h3>

<p>This part of the project is still in its infancy. Most of the work for this is done (but it is mainly scattered in model-zoo). The model zoo is  essentially is targeted to allow users to import all sorts of models in their code. The models might be untrained (which most of the models are currently are). So the primary motivation is that if we want to train a <code>ResNeXt</code> model, we don&rsquo;t have to redefine something which has already been done by someone. We should be able to load the model without any effort.</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">model<span style="color:#666"></span> <span style="color:#666">=</span> VGG19<span style="color:#666"></span>() <span style="color:#080;font-style:italic"># This fetches an untrained VGG19 model</span>
model_trained<span style="color:#666"></span> <span style="color:#666">=</span> trained<span style="color:#666"></span>(VGG19<span style="color:#666"></span>) <span style="color:#080;font-style:italic"># Get the trained VGG19 model. This is the same as previously calling VGG19()</span>
trained<span style="color:#666"></span>(VGG11<span style="color:#666"></span>) <span style="color:#080;font-style:italic"># We get an error as we don&#39;t currently have a trained VGG11 model but VGG11() works fine</span></code></pre></div>
<h1 id="brief-description-of-the-packages">Brief Description of the Packages</h1>

<h3 id="deepdream-jl">DeepDream.jl</h3>

<p>This package provides a simple API to generate <a href="https://en.wikipedia.org/wiki/DeepDream">dreams</a> on the desired image. You need to provide the image, choose what type of dream you want and which model to use. This package relies on Flux and Metalhead for its trained models.</p>

<p align = "center">
    <img src = "../../../../images/blog/2018-08-13-GSoC-Flux-Computer-Vision/deepdream.jpg" width="750" height="750">
</p>

<p>The above image was generated using <code>guided deepdream</code>.</p>

<h3 id="cnnvisualize-jl">CNNVisualize.jl</h3>

<p>Over the years several visualization algorithms have been developed to understand the functioning of neural networks. This package aims to implement such algorithms. Most of these, are going to work out of the box for Metalhead. This is currently a work in progress package, however, most of it is complete.</p>

<p>Here&rsquo;s a small demo of the package</p>

<script src="https://gist.github.com/avik-pal/7bea8a2f004268963dae431776215746.js"></script>

<h3 id="faststyletransfer-jl">FastStyleTransfer.jl</h3>

<p>This is the implementation of the paper <strong><a href="https://arxiv.org/pdf/1603.08155">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a></strong>. There are some obvious deviations from the paper. We used the best layer implementations that were currently available in Flux. As for the exact architecture it is still in developement. We provide three pre-trained models with this package. The API has been kept as simple as possible.</p>

<p>Below is a small example of style transfer on MonaLisa</p>

<p align = "center">
    <img src = "../../../../images/blog/2018-08-13-GSoC-Flux-Computer-Vision/styletransfer.jpg" width="750" height="750">
</p>

<h1 id="overview-of-the-work-done-in-gsoc-2018">Overview of the Work done in GSoC 2018</h1>

<p>As you can see from the above PR descriptions a lot of my work has been around benchmarking Flux models and making speed ups wherever possible. The initial part of my job was to add some new computer vision models to the Flux model-zoo. So we added models like <code>VGGNets</code>, <code>ResNets</code>, <code>DenseNets</code>, etc. to the Flux model-zoo. Also, we were able to port some of these models to the <code>Metalhead</code> package which is specially designed to address Computer Vision problems. After lots of experimentation and help from some people of the JuliaLang community, we were able to fix some of the accuracy problems we were encountering. Next, we went on to develop a package to perform FastStyleTransfer. It allows users to easily <code>train</code> their models and also <code>stylize</code> images with great ease. We was also able to train some of the densenet models and recreate the results of the MURA paper.</p>

<p>Next up was to perform benchmarks for the current implementations in Flux and solve the bottlenecks wherever possible. So we wrote the benchmarking scripts for Flux and Pytorch and performed heads on comparison between them. For the first time, it turned out that Pytorch is much faster than Flux. However, we were able to find the reason for this slow speed. Turned out it was because of the lack of a specialized kernel for broadcasted addition and its backward pass. So the immediate solution was to <code>wrap some of the CUDNN Functions</code> and integrate them with Flux. Doing this actually brings down the time taken by those layers a lot. Currently, we are at-par with Pytorch wrt the time for each of the individual layers.</p>

<h1 id="experience-at-juliacon">Experience at JuliaCON</h1>

<p>I was able to attend JuliaCon 2018 in London. Thanks to <strong>The Julia Project</strong> and <strong>NumFOCUS</strong> for funding this trip. I got the opportunity to present a poster on the work I had done during my GSoC. It was the first conference I was attending, so it was indeed quite a unique experience. I was able to share my work with other people and even got some valuable advice regarding it. Also, I discovered some new cool open-sourced projects that I would like to contribute to in the future. Finally, it&rsquo;s always a pleasure to meet the people I have been interacting with in Slack.</p>

<h1 id="why-use-julia-and-flux-for-deep-learning">Why use Julia and Flux for Deep Learning?</h1>

<p>There is a <a href="https://julialang.org/blog/2017/12/ml&amp;pl">brilliant post</a> on how Julia can play its part as a Language for Machine Learning. That post summarizes the reasons from the viewpoint of people highly experienced in the field of Machine Learning. Here I shall be presenting the reasons from a layman&rsquo;s point of view.</p>

<p>Just think about implementing a standard Computer Vision model in one of the popular frameworks, like Pytorch or Tensorflow. It&rsquo;s pretty simple, right? Just call the necessary layers using their API, and you&rsquo;re done. Now imagine having to define something that is not present in their standard library. You need to first write your custom layer (both forward and backward passes, in case you are wondering) in C++ and if that was not hard enough you go about to define the GPU Kernel for that code in CUDA C. Now you integrate this layer (obviously in Python) with Pytorch or Tensorflow as per their particular API. And good luck debugging the SegFaults that you get.</p>

<p>Now let&rsquo;s see how you do that in Flux. You start by writing the layer in Julia and its CUDA GPU version using <strong>CUDAnative</strong> (cheers to <strong>Tim Besand [@maleadt]</strong> for his excellent work). As for integration into the Flux AD you simply use the <code>@grad</code> macro. It&rsquo;s that simple!</p>

<p>One complaint you might be having is the unavailability of a lot of trained models. However, thanks to <strong>ONNX.jl</strong> and <strong>Keras.jl</strong> the problem is more or less resolved. Both of these are the work of <strong>Ayush Shridhar</strong>. Using these you can use models trained using Pytorch or CNTK, as long as they are stored in ONNX format. Also, now you have a wide range of Reinforcement Learning Models like <strong>AlphaGo.jl</strong> (by <strong>Tejan Karmali</strong>) written using Flux besides the Computer Vision models in <strong>model-zoo</strong> and <strong>Metalhead.jl</strong>.</p>

<h1 id="future-works-for-the-project">Future Works for the Project</h1>

<p>This Project has deviated highly from what I had initially proposed but its mostly for good. The things implemented as a part of this project should surely help in the faster training of Deep Neural Networks in Flux and also help create more complicated models using Flux. That being said an exciting thing for the future of this Project would be to complete the addition of <strong>Object Classification models</strong> in Metalhead as proposed in this <a href="https://github.com/FluxML/Metalhead.jl/issues/16">issue</a>. Another interesting thing to have would be some <strong>Object Detection</strong> models built using Flux in one place. Also, we should continue to solve the current bottlenecks that are left to be addressed. We should keep adding the benchmarks to <a href="https://github.com/avik-pal/DeepLearningBenchmarks">DeepLearningBenchmarks</a> which is vital for the identification of bottlenecks.</p>

<h1 id="acknowledgements">Acknowledgements</h1>

<p>Firstly, I should thank <strong>Google</strong> for organizing Google Summer of Code which gave me this excellent opportunity to work with the Open Source Community. Also, I thank <strong>NumFOCUS</strong> and <strong>JuliaLang</strong> for selecting me to work on this project. Next, I would thank my mentors <strong>Viral Shah</strong> and <strong>Mine Innes</strong> for their constant support and guiding me through my project. Finally, let me thank the brilliant <strong>JuliaLang Community</strong> for clearing my doubts and being an excellent source for learning.</p>

<hr />

<p align = "center">
    <img src = "../../../../images/blog/2018-08-13-GSoC-Flux-Computer-Vision/gsoc_logo.png" width="700">
</p>


  </div>

  

  </div>
  </div>
  </div>

  <br />

  


  <head>
  <meta name="description" content="We thank our contributors, donators, and Fastly for their support in keeping the Julia Language going. Donate here to help pay for Julia's needs."/>
</head>

<footer class="container-fluid footer-copy">
    <div class="container">
      <div class="row">
        <div class="col-md-10 py-2">
          <p>
            We thank <a style="color: #7a95dd" href="https://www.fastly.com">Fastly</a> for their generous infrastructure support. Donations help pay for community resources such as CI, Discourse, workshops, travel, JuliaCon, and other such needs.
          </p>
          <p>
            Â©2020 JuliaLang.org contributors. The website content uses the <a style="color: #7a95dd" href="https://github.com/JuliaLang/www.julialang.org/blob/master/LICENSE.md">MIT license</a>.
          </p>
        </div>
        <div class="col-md-2 py-2">
          <a class="btn btn-success" href="https://numfocus.org/donate-to-julia">Donate</a>
        </div>
      </div>
    </div>
</footer>


  <script src="../../../../v2/js/jquery.min.js"></script>
<script src="../../../../v2/js/bootstrap.min.js"></script>
<script src="../../../../v2/js/platform.js"></script>
<script src="../../../../v2/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script async defer src="https://buttons.github.io/buttons.js"></script>

</body>

</html>
